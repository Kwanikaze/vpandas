{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRF_VAE",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbFx0JkYCAGrIDwSIwtQQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwanikaze/vpandas/blob/master/MRF_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaO7CHX93gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNkadXIh0gD",
        "colab_type": "text"
      },
      "source": [
        "# Load Data and Create Sample Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE259FbtK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create two datasets from global df that are one-hot encoded\n",
        "def OHE_sample(sample_df, features_to_OHE: list):\n",
        "  for feature in features_to_OHE:\n",
        "    feature_OHE = pd.get_dummies(prefix = feature,data= sample_df[feature])\n",
        "    sample_df = pd.concat([sample_df,feature_OHE],axis=1)\n",
        "  sample_df.drop(features_to_OHE,axis=1,inplace=True)\n",
        "  print(sample_df)\n",
        "  return sample_df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykDGUc_-Q2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "69276db7-f1b8-43ca-ab25-91a469e03f09"
      },
      "source": [
        "# Load global relation\n",
        "df = pd.read_csv(\"data_8.csv\")\n",
        "print(df.shape)\n",
        "\n",
        "#Create two datasets containing AB and BC\n",
        "num_samples = 1000\n",
        "sample1_df = df[['A','B']].sample(n=num_samples, random_state=2)\n",
        "print(sample1_df.head())\n",
        "sample2_df = df[['B','C']].sample(n=num_samples, random_state=3)\n",
        "print(sample2_df.head())\n",
        "\n",
        "# Make A,B,C inputs all 8 bits\n",
        "#Does data need to respect Gaussian distribution?\n",
        "#Could add noise so not exactly OHE: 0.01...0.9...0.01\n",
        "sample1_OHE = OHE_sample(sample1_df,['A','B'])\n",
        "sample2_OHE = OHE_sample(sample2_df,['B','C'])\n",
        "\n",
        "# Could onvert pandas dataframes to list of lists of lists\n",
        "# [ [[OHE A1],[OHE B1]], [[OHE A2],[OHE B2]], ...  ]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5056, 3)\n",
            "      A  B\n",
            "4333  7  6\n",
            "2638  6  4\n",
            "2254  4  4\n",
            "3116  5  5\n",
            "3998  6  6\n",
            "      B  C\n",
            "4616  7  6\n",
            "2276  4  6\n",
            "3448  5  4\n",
            "4064  6  5\n",
            "1204  2  3\n",
            "      A_0  A_1  A_2  A_3  A_4  A_5  A_6  ...  B_1  B_2  B_3  B_4  B_5  B_6  B_7\n",
            "4333    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2638    0    0    0    0    0    0    1  ...    0    0    0    1    0    0    0\n",
            "2254    0    0    0    0    1    0    0  ...    0    0    0    1    0    0    0\n",
            "3116    0    0    0    0    0    1    0  ...    0    0    0    0    1    0    0\n",
            "3998    0    0    0    0    0    0    1  ...    0    0    0    0    0    1    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "1857    0    1    0    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "3813    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "604     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "621     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1322    0    1    0    0    0    0    0  ...    0    1    0    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n",
            "      B_0  B_1  B_2  B_3  B_4  B_5  B_6  ...  C_1  C_2  C_3  C_4  C_5  C_6  C_7\n",
            "4616    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2276    0    0    0    0    1    0    0  ...    0    0    0    0    0    1    0\n",
            "3448    0    0    0    0    0    1    0  ...    0    0    0    1    0    0    0\n",
            "4064    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "1204    0    0    1    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "3358    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "1496    0    0    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4025    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "4689    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    1\n",
            "2155    0    0    0    1    0    0    0  ...    0    0    1    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWt2iUw9xE",
        "colab_type": "text"
      },
      "source": [
        "# Global Relation Bayesian Network Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZqS2rxNrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "outputId": "e242f003-1a00-4832-c1ec-731aaa7d0a1d"
      },
      "source": [
        "!pip install pgmpy==0.1.9\n",
        "import pgmpy\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "def groundTruth(df,evidence):\n",
        "    \"\"\"\n",
        "    Extracts ground truth from global relation\n",
        "    \"\"\"\n",
        "    model = BayesianModel([('B', 'A'), ('B', 'C')])\n",
        "    model.fit(df)\n",
        "    nx.draw(model, with_labels=True)\n",
        "    plt.show()\n",
        "    print('\\n Global Relation Ground Truth')\n",
        "    #for var in model.nodes():\n",
        "    #    print(model.get_cpds(var))\n",
        "    inference = VariableElimination(model)\n",
        "    q = inference.query(variables=['A','B','C'])\n",
        "    joint_prob = q.values.flatten()\n",
        "    #print(joint_prob)\n",
        "    #print('\\n P(A,B,C) \\n Ground Truth')\n",
        "    #print(q)\n",
        "    q = inference.query(variables=['C'], evidence=evidence)\n",
        "    print('\\n P(C|A=0) \\n Ground Truth')\n",
        "    print(q)\n",
        "\n",
        "groundTruth(df,{'A':0})"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pgmpy==0.1.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/b1/18dfdfcb10dcce71fd39f8c6801407e9aebd953939682558a5317e4a021c/pgmpy-0.1.9-py3-none-any.whl (331kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 337kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pgmpy\n",
            "Successfully installed pgmpy-0.1.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "Finding Elimination Order: : 100%|██████████| 1/1 [00:00<00:00, 438.09it/s]\n",
            "Eliminating: B: 100%|██████████| 1/1 [00:00<00:00, 412.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "\n",
            " P(C|A=0) \n",
            " Ground Truth\n",
            "+------+----------+\n",
            "| C    |   phi(C) |\n",
            "+======+==========+\n",
            "| C(0) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(1) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(2) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(3) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(4) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(5) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(6) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(7) |   0.0000 |\n",
            "+------+----------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3YIf_-iAm8",
        "colab_type": "text"
      },
      "source": [
        "# Create VAE-MRF Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UMLBM0iE4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VAE Parameters\n",
        "num = 8 # digits from 0 to 7\n",
        "latent_dims = 3\n",
        "num_epochs = 1000\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "variational_beta = 0.00001 #tuned"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FiF8-RkNLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super().__init__()\n",
        "        self.latent_dims = latent_dims\n",
        "        self.fc1 = nn.Linear(num, latent_dims)\n",
        "        self.fc_mu = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvar = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_out = nn.Linear(latent_dims,num)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.sigmoid(self.fc1(x))\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        if z.size()[0] == self.latent_dims: #resize from [3] to [1,3] if fed only a single sample\n",
        "          z = z.view(1, self.latent_dims)\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        recon = softmax(self.fc_out(z))\n",
        "        return recon\n",
        "\n",
        "    def forward(self, x, latent_dims):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def latent(self,x,latent_dims):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "      # ignore latent_mu, latent_logvar, instead generate z values from standard normal\n",
        "      z = torch.randn(num_samples, self.latent_dims)\n",
        "      z = z.to(device)\n",
        "      samples = self.decode(z)\n",
        "      return samples\n",
        "\n",
        "def vae_loss(batch_recon, x_batch_targets, mu, logvar):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  CE = criterion(batch_recon, x_batch_targets)\n",
        "  #print(CE)\n",
        "  KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\n",
        "  #print(KLd)\n",
        "  return CE,variational_beta*KLd, CE + variational_beta*KLd"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7LH-GQRW01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainVAE(VAE, latent_dims):\n",
        "  VAE.train()\n",
        "  x_train, x_target = generate_data(num=num)\n",
        "  inds = list(range(x_train.shape[0]))\n",
        "  N = x_train.shape[0] # 800\n",
        "  freq = num_epochs // 10 # floor division\n",
        "\n",
        "  loss_hist = []\n",
        "  x_train = Variable(torch.from_numpy(x_train))\n",
        "  x_target = Variable(torch.from_numpy(x_target))\n",
        "  for epoch in range(num_epochs):\n",
        "      inds = np.random.permutation(inds)\n",
        "      x_train = x_train[inds]\n",
        "      x_train = x_train.to(device)\n",
        "      x_target = x_target[inds]\n",
        "      x_target = x_target.to(device)\n",
        "      \n",
        "      loss = 0\n",
        "      CE = 0\n",
        "      KLd = 0\n",
        "      num_batches = N / batch_size\n",
        "      for b in range(0, N, batch_size):\n",
        "          #get the mini-batch\n",
        "          x_batch = x_train[b: b+batch_size]\n",
        "          x_target_batch = x_target[b: b+batch_size]\n",
        "          \n",
        "          #feed forward\n",
        "          batch_recon,latent_mu,latent_logvar = VAE(x=x_batch.float(),latent_dims = latent_dims)\n",
        "          \n",
        "          # Error\n",
        "          #Convert x_batch from OHE vectors to single scalar for target class, of each sample in batch \n",
        "          _, x_batch_targets = x_batch.max(dim=1)\n",
        "          train_CE, train_KLd, train_loss = vae_loss(batch_recon, x_batch_targets, latent_mu, latent_logvar)\n",
        "          #print(batch_recon.size())\n",
        "          #print(x_batch_targets.size())\n",
        "          loss += train_loss.item() / N # update epoch loss\n",
        "          CE += train_CE.item() / N \n",
        "          KLd += train_KLd.item() / N \n",
        "\n",
        "          #Backprop the error, compute the gradient\n",
        "          optimizer.zero_grad()\n",
        "          train_loss.backward()\n",
        "          \n",
        "          #update parameters based on gradient\n",
        "          optimizer.step()\n",
        "          \n",
        "      #Record loss per epoch        \n",
        "      loss_hist.append(loss)\n",
        "      \n",
        "      if epoch % freq == 0:\n",
        "          print()\n",
        "          print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\n",
        "          \n",
        "          #Test with all training data\n",
        "          VAE.eval()\n",
        "          train_recon, train_mu, train_logvar = VAE(x = x_train.float(),latent_dims=latent_dims)\n",
        "          _, x_targets = x_target.max(dim=1)\n",
        "          CE,KLd,test_loss = vae_loss(train_recon, x_targets, train_mu, train_logvar)\n",
        "          print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\n",
        "      \n",
        "  print(\"\\nTraining finished!\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCII451nHRR",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "Requires alternating between AB and BC samples where B is the same.\n",
        "\n",
        "Have a separate plate for each.\n",
        "In Bayesian network, need to learn P(B),P(A|B), P(C|B). \\\\\n",
        "In MRF need to learn factors $\\phi(A,B)$ and $\\phi(B,C)$.\n",
        "\n",
        "We want to query P(C|A), therefore at test time there will be no input to the B encoder.\n",
        "\n",
        "Do we need to incorporate the parition function Z? If want probabilities that sum to 1 then yes. But if just looking to have input into the decoders then normalizing isn't necessary?\n",
        "\n",
        "Koller Definition 4.3: \\\\\n",
        "$Z = \\sum_{AB,BC} \\phi(A,B) \\times \\phi(B,C)$ \\\\\n",
        "$P(A,B,C) = \\frac{1}{Z} \\phi(A,B) \\times \\phi(B,C)$ \n",
        "\n",
        "To learn $\\phi(A,B)$ where X = A and Y=B, need to re-construct A and B, have separate loss terms for the A decoder and the B decoder and backpropogate to learn the mean vectors, variance matrices and covariance matrices.\n",
        "\n",
        "Need to work in log-space for numerical stability.\n",
        "\n",
        "Assume the A encoder outputs $\\mu_A, \\Sigma_{AA}$ and the B encoder outputs $\\mu_B, \\Sigma_{BB}$.\n",
        "\n",
        "The latent variables have structure by learning $\\Sigma_{AB}, \\Sigma_{BA} = \\Sigma_{AB}^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Re5YHgVF-q",
        "colab_type": "text"
      },
      "source": [
        "Koller Equation 7.3: \\\\\n",
        "$P(X,Y) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_X \\\\ \\mu_Y \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{array} \\right] \\right) $ \n",
        "\n",
        "From Koller Theorem 7.4: \\\\\n",
        "$P(Y|X) = Normal (\\beta_0 + \\beta^TX, \\sigma^2)$ \\\\\n",
        "such that \\\\\n",
        "$\\beta_0 = \\mu_Y - \\Sigma_{YX} \\Sigma^{-1}_{XX}\\mu_X$ \\\\\n",
        "$\\beta = \\Sigma^{-1}_{XX} \\Sigma_{YX}$ \\\\\n",
        "$\\sigma^2 = \\Sigma_{YY} - \\Sigma_{YX}\\Sigma^{-1}_{XX}\\Sigma_{XY}$\n",
        "\n",
        "which is equivalent to the Matrix Cookbook (353 and 354):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjRUnGgjnIvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "e7d97961-3c5c-4390-dd49-d2f50c9e45a6"
      },
      "source": [
        "# Focus on just AB Plate for now\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "VAE = VariationalAutoencoder(latent_dims=3)\n",
        "VAE = VAE.to(device)\n",
        "num_params = sum(p.numel() for p in VAE.parameters() if p.requires_grad)\n",
        "print(VAE.parameters)\n",
        "print(\"Number of parameters: %d\" % num_params) #8*3 + 3 = 27, 3*8 + 8 = 32 3*3+3 = 12 *2 = 24, 27+32+24=83\n",
        "\n",
        "# optimizer object\n",
        "optimizer = torch.optim.Adam(params = VAE.parameters(), lr = learning_rate)\n",
        "\n",
        "trainVAE(VAE, latent_dims=3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.parameters of VariationalAutoencoder(\n",
            "  (fc1): Linear(in_features=8, out_features=3, bias=True)\n",
            "  (fc_mu): Linear(in_features=3, out_features=3, bias=True)\n",
            "  (fc_logvar): Linear(in_features=3, out_features=3, bias=True)\n",
            "  (fc_out): Linear(in_features=3, out_features=8, bias=True)\n",
            ")>\n",
            "Number of parameters: 83\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b23d19a97fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrainVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-cf33599c3c59>\u001b[0m in \u001b[0;36mtrainVAE\u001b[0;34m(VAE, latent_dims)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrainVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mVAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 800\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrqYmOIxeZvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}