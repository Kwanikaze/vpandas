{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRF_VAE",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPcQLyyEXUeVTuQgO58ArLc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwanikaze/vpandas/blob/master/MRF_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaO7CHX93gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNkadXIh0gD",
        "colab_type": "text"
      },
      "source": [
        "# Load Data and Create Sample Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE259FbtK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create two datasets from global df that are one-hot encoded\n",
        "def OHE_sample(sample_df, features_to_OHE: list):\n",
        "  for feature in features_to_OHE:\n",
        "    feature_OHE = pd.get_dummies(prefix = feature,data= sample_df[feature])\n",
        "    sample_df = pd.concat([sample_df,feature_OHE],axis=1)\n",
        "  sample_df.drop(features_to_OHE,axis=1,inplace=True)\n",
        "  print(sample_df)\n",
        "  return sample_df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykDGUc_-Q2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "957f4e57-05c3-4104-aa03-57924a55af74"
      },
      "source": [
        "# Load global relation\n",
        "df = pd.read_csv(\"data_8.csv\")\n",
        "print(df.shape)\n",
        "\n",
        "#Create two datasets containing AB and BC\n",
        "num_samples = 1000\n",
        "sample1_df = df[['A','B']].sample(n=num_samples, random_state=2)\n",
        "print(sample1_df.head())\n",
        "sample2_df = df[['B','C']].sample(n=num_samples, random_state=3)\n",
        "print(sample2_df.head())\n",
        "\n",
        "# Make A,B,C inputs all 8 bits\n",
        "#Does data need to respect Gaussian distribution?\n",
        "#Could add noise so not exactly OHE: 0.01...0.9...0.01\n",
        "sample1_OHE = OHE_sample(sample1_df,['A','B'])\n",
        "sample2_OHE = OHE_sample(sample2_df,['B','C'])\n",
        "\n",
        "# Could onvert pandas dataframes to list of lists of lists\n",
        "# [ [[OHE A1],[OHE B1]], [[OHE A2],[OHE B2]], ...  ]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5056, 3)\n",
            "      A  B\n",
            "4333  7  6\n",
            "2638  6  4\n",
            "2254  4  4\n",
            "3116  5  5\n",
            "3998  6  6\n",
            "      B  C\n",
            "4616  7  6\n",
            "2276  4  6\n",
            "3448  5  4\n",
            "4064  6  5\n",
            "1204  2  3\n",
            "      A_0  A_1  A_2  A_3  A_4  A_5  A_6  ...  B_1  B_2  B_3  B_4  B_5  B_6  B_7\n",
            "4333    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2638    0    0    0    0    0    0    1  ...    0    0    0    1    0    0    0\n",
            "2254    0    0    0    0    1    0    0  ...    0    0    0    1    0    0    0\n",
            "3116    0    0    0    0    0    1    0  ...    0    0    0    0    1    0    0\n",
            "3998    0    0    0    0    0    0    1  ...    0    0    0    0    0    1    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "1857    0    1    0    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "3813    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "604     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "621     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1322    0    1    0    0    0    0    0  ...    0    1    0    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n",
            "      B_0  B_1  B_2  B_3  B_4  B_5  B_6  ...  C_1  C_2  C_3  C_4  C_5  C_6  C_7\n",
            "4616    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2276    0    0    0    0    1    0    0  ...    0    0    0    0    0    1    0\n",
            "3448    0    0    0    0    0    1    0  ...    0    0    0    1    0    0    0\n",
            "4064    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "1204    0    0    1    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "3358    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "1496    0    0    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4025    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "4689    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    1\n",
            "2155    0    0    0    1    0    0    0  ...    0    0    1    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWt2iUw9xE",
        "colab_type": "text"
      },
      "source": [
        "# Global Relation Bayesian Network Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZqS2rxNrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8deb3df0-d713-4e7f-9864-b7a5a2a9361c"
      },
      "source": [
        "!pip install pgmpy==0.1.9\n",
        "import pgmpy\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "def groundTruth(df,evidence):\n",
        "    \"\"\"\n",
        "    Extracts ground truth from global relation\n",
        "    \"\"\"\n",
        "    model = BayesianModel([('B', 'A'), ('B', 'C')])\n",
        "    model.fit(df)\n",
        "    nx.draw(model, with_labels=True)\n",
        "    plt.show()\n",
        "    print('\\n Global Relation Ground Truth')\n",
        "    #for var in model.nodes():\n",
        "    #    print(model.get_cpds(var))\n",
        "    inference = VariableElimination(model)\n",
        "    \n",
        "    #q = inference.query(variables=['A','B','C'])\n",
        "    #joint_prob = q.values.flatten()\n",
        "    #print(joint_prob)\n",
        "    #print('\\n P(A,B,C) \\n Ground Truth')\n",
        "    #print(q)\n",
        "    q = inference.query(variables=['C'], evidence=evidence)\n",
        "    print(q)\n",
        "\n",
        "print('\\n P(C|A=0) \\n Ground Truth')\n",
        "groundTruth(df,{'A':0})\n",
        "\n",
        "print('\\n P(C|B=0) \\n Ground Truth')\n",
        "groundTruth(df,{'B':0})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pgmpy==0.1.9 in /usr/local/lib/python3.6/dist-packages (0.1.9)\n",
            "\n",
            " P(C|A=0) \n",
            " Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : 100%|██████████| 1/1 [00:00<00:00, 325.72it/s]\n",
            "Eliminating: B: 100%|██████████| 1/1 [00:00<00:00, 442.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| C    |   phi(C) |\n",
            "+======+==========+\n",
            "| C(0) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(1) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(2) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(3) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(4) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(5) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(6) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(7) |   0.0000 |\n",
            "+------+----------+\n",
            "\n",
            " P(C|B=0) \n",
            " Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : 100%|██████████| 1/1 [00:00<00:00, 184.81it/s]\n",
            "Eliminating: A: 100%|██████████| 1/1 [00:00<00:00, 222.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| C    |   phi(C) |\n",
            "+======+==========+\n",
            "| C(0) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(1) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(2) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(3) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(4) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(5) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(6) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(7) |   0.0000 |\n",
            "+------+----------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3YIf_-iAm8",
        "colab_type": "text"
      },
      "source": [
        "# VAE-MRF Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UMLBM0iE4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VAE Parameters\n",
        "num = 8 # digits from 0 to 7\n",
        "latent_dims = 3 # Latent z_A,z_B,z_C all are all same dimension size\n",
        "num_epochs = 1000\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "variational_beta = 0.00001 #tuned"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FiF8-RkNLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder_MRF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1A = nn.Linear(num, latent_dims)\n",
        "        self.fc_muA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outA = nn.Linear(latent_dims,num)\n",
        "        \n",
        "        self.fc1B = nn.Linear(num, latent_dims)\n",
        "        self.fc_muB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outB = nn.Linear(latent_dims,num)\n",
        "\n",
        "        #Covariance: Sigma_{AB} = Sigma_{BA}^T\n",
        "        # Sigma_AB is the top right term\n",
        "        #self.covarianceAB = nn.Parameter(torch.zeros(latent_dims,latent_dims),requires_grad=True)\n",
        "        self.covarianceAB = torch.randn(size=(latent_dims,latent_dims))\n",
        "        self.covarianceAB = torch.nn.Parameter(self.covarianceAB,requires_grad=True)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        #self.covarianceAB = nn.Parameter(torch.rand(size=(latent_dims,latent_dims), requires_grad=True))\n",
        "        #print(self.covarianceAB)\n",
        "\n",
        "    def reparameterize(self, mu, logvar): #mu.size() = batch_size, 3\n",
        "        std = torch.exp(0.5*logvar) #batch_size,3 \n",
        "        eps = torch.randn_like(std) #batch_size,3\n",
        "        #print('eps size')\n",
        "        #print(eps.size())\n",
        "        return mu + eps*std # batch_size,3\n",
        "\n",
        "\n",
        "    # Conditional of Multivariate Gaussian: matrix cookbook 353 and 354\n",
        "    def conditional(self, muA, logvarA, muB, logvarB, z, attribute):\n",
        "        #Convert logvarA vector to diagonal matrix\n",
        "        logvarA = torch.exp(0.5*logvarA)\n",
        "        logvarB = torch.exp(0.5*logvarB)\n",
        "        covarianceA = torch.diag_embed(logvarA) #batch_size,3,3\n",
        "        covarianceB = torch.diag_embed(logvarB)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        muA = muA.unsqueeze(2)\n",
        "        muB = muB.unsqueeze(2)\n",
        "        z = z.unsqueeze(2)\n",
        "        if attribute == 'A':\n",
        "          mu_cond = muA + torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                    torch.inverse(covarianceB)),\n",
        "                                   (z - muB)) # z is zB\n",
        "          logvar_cond = covarianceA - torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                      torch.inverse(covarianceB)),\n",
        "                                             torch.transpose(self.covarianceAB,0,1))\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims) # regularization\n",
        "        elif attribute == 'B':\n",
        "          mu_cond = muB + torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1),\n",
        "                                                    torch.inverse(covarianceA)), \n",
        "                                       (z - muA)) # z is zA\n",
        "          logvar_cond = covarianceB - torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1), \n",
        "                                                              torch.inverse(covarianceA)),\n",
        "                                                 self.covarianceAB)\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims)\n",
        "        #print('mu_cond, logvar_cond, eps, xx size')\n",
        "        #print(mu_cond.size()) # 64x3x1\n",
        "        #print(mu_cond)\n",
        "        #print(logvar_cond.size()) #64x3x3\n",
        "        #print(logvar_cond)\n",
        "\n",
        "        # METHOD1: re-parameterization trick\n",
        "        eps = torch.randn_like(mu_cond) #64x3x1, 64x3x3 if use logvar_cond\n",
        "        #print(eps.size())\n",
        "        #xx = eps*logvar_cond #64x3x3\n",
        "        #xx = torch.matmul(logvar_cond,eps) #64x3x1\n",
        "        #print(xx.size())\n",
        "        sample = mu_cond + torch.matmul(logvar_cond,eps)\n",
        "        sample = sample.squeeze(2) #64x3\n",
        "\n",
        "        #METHOD 2 - random sampling, can't backprop\n",
        "        #mu_cond = mu_cond.squeeze(2)\n",
        "        #distrib = MultivariateNormal(loc=mu_cond, covariance_matrix=logvar_cond)\n",
        "        #sample = distrib.rsample() # 64x3\n",
        "        \n",
        "        #print(sample.size())\n",
        "        return sample\n",
        "        #return self.reparameterize(mu_cond, logvar_cond) # logvar_cond is not a diagonal covariance matrix\n",
        "        #VAE reparameterization trick with non-diagonal covariance?\n",
        "        #https://stats.stackexchange.com/questions/388620/variational-autoencoder-and-covariance-matrix\n",
        "\n",
        "    def encode(self, x, attribute):\n",
        "        if attribute == 'A':\n",
        "          h1 = torch.sigmoid(self.fc1A(x))\n",
        "          return self.fc_muA(h1), self.fc_logvarA(h1)\n",
        "        elif attribute == 'B':\n",
        "          h1 = torch.sigmoid(self.fc1B(x))\n",
        "          return self.fc_muB(h1), self.fc_logvarB(h1)\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "    def decode(self, z, attribute):\n",
        "        if z.size()[0] == latent_dims: #resize from [3] to [1,3] if fed only a single sample\n",
        "            z = z.view(1, latent_dims)\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        if attribute == 'A':\n",
        "          reconA = softmax(self.fc_outA(z))\n",
        "          return reconA\n",
        "        elif attribute == 'B':\n",
        "          reconB = softmax(self.fc_outB(z))\n",
        "          return reconB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "    \n",
        "    def forward(self, xA, xB, attribute):\n",
        "        muA, logvarA = self.encode(xA, attribute='A') #logvar is size [64,3]\n",
        "        muB, logvarB = self.encode(xB, attribute='B')\n",
        "        if attribute == 'A':\n",
        "          zB = self.reparameterize(muB, logvarB)\n",
        "          zA = self.conditional(muA, logvarA, muB, logvarB, zB, attribute)\n",
        "          return self.decode(zA,attribute), muA, logvarA\n",
        "        elif attribute == 'B':\n",
        "          zA = self.reparameterize(muA, logvarA)\n",
        "          zB = self.conditional(muA, logvarA, muB, logvarB, zA, attribute)\n",
        "          return self.decode(zB,attribute), muB, logvarB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "    def forward_single_attribute(self, x, attribute):\n",
        "        mu, logvar = self.encode(x,attribute)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z, attribute), mu, logvar\n",
        "\n",
        "def vae_loss(batch_recon, batch_targets, mu, logvar):\n",
        "  #print('batch_targets, batch_recon size')\n",
        "  #print(batch_targets.size())\n",
        "  #print(batch_recon.size())\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  CE = criterion(batch_recon, batch_targets)\n",
        "  #print(CE)\n",
        "  KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\n",
        "  #print(KLd)\n",
        "  return CE,variational_beta*KLd, CE + variational_beta*KLd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Re5YHgVF-q",
        "colab_type": "text"
      },
      "source": [
        "Koller Equation 7.3: \\\\\n",
        "$P(X,Y) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_X \\\\ \\mu_Y \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{array} \\right] \\right) $ \n",
        "\n",
        "From Koller Theorem 7.4: \\\\\n",
        "$P(Y|X) = Normal (\\beta_0 + \\beta^TX, \\sigma^2)$ \\\\\n",
        "such that \\\\\n",
        "$\\beta_0 = \\mu_Y - \\Sigma_{YX} \\Sigma^{-1}_{XX}\\mu_X$ \\\\\n",
        "$\\beta = \\Sigma^{-1}_{XX} \\Sigma_{YX}$ \\\\\n",
        "$\\sigma^2 = \\Sigma_{YY} - \\Sigma_{YX}\\Sigma^{-1}_{XX}\\Sigma_{XY}$\n",
        "\n",
        "which is equivalent to the Matrix Cookbook (353 and 354).\n",
        "\n",
        "A symmetric matrix is positive definite if:\n",
        "\n",
        "- all the diagonal entries are positive, and\n",
        "- each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7LH-GQRW01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainVAE(VAE):\n",
        "  VAE.train() #set model mode to train\n",
        "  xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "  xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "  #print(xA.shape)\n",
        "\n",
        "  #sample2_OHE when do BC plate\n",
        "  \n",
        "  indsA = list(range(xA.shape[0]))\n",
        "  indsB = list(range(xB.shape[0]))\n",
        "  N = num_samples # 1000\n",
        "  freq = num_epochs // 10 # floor division\n",
        "\n",
        "  loss_hist = []\n",
        "  xA = Variable(torch.from_numpy(xA))\n",
        "  xB = Variable(torch.from_numpy(xB))\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "      #print('epoch' + str(epoch))\n",
        "      indsA = np.random.permutation(indsA)\n",
        "      xA = xA[indsA]\n",
        "      xA = xA.to(device)\n",
        "      indsB = np.random.permutation(indsB)\n",
        "      xB = xB[indsB]\n",
        "      xB = xB.to(device)\n",
        "      \n",
        "      loss = 0\n",
        "      CE = 0\n",
        "      KLd = 0\n",
        "      num_batches = N / batch_size\n",
        "      for b in range(0, N, batch_size):\n",
        "          #get the mini-batch\n",
        "          x_batchA = xA[b: b+batch_size]\n",
        "          x_batchB = xB[b: b+batch_size]\n",
        "          \n",
        "          #feed forward\n",
        "          batch_reconA,latent_muA,latent_logvarA = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='A')\n",
        "          batch_reconB,latent_muB,latent_logvarB = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='B')\n",
        "          #print('batch_recon size')\n",
        "          #print(batch_reconA.size())\n",
        "          # Error\n",
        "          #Convert x_batchA and x_batchB from OHE vectors to single scalar\n",
        "          # max returns index location of max value in each sample of batch \n",
        "          _, xA_batch_targets = x_batchA.max(dim=1)\n",
        "          _, xB_batch_targets = x_batchB.max(dim=1)\n",
        "          train_CE_A, train_KLd_A, train_loss_A = vae_loss(batch_reconA, xA_batch_targets, latent_muA, latent_logvarA)\n",
        "          train_CE_B, train_KLd_B, train_loss_B = vae_loss(batch_reconB, xB_batch_targets, latent_muB, latent_logvarB)\n",
        "          #print(batch_reconA.size())\n",
        "          #print(xA_batch_targets.size())\n",
        "          loss += train_loss_A.item() / N # update epoch loss\n",
        "          loss += train_loss_B.item() / N\n",
        "          CE += train_CE_A.item() / N\n",
        "          CE += train_CE_B.item() / N \n",
        "          KLd += train_KLd_A.item() / N\n",
        "          KLd += train_KLd_B.item() / N\n",
        "\n",
        "          #Backprop the error, compute the gradient\n",
        "          optimizer.zero_grad()\n",
        "          train_loss = train_loss_A + train_loss_B\n",
        "          train_loss.backward()\n",
        "          \n",
        "          #update parameters based on gradient\n",
        "          optimizer.step()\n",
        "          \n",
        "      #Record loss per epoch        \n",
        "      loss_hist.append(loss)\n",
        "      \n",
        "      if epoch % freq == 0:\n",
        "          print(VAE.covarianceAB)\n",
        "          print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\n",
        "          \n",
        "          #Test with all training data\n",
        "          VAE.eval()\n",
        "          train_reconA, train_muA, train_logvarA = VAE.forward(xA.float(),xB.float(), attribute='A')\n",
        "          train_reconB, train_muB, train_logvarB = VAE.forward(xA.float(),xB.float(), attribute='B')\n",
        "          _, xA_targets = xA.max(dim=1)\n",
        "          _, xB_targets = xB.max(dim=1)\n",
        "          CE_A,KLd_A,test_loss_A = vae_loss(train_reconA, xA_targets, train_muA, train_logvarA)\n",
        "          CE_B,KLd_B,test_loss_B = vae_loss(train_reconB, xB_targets, train_muB, train_logvarB)\n",
        "\n",
        "          CE = CE_A + CE_B\n",
        "          Kld = KLd_A + KLd_B\n",
        "          test_loss = test_loss_A + test_loss_B\n",
        "          print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\n",
        "      \n",
        "  print(\"\\nTraining finished!\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCII451nHRR",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "Requires alternating between AB and BC samples where B is the same. What if B is not the same in both datasets? How to train?\n",
        "\n",
        "Have a separate plate for each.\n",
        "In Bayesian network, need to learn P(B),P(A|B), P(C|B). \\\\\n",
        "In MRF need to learn factors $\\phi(A,B)$ and $\\phi(B,C)$.\n",
        "\n",
        "We want to query P(C|A), therefore at test time there will be no input to the B encoder.\n",
        "\n",
        "Do we need to incorporate the parition function Z? If want probabilities that sum to 1 then yes. But if just looking to have input into the decoders then normalizing isn't necessary?\n",
        "\n",
        "Koller Definition 4.3: \\\\\n",
        "$Z = \\sum_{AB,BC} \\phi(A,B) \\times \\phi(B,C)$ \\\\\n",
        "$P(A,B,C) = \\frac{1}{Z} \\phi(A,B) \\times \\phi(B,C)$ \n",
        "\n",
        "To learn $\\phi(A,B)$ where X = A and Y=B, need to re-construct A and B, have separate loss terms for the A decoder and the B decoder and backpropogate to learn the mean vectors, variance matrices and covariance matrices.\n",
        "\n",
        "Need to work in log-space for numerical stability.\n",
        "\n",
        "Assume the A encoder outputs $\\mu_A, \\Sigma_{AA}$ and the B encoder outputs $\\mu_B, \\Sigma_{BB}$.\n",
        "\n",
        "The latent variables have structure by learning $\\Sigma_{AB}, \\Sigma_{BA} = \\Sigma_{AB}^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjRUnGgjnIvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "e1c2c9db-a597-4cb5-ccb8-901159a2935f"
      },
      "source": [
        "# Focus on just AB Plate for now\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "VAE = VariationalAutoencoder_MRF()\n",
        "VAE = VAE.to(device)\n",
        "num_params = sum(p.numel() for p in VAE.parameters() if p.requires_grad)\n",
        "\n",
        "#for param in VAE.parameters():\n",
        "#    print(type(param.data), param.size())\n",
        "#print(list(VAE.parameters()))\n",
        "#print(VAE.parameters)\n",
        "print(\"Number of parameters: %d\" % num_params) #8*3 + 3 = 27, 3*8 + 8 = 32 3*3+3 = 12 *2 = 24, 27+32+24=83\n",
        "\n",
        "# optimizer object\n",
        "optimizer = torch.optim.Adam(params = VAE.parameters(), lr = learning_rate)\n",
        "\n",
        "trainVAE(VAE)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 175\n",
            "Parameter containing:\n",
            "tensor([[ 0.7059,  0.5864,  0.4040],\n",
            "        [-0.1584,  0.6646, -0.0772],\n",
            "        [ 1.1887,  2.6407, -1.4515]], requires_grad=True)\n",
            "Epoch 1/1000\t CE: 0.06757, KLd: 0.00001, Train loss=0.06758\t\t CE: 4.21662, KLd: 0.00001, Test loss: 4.22715Parameter containing:\n",
            "tensor([[ 0.3183,  0.3620,  0.2335],\n",
            "        [-0.2703,  0.6237,  0.0354],\n",
            "        [ 0.7296,  1.8446, -1.0639]], requires_grad=True)\n",
            "Epoch 101/1000\t CE: 0.06049, KLd: 0.00008, Train loss=0.06057\t\t CE: 3.76424, KLd: 0.00008, Test loss: 3.84367Parameter containing:\n",
            "tensor([[ 0.0818,  0.0788,  0.1162],\n",
            "        [-0.1212,  0.1707,  0.0593],\n",
            "        [ 0.1685,  0.7646, -0.2825]], requires_grad=True)\n",
            "Epoch 201/1000\t CE: 0.04447, KLd: 0.00019, Train loss=0.04466\t\t CE: 2.77225, KLd: 0.00019, Test loss: 2.96676Parameter containing:\n",
            "tensor([[ 0.0958,  0.0024,  0.0506],\n",
            "        [-0.0325,  0.1431,  0.0519],\n",
            "        [ 0.1347,  0.2782, -0.2311]], requires_grad=True)\n",
            "Epoch 301/1000\t CE: 0.04104, KLd: 0.00025, Train loss=0.04130\t\t CE: 2.56463, KLd: 0.00025, Test loss: 2.81656Parameter containing:\n",
            "tensor([[ 0.1099, -0.0070,  0.0547],\n",
            "        [-0.0238,  0.1503,  0.0484],\n",
            "        [ 0.1337,  0.1854, -0.2203]], requires_grad=True)\n",
            "Epoch 401/1000\t CE: 0.04084, KLd: 0.00025, Train loss=0.04109\t\t CE: 2.55262, KLd: 0.00025, Test loss: 2.80426Parameter containing:\n",
            "tensor([[ 0.1713, -0.0115,  0.0841],\n",
            "        [-0.0276,  0.1855,  0.0669],\n",
            "        [ 0.1476,  0.1657, -0.2240]], requires_grad=True)\n",
            "Epoch 501/1000\t CE: 0.04081, KLd: 0.00022, Train loss=0.04103\t\t CE: 2.55058, KLd: 0.00022, Test loss: 2.77167Parameter containing:\n",
            "tensor([[ 0.2134, -0.0179,  0.1292],\n",
            "        [-0.0433,  0.2082,  0.1089],\n",
            "        [ 0.1696,  0.1790, -0.2144]], requires_grad=True)\n",
            "Epoch 601/1000\t CE: 0.04080, KLd: 0.00019, Train loss=0.04099\t\t CE: 2.54995, KLd: 0.00019, Test loss: 2.73493Parameter containing:\n",
            "tensor([[ 0.2082, -0.0098,  0.1474],\n",
            "        [-0.0765,  0.2032,  0.1240],\n",
            "        [ 0.1607,  0.2165, -0.1851]], requires_grad=True)\n",
            "Epoch 701/1000\t CE: 0.04079, KLd: 0.00016, Train loss=0.04095\t\t CE: 2.54956, KLd: 0.00016, Test loss: 2.70693Parameter containing:\n",
            "tensor([[ 0.1993,  0.0030,  0.1448],\n",
            "        [-0.1100,  0.1769,  0.1124],\n",
            "        [ 0.1360,  0.2271, -0.1643]], requires_grad=True)\n",
            "Epoch 801/1000\t CE: 0.04079, KLd: 0.00014, Train loss=0.04092\t\t CE: 2.54922, KLd: 0.00014, Test loss: 2.68581Parameter containing:\n",
            "tensor([[ 0.1727,  0.0332,  0.1656],\n",
            "        [-0.1413,  0.1531,  0.1222],\n",
            "        [ 0.1091,  0.2247, -0.1440]], requires_grad=True)\n",
            "Epoch 901/1000\t CE: 0.04078, KLd: 0.00012, Train loss=0.04091\t\t CE: 2.54888, KLd: 0.00012, Test loss: 2.67186\n",
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKiDijtuUHt",
        "colab_type": "text"
      },
      "source": [
        "## Check encoder, decoders work on their own\n",
        "- could also train encoder and decoders on their own?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrqYmOIxeZvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "91836b67-1353-47d8-c61d-432d2ffa947b"
      },
      "source": [
        "x_test = np.eye(num)[np.arange(num)]                        # Test data (one-hot encoded)\n",
        "x_test = Variable(torch.from_numpy(x_test))\n",
        "x_test = x_test.to(device)\n",
        "\n",
        "print(\"Print prediction results for A only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE.forward_single_attribute(x=x.float(), attribute='A')[0].cpu().detach().numpy(),decimals=2)))\n",
        "print(\"Print prediction results for B only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE.forward_single_attribute(x=x.float(), attribute='B')[0].cpu().detach().numpy(),decimals=2)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Print prediction results for A only:\n",
            "\tInput: [1. 0. 0. 0. 0. 0. 0. 0.] \t Output: [[1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 1. 0. 0. 0. 0. 0. 0.] \t Output: [[0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 1. 0. 0. 0. 0. 0.] \t Output: [[0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 1. 0. 0. 0. 0.] \t Output: [[0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 1. 0. 0. 0.] \t Output: [[0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 0. 1. 0. 0.] \t Output: [[0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 0. 0. 1. 0.] \t Output: [[0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 0. 0. 0. 1.] \t Output: [[0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "Print prediction results for B only:\n",
            "\tInput: [1. 0. 0. 0. 0. 0. 0. 0.] \t Output: [[1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 1. 0. 0. 0. 0. 0. 0.] \t Output: [[0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 1. 0. 0. 0. 0. 0.] \t Output: [[0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 1. 0. 0. 0. 0.] \t Output: [[0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 1. 0. 0. 0.] \t Output: [[0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 0. 1. 0. 0.] \t Output: [[0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 0. 0. 1. 0.] \t Output: [[0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "\tInput: [0. 0. 0. 0. 0. 0. 0. 1.] \t Output: [[0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vBTljFA9YMa",
        "colab_type": "text"
      },
      "source": [
        "# Query P(A|B), P(B|A)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuZdoZXu9biT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgk-LlXB64eb",
        "colab_type": "text"
      },
      "source": [
        "# To Do\n",
        "\n",
        "- Check input output makes sense\n",
        "- Query P(A|B), P(B|A)\n",
        "- Add BC Plate\n",
        "- Check if training on only A improves performance\n",
        "- Formalize in Overleaf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ_f2Kmg7H9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}