{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRF_VAE",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMxCIyV2mwxZ/iosA7g+N6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwanikaze/vpandas/blob/master/MRF_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaO7CHX93gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNkadXIh0gD",
        "colab_type": "text"
      },
      "source": [
        "# Load Data and Create Sample Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE259FbtK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create two datasets from global df that are one-hot encoded\n",
        "def OHE_sample(sample_df, features_to_OHE: list):\n",
        "  for feature in features_to_OHE:\n",
        "    feature_OHE = pd.get_dummies(prefix = feature,data= sample_df[feature])\n",
        "    sample_df = pd.concat([sample_df,feature_OHE],axis=1)\n",
        "  sample_df.drop(features_to_OHE,axis=1,inplace=True)\n",
        "  print(sample_df)\n",
        "  return sample_df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykDGUc_-Q2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "24e7122a-d3d0-47fa-a7ea-2bcfe50e4754"
      },
      "source": [
        "# Load global relation\n",
        "df = pd.read_csv(\"data_8.csv\")\n",
        "print(df.shape)\n",
        "\n",
        "#Create two datasets containing AB and BC\n",
        "num_samples = 1000\n",
        "sample1_df = df[['A','B']].sample(n=num_samples, random_state=2)\n",
        "print(sample1_df.head())\n",
        "sample2_df = df[['B','C']].sample(n=num_samples, random_state=3)\n",
        "print(sample2_df.head())\n",
        "\n",
        "# Make A,B,C inputs all 8 bits\n",
        "#Does data need to respect Gaussian distribution?\n",
        "#Could add noise so not exactly OHE: 0.01...0.9...0.01\n",
        "sample1_OHE = OHE_sample(sample1_df,['A','B'])\n",
        "sample2_OHE = OHE_sample(sample2_df,['B','C'])\n",
        "\n",
        "# Could onvert pandas dataframes to list of lists of lists\n",
        "# [ [[OHE A1],[OHE B1]], [[OHE A2],[OHE B2]], ...  ]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5056, 3)\n",
            "      A  B\n",
            "4333  7  6\n",
            "2638  6  4\n",
            "2254  4  4\n",
            "3116  5  5\n",
            "3998  6  6\n",
            "      B  C\n",
            "4616  7  6\n",
            "2276  4  6\n",
            "3448  5  4\n",
            "4064  6  5\n",
            "1204  2  3\n",
            "      A_0  A_1  A_2  A_3  A_4  A_5  A_6  ...  B_1  B_2  B_3  B_4  B_5  B_6  B_7\n",
            "4333    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2638    0    0    0    0    0    0    1  ...    0    0    0    1    0    0    0\n",
            "2254    0    0    0    0    1    0    0  ...    0    0    0    1    0    0    0\n",
            "3116    0    0    0    0    0    1    0  ...    0    0    0    0    1    0    0\n",
            "3998    0    0    0    0    0    0    1  ...    0    0    0    0    0    1    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "1857    0    1    0    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "3813    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "604     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "621     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1322    0    1    0    0    0    0    0  ...    0    1    0    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n",
            "      B_0  B_1  B_2  B_3  B_4  B_5  B_6  ...  C_1  C_2  C_3  C_4  C_5  C_6  C_7\n",
            "4616    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2276    0    0    0    0    1    0    0  ...    0    0    0    0    0    1    0\n",
            "3448    0    0    0    0    0    1    0  ...    0    0    0    1    0    0    0\n",
            "4064    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "1204    0    0    1    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "3358    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "1496    0    0    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4025    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "4689    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    1\n",
            "2155    0    0    0    1    0    0    0  ...    0    0    1    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWt2iUw9xE",
        "colab_type": "text"
      },
      "source": [
        "# Global Relation Bayesian Network Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZqS2rxNrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "152c7bb9-c449-46c5-b8d7-199df0ffea6a"
      },
      "source": [
        "!pip install pgmpy==0.1.9\n",
        "import pgmpy\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "def groundTruth(df,evidence):\n",
        "    \"\"\"\n",
        "    Extracts ground truth from global relation\n",
        "    \"\"\"\n",
        "    model = BayesianModel([('B', 'A'), ('B', 'C')])\n",
        "    model.fit(df)\n",
        "    nx.draw(model, with_labels=True)\n",
        "    plt.show()\n",
        "    print('\\n Global Relation Ground Truth')\n",
        "    #for var in model.nodes():\n",
        "    #    print(model.get_cpds(var))\n",
        "    inference = VariableElimination(model)\n",
        "    \n",
        "    #q = inference.query(variables=['A','B','C'])\n",
        "    #joint_prob = q.values.flatten()\n",
        "    #print(joint_prob)\n",
        "    #print('\\n P(A,B,C) \\n Ground Truth')\n",
        "    #print(q)\n",
        "    q = inference.query(variables=['C'], evidence=evidence)\n",
        "    print(q)\n",
        "\n",
        "print('\\n P(C|A=0) \\n Ground Truth')\n",
        "groundTruth(df,{'A':0})\n",
        "\n",
        "print('\\n P(C|B=0) \\n Ground Truth')\n",
        "groundTruth(df,{'B':0})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pgmpy==0.1.9 in /usr/local/lib/python3.6/dist-packages (0.1.9)\n",
            "\n",
            " P(C|A=0) \n",
            " Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : 100%|██████████| 1/1 [00:00<00:00, 606.90it/s]\n",
            "Eliminating: B: 100%|██████████| 1/1 [00:00<00:00, 315.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| C    |   phi(C) |\n",
            "+======+==========+\n",
            "| C(0) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(1) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(2) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(3) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(4) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(5) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(6) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(7) |   0.0000 |\n",
            "+------+----------+\n",
            "\n",
            " P(C|B=0) \n",
            " Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : 100%|██████████| 1/1 [00:00<00:00, 200.32it/s]\n",
            "Eliminating: A: 100%|██████████| 1/1 [00:00<00:00, 221.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| C    |   phi(C) |\n",
            "+======+==========+\n",
            "| C(0) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(1) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(2) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(3) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(4) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(5) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(6) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(7) |   0.0000 |\n",
            "+------+----------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3YIf_-iAm8",
        "colab_type": "text"
      },
      "source": [
        "# VAE-MRF Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UMLBM0iE4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VAE Parameters\n",
        "num = 8 # digits from 0 to 7\n",
        "latent_dims = 3 # Latent z_A,z_B,z_C all are all same dimension size\n",
        "num_epochs = 1500\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "variational_beta = 0.00001 #tuned"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FiF8-RkNLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder_MRF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1A = nn.Linear(num, latent_dims)\n",
        "        self.fc_muA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outA = nn.Linear(latent_dims,num)\n",
        "        \n",
        "        self.fc1B = nn.Linear(num, latent_dims)\n",
        "        self.fc_muB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outB = nn.Linear(latent_dims,num)\n",
        "\n",
        "        #Covariance: Sigma_{AB} = Sigma_{BA}^T\n",
        "        # Sigma_AB is the top right term\n",
        "        #self.covarianceAB = nn.Parameter(torch.zeros(latent_dims,latent_dims),requires_grad=True)\n",
        "        self.covarianceAB = torch.randn(size=(latent_dims,latent_dims))\n",
        "        self.covarianceAB = torch.nn.Parameter(self.covarianceAB,requires_grad=True)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        #self.covarianceAB = nn.Parameter(torch.rand(size=(latent_dims,latent_dims), requires_grad=True))\n",
        "        #print(self.covarianceAB)\n",
        "\n",
        "    def reparameterize(self, mu, logvar): #mu.size() = batch_size, 3\n",
        "        std = torch.exp(0.5*logvar) #batch_size,3 \n",
        "        eps = torch.randn_like(std) #batch_size,3\n",
        "        #print('eps size')\n",
        "        #print(eps.size())\n",
        "        return mu + eps*std # batch_size,3\n",
        "\n",
        "\n",
        "    # Conditional of Multivariate Gaussian: matrix cookbook 353 and 354\n",
        "    def conditional(self, muA, logvarA, muB, logvarB, z, attribute):\n",
        "        #Convert logvarA vector to diagonal matrix\n",
        "        logvarA = torch.exp(0.5*logvarA)\n",
        "        logvarB = torch.exp(0.5*logvarB)\n",
        "        covarianceA = torch.diag_embed(logvarA) #batch_size,3,3\n",
        "        covarianceB = torch.diag_embed(logvarB)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        muA = muA.unsqueeze(2)\n",
        "        muB = muB.unsqueeze(2)\n",
        "        z = z.unsqueeze(2)\n",
        "        if attribute == 'A':\n",
        "          mu_cond = muA + torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                    torch.inverse(covarianceB)),\n",
        "                                   (z - muB)) # z is zB\n",
        "          logvar_cond = covarianceA - torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                      torch.inverse(covarianceB)),\n",
        "                                             torch.transpose(self.covarianceAB,0,1))\n",
        "          logvar_cond = logvar_cond + 20*torch.eye(latent_dims) # regularization\n",
        "        elif attribute == 'B':\n",
        "          mu_cond = muB + torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1),\n",
        "                                                    torch.inverse(covarianceA)), \n",
        "                                       (z - muA)) # z is zA\n",
        "          logvar_cond = covarianceB - torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1), \n",
        "                                                              torch.inverse(covarianceA)),\n",
        "                                                 self.covarianceAB)\n",
        "          logvar_cond = logvar_cond + 20*torch.eye(latent_dims)\n",
        "        #print('mu_cond, logvar_cond, eps, xx size')\n",
        "        #print(mu_cond.size()) # 64x3x1\n",
        "        #print(mu_cond)\n",
        "        #print(logvar_cond.size()) #64x3x3\n",
        "        #print(logvar_cond)\n",
        "\n",
        "        # METHOD1: re-parameterization trick\n",
        "        eps = torch.randn_like(mu_cond) #64x3x1, 64x3x3 if use logvar_cond\n",
        "        #print(eps.size())\n",
        "        #xx = eps*logvar_cond #64x3x3\n",
        "        #xx = torch.matmul(logvar_cond,eps) #64x3x1\n",
        "        #print(xx.size())\n",
        "        sample = mu_cond + torch.matmul(logvar_cond,eps)\n",
        "        sample = sample.squeeze(2) #64x3\n",
        "\n",
        "        #METHOD 2 - random sampling, can't backprop\n",
        "        #mu_cond = mu_cond.squeeze(2)\n",
        "        #distrib = MultivariateNormal(loc=mu_cond, covariance_matrix=logvar_cond)\n",
        "        #sample = distrib.rsample() # 64x3\n",
        "        \n",
        "        #print(sample.size())\n",
        "        return sample\n",
        "        #return self.reparameterize(mu_cond, logvar_cond) # logvar_cond is not a diagonal covariance matrix\n",
        "        #VAE reparameterization trick with non-diagonal covariance?\n",
        "        #https://stats.stackexchange.com/questions/388620/variational-autoencoder-and-covariance-matrix\n",
        "\n",
        "    def encode(self, x, attribute):\n",
        "        if attribute == 'A':\n",
        "          h1 = torch.sigmoid(self.fc1A(x))\n",
        "          return self.fc_muA(h1), self.fc_logvarA(h1)\n",
        "        elif attribute == 'B':\n",
        "          h1 = torch.sigmoid(self.fc1B(x))\n",
        "          return self.fc_muB(h1), self.fc_logvarB(h1)\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "    def decode(self, z, attribute):\n",
        "        if z.size()[0] == latent_dims: #resize from [3] to [1,3] if fed only a single sample\n",
        "            z = z.view(1, latent_dims)\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        if attribute == 'A':\n",
        "          reconA = softmax(self.fc_outA(z))\n",
        "          return reconA\n",
        "        elif attribute == 'B':\n",
        "          reconB = softmax(self.fc_outB(z))\n",
        "          return reconB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "    \n",
        "    def forward(self, xA, xB, attribute):\n",
        "        muA, logvarA = self.encode(xA, attribute='A') #logvar is size [64,3]\n",
        "        muB, logvarB = self.encode(xB, attribute='B')\n",
        "        if attribute == 'A':\n",
        "          zB = self.reparameterize(muB, logvarB)\n",
        "          zA = self.conditional(muA, logvarA, muB, logvarB, zB, attribute)\n",
        "          return self.decode(zA,attribute), muA, logvarA\n",
        "        elif attribute == 'B':\n",
        "          zA = self.reparameterize(muA, logvarA)\n",
        "          zB = self.conditional(muA, logvarA, muB, logvarB, zA, attribute)\n",
        "          return self.decode(zB,attribute), muB, logvarB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "def vae_loss(batch_recon, batch_targets, mu, logvar):\n",
        "  #print('batch_targets, batch_recon size')\n",
        "  #print(batch_targets.size())\n",
        "  #print(batch_recon.size())\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  CE = criterion(batch_recon, batch_targets)\n",
        "  #print(CE)\n",
        "  KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\n",
        "  #print(KLd)\n",
        "  return CE,variational_beta*KLd, CE + variational_beta*KLd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Re5YHgVF-q",
        "colab_type": "text"
      },
      "source": [
        "Koller Equation 7.3: \\\\\n",
        "$P(X,Y) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_X \\\\ \\mu_Y \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{array} \\right] \\right) $ \n",
        "\n",
        "From Koller Theorem 7.4: \\\\\n",
        "$P(Y|X) = Normal (\\beta_0 + \\beta^TX, \\sigma^2)$ \\\\\n",
        "such that \\\\\n",
        "$\\beta_0 = \\mu_Y - \\Sigma_{YX} \\Sigma^{-1}_{XX}\\mu_X$ \\\\\n",
        "$\\beta = \\Sigma^{-1}_{XX} \\Sigma_{YX}$ \\\\\n",
        "$\\sigma^2 = \\Sigma_{YY} - \\Sigma_{YX}\\Sigma^{-1}_{XX}\\Sigma_{XY}$\n",
        "\n",
        "which is equivalent to the Matrix Cookbook (353 and 354).\n",
        "\n",
        "A symmetric matrix is positive definite if:\n",
        "\n",
        "- all the diagonal entries are positive, and\n",
        "- each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7LH-GQRW01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainVAE(VAE):\n",
        "  VAE.train() #set model mode to train\n",
        "  xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "  xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "  #print(xA.shape)\n",
        "\n",
        "  #sample2_OHE when do BC plate\n",
        "  \n",
        "  indsA = list(range(xA.shape[0]))\n",
        "  indsB = list(range(xB.shape[0]))\n",
        "  N = num_samples # 1000\n",
        "  freq = num_epochs // 10 # floor division\n",
        "\n",
        "  loss_hist = []\n",
        "  xA = Variable(torch.from_numpy(xA))\n",
        "  xB = Variable(torch.from_numpy(xB))\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "      #print('epoch' + str(epoch))\n",
        "      indsA = np.random.permutation(indsA)\n",
        "      xA = xA[indsA]\n",
        "      xA = xA.to(device)\n",
        "      indsB = np.random.permutation(indsB)\n",
        "      xB = xB[indsB]\n",
        "      xB = xB.to(device)\n",
        "      \n",
        "      loss = 0\n",
        "      CE = 0\n",
        "      KLd = 0\n",
        "      num_batches = N / batch_size\n",
        "      for b in range(0, N, batch_size):\n",
        "          #get the mini-batch\n",
        "          x_batchA = xA[b: b+batch_size]\n",
        "          x_batchB = xB[b: b+batch_size]\n",
        "          \n",
        "          #feed forward\n",
        "          batch_reconA,latent_muA,latent_logvarA = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='A')\n",
        "          batch_reconB,latent_muB,latent_logvarB = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='B')\n",
        "          #print('batch_recon size')\n",
        "          #print(batch_reconA.size())\n",
        "          # Error\n",
        "          #Convert x_batchA and x_batchB from OHE vectors to single scalar\n",
        "          # max returns index location of max value in each sample of batch \n",
        "          _, xA_batch_targets = x_batchA.max(dim=1)\n",
        "          _, xB_batch_targets = x_batchB.max(dim=1)\n",
        "          train_CE_A, train_KLd_A, train_loss_A = vae_loss(batch_reconA, xA_batch_targets, latent_muA, latent_logvarA)\n",
        "          train_CE_B, train_KLd_B, train_loss_B = vae_loss(batch_reconB, xB_batch_targets, latent_muB, latent_logvarB)\n",
        "          #print(batch_reconA.size())\n",
        "          #print(xA_batch_targets.size())\n",
        "          loss += train_loss_A.item() / N # update epoch loss\n",
        "          loss += train_loss_B.item() / N\n",
        "          CE += train_CE_A.item() / N\n",
        "          CE += train_CE_B.item() / N \n",
        "          KLd += train_KLd_A.item() / N\n",
        "          KLd += train_KLd_B.item() / N\n",
        "\n",
        "          #Backprop the error, compute the gradient\n",
        "          optimizer.zero_grad()\n",
        "          train_loss = train_loss_A + train_loss_B\n",
        "          train_loss.backward()\n",
        "          \n",
        "          #update parameters based on gradient\n",
        "          optimizer.step()\n",
        "          \n",
        "      #Record loss per epoch        \n",
        "      loss_hist.append(loss)\n",
        "      \n",
        "      if epoch % freq == 0:\n",
        "          print(VAE.covarianceAB)\n",
        "          print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\n",
        "          \n",
        "          #Test with all training data\n",
        "          VAE.eval()\n",
        "          train_reconA, train_muA, train_logvarA = VAE.forward(xA.float(),xB.float(), attribute='A')\n",
        "          train_reconB, train_muB, train_logvarB = VAE.forward(xA.float(),xB.float(), attribute='B')\n",
        "          _, xA_targets = xA.max(dim=1)\n",
        "          _, xB_targets = xB.max(dim=1)\n",
        "          CE_A,KLd_A,test_loss_A = vae_loss(train_reconA, xA_targets, train_muA, train_logvarA)\n",
        "          CE_B,KLd_B,test_loss_B = vae_loss(train_reconB, xB_targets, train_muB, train_logvarB)\n",
        "\n",
        "          CE = CE_A + CE_B\n",
        "          Kld = KLd_A + KLd_B\n",
        "          test_loss = test_loss_A + test_loss_B\n",
        "          print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\n",
        "      \n",
        "  print(\"\\nTraining finished!\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCII451nHRR",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "Requires alternating between AB and BC samples where B is the same. What if B is not the same in both datasets? How to train?\n",
        "\n",
        "Have a separate plate for each.\n",
        "In Bayesian network, need to learn P(B),P(A|B), P(C|B). \\\\\n",
        "In MRF need to learn factors $\\phi(A,B)$ and $\\phi(B,C)$.\n",
        "\n",
        "We want to query P(C|A), therefore at test time there will be no input to the B encoder.\n",
        "\n",
        "Do we need to incorporate the parition function Z? If want probabilities that sum to 1 then yes. But if just looking to have input into the decoders then normalizing isn't necessary?\n",
        "\n",
        "Koller Definition 4.3: \\\\\n",
        "$Z = \\sum_{AB,BC} \\phi(A,B) \\times \\phi(B,C)$ \\\\\n",
        "$P(A,B,C) = \\frac{1}{Z} \\phi(A,B) \\times \\phi(B,C)$ \n",
        "\n",
        "To learn $\\phi(A,B)$ where X = A and Y=B, need to re-construct A and B, have separate loss terms for the A decoder and the B decoder and backpropogate to learn the mean vectors, variance matrices and covariance matrices.\n",
        "\n",
        "Need to work in log-space for numerical stability.\n",
        "\n",
        "Assume the A encoder outputs $\\mu_A, \\Sigma_{AA}$ and the B encoder outputs $\\mu_B, \\Sigma_{BB}$.\n",
        "\n",
        "The latent variables have structure by learning $\\Sigma_{AB}, \\Sigma_{BA} = \\Sigma_{AB}^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjRUnGgjnIvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "57dc0f6f-7c30-4076-8f09-bfb757873a7c"
      },
      "source": [
        "# Focus on just AB Plate for now\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "VAE = VariationalAutoencoder_MRF()\n",
        "VAE = VAE.to(device)\n",
        "num_params = sum(p.numel() for p in VAE.parameters() if p.requires_grad)\n",
        "\n",
        "#for param in VAE.parameters():\n",
        "#    print(type(param.data), param.size())\n",
        "#print(list(VAE.parameters()))\n",
        "#print(VAE.parameters)\n",
        "print(\"Number of parameters: %d\" % num_params) #8*3 + 3 = 27, 3*8 + 8 = 32 3*3+3 = 12 *2 = 24, 27+32+24=83\n",
        "\n",
        "# optimizer object\n",
        "optimizer = torch.optim.Adam(params = VAE.parameters(), lr = learning_rate)\n",
        "\n",
        "trainVAE(VAE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 175\n",
            "Parameter containing:\n",
            "tensor([[ 0.9364,  1.8984,  0.8099],\n",
            "        [-0.3927,  1.3259,  0.4021],\n",
            "        [-0.0144, -1.6432,  0.3048]], requires_grad=True)\n",
            "Epoch 1/1500\t CE: 0.06817, KLd: 0.00001, Train loss=0.06818\t\t CE: 4.26773, KLd: 0.00001, Test loss: 4.27894Parameter containing:\n",
            "tensor([[ 1.1438,  2.1451,  0.7492],\n",
            "        [-0.9573,  1.3344,  0.6471],\n",
            "        [-0.2362, -2.0696,  0.5494]], requires_grad=True)\n",
            "Epoch 151/1500\t CE: 0.06669, KLd: 0.00004, Train loss=0.06674\t\t CE: 4.15996, KLd: 0.00004, Test loss: 4.20248Parameter containing:\n",
            "tensor([[ 1.6278,  1.6855,  1.3468],\n",
            "        [-1.9290,  1.2504,  1.1126],\n",
            "        [ 0.2028, -1.9445,  0.8298]], requires_grad=True)\n",
            "Epoch 301/1500\t CE: 0.06156, KLd: 0.00023, Train loss=0.06178\t\t CE: 3.85009, KLd: 0.00023, Test loss: 4.07638Parameter containing:\n",
            "tensor([[ 1.4964,  1.4181,  1.2622],\n",
            "        [-1.8172,  0.9592,  1.0660],\n",
            "        [ 0.2374, -1.7516,  1.5133]], requires_grad=True)\n",
            "Epoch 451/1500\t CE: 0.05666, KLd: 0.00048, Train loss=0.05714\t\t CE: 3.54582, KLd: 0.00048, Test loss: 4.02306Parameter containing:\n",
            "tensor([[ 1.3457,  1.1969,  1.0962],\n",
            "        [-1.6783,  0.7978,  1.2127],\n",
            "        [ 0.2018, -1.4752,  1.5689]], requires_grad=True)\n",
            "Epoch 601/1500\t CE: 0.05187, KLd: 0.00077, Train loss=0.05264\t\t CE: 3.25629, KLd: 0.00077, Test loss: 4.02749"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKiDijtuUHt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrqYmOIxeZvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}