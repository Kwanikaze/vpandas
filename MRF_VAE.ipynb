{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRF_VAE",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgX7eJ745hMSUE3a2ngkms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwanikaze/vpandas/blob/master/MRF_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaO7CHX93gN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "8813e7ad-8ee1-4fde-e3c9-2538cc59e19d"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "!pip install pgmpy==0.1.9\n",
        "import pgmpy\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "!pip install -i https://test.pypi.org/simple/ PPandas==0.0.1.7.1\n",
        "!pip install python-intervals\n",
        "!pip install geopandas\n",
        "!pip install geovoronoi\n",
        "import ppandas\n",
        "from ppandas import PDataFrame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pgmpy==0.1.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/b1/18dfdfcb10dcce71fd39f8c6801407e9aebd953939682558a5317e4a021c/pgmpy-0.1.9-py3-none-any.whl (331kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 2.7MB/s eta 0:00:01\r\u001b[K     |███                             | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |████                            | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 51kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 71kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 81kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 163kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 174kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 184kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 235kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 245kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 256kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 266kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 276kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 317kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 327kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 337kB 4.3MB/s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNkadXIh0gD",
        "colab_type": "text"
      },
      "source": [
        "# Load Data and Create Sample Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE259FbtK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create two datasets from global df that are one-hot encoded\n",
        "def OHE_sample(sample_df, features_to_OHE: list):\n",
        "  for feature in features_to_OHE:\n",
        "    feature_OHE = pd.get_dummies(prefix = feature,data= sample_df[feature])\n",
        "    sample_df = pd.concat([sample_df,feature_OHE],axis=1)\n",
        "  sample_df.drop(features_to_OHE,axis=1,inplace=True)\n",
        "  print(sample_df)\n",
        "  return sample_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykDGUc_-Q2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load global relation\n",
        "df = pd.read_csv(\"data_8.csv\")\n",
        "print(df.shape)\n",
        "\n",
        "#Create two datasets containing AB and BC\n",
        "num_samples = 1000\n",
        "sample1_df = df[['A','B']].sample(n=num_samples, random_state=2)\n",
        "print(sample1_df.head())\n",
        "sample2_df = df[['B','C']].sample(n=num_samples, random_state=3)\n",
        "print(sample2_df.head())\n",
        "\n",
        "# Make A,B,C inputs all 8 bits\n",
        "#Could add noise so not exactly OHE: 0.01...0.9...0.01\n",
        "sample1_OHE = OHE_sample(sample1_df,['A','B'])\n",
        "sample2_OHE = OHE_sample(sample2_df,['B','C'])\n",
        "\n",
        "# Could onvert pandas dataframes to list of lists of lists\n",
        "# [ [[OHE A1],[OHE B1]], [[OHE A2],[OHE B2]], ...  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWt2iUw9xE",
        "colab_type": "text"
      },
      "source": [
        "# Global Relation Bayesian Network Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZqS2rxNrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def groundTruth(df,query_attribute,evidence):\n",
        "    \"\"\"\n",
        "    Extracts ground truth from global relation\n",
        "    \"\"\"\n",
        "    model = BayesianModel([('B', 'A'), ('B', 'C')])\n",
        "    model.fit(df)\n",
        "    nx.draw(model, with_labels=True)\n",
        "    plt.show()\n",
        "    print('\\n Global Relation Ground Truth')\n",
        "    #for var in model.nodes():\n",
        "    #    print(model.get_cpds(var))\n",
        "    inference = VariableElimination(model)\n",
        "    \n",
        "    #q = inference.query(variables=['A','B','C'])\n",
        "    #joint_prob = q.values.flatten()\n",
        "    #print(joint_prob)\n",
        "    #print('\\n P(A,B,C) \\n Ground Truth')\n",
        "    #print(q)\n",
        "    q = inference.query(variables=[query_attribute], evidence=evidence)\n",
        "    print(q)\n",
        "\n",
        "print('\\n P(B|A=0) \\n Ground Truth')\n",
        "groundTruth(df,query_attribute = 'B', evidence = {'A':0})\n",
        "\n",
        "print('\\n P(A|B=0) \\n Ground Truth')\n",
        "groundTruth(df,query_attribute = 'A', evidence = {'B':0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bTvWAZ9UARW",
        "colab_type": "text"
      },
      "source": [
        "# ppandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bto996MFUCnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def ppandas_query(sample1_df,sample2_df,num_samples,query_attribute,evidence):\n",
        "    pd1 = PDataFrame(['B'],sample1_df)\n",
        "    pd2 = PDataFrame(['B'],sample2_df)\n",
        "    pd_join = pd1.pjoin(pd2)\n",
        "    q = pd_join.query(['A','B','C'])\n",
        "    #print(\"\\n ppandas P(A,B,C) , n={} \\n \".format(num_samples))\n",
        "    #Re-arrange columns from CBA to ABC\n",
        "    cols = q.columns.tolist()\n",
        "    #print('the cols')\n",
        "    #print(cols)\n",
        "    #4th column rename to Probabability(A,B,C)\n",
        "    q = q.rename(columns={q.columns[3]:'Probability(A,B,C)'})\n",
        "    #Reorder columns\n",
        "    q = q[['A','B','C','Probability(A,B,C)']]\n",
        "    q= q.sort_values(by=['A','B','C'])\n",
        "    #print(q)\n",
        "    #Sort rows in dataframe by descending order\n",
        "    joint_prob_ppandas = q\n",
        "    #joint_prob_ppandas = q.values.flatten()[3::4] #start at 4th value(index 3), stepsize is 4\n",
        "    #print('joint')\n",
        "    #print(joint_prob_ppandas)\n",
        "    print(\"\\n ppandas P({}|{}) , n={} \\n \".format(query_attribute,evidence,num_samples))\n",
        "    q1 = pd_join.query([query_attribute],evidence_vars=evidence)\n",
        "    print(q1)\n",
        "    q1 = pd_join.map_query([query_attribute],evidence_vars=evidence)\n",
        "    query_C_result = -1\n",
        "    for _,value in q1.items():\n",
        "        query_C_result = value\n",
        "    #pd_join.visualise()\n",
        "    return joint_prob_ppandas,query_C_result\n",
        "\n",
        "\n",
        "joint_prob_ppandas, ppandas_C = ppandas_query(sample1_df,sample2_df,num_samples,query_attribute='B',evidence={'A':0})\n",
        "#print(ppandas_C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3YIf_-iAm8",
        "colab_type": "text"
      },
      "source": [
        "# VAE-MRF Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UMLBM0iE4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VAE Parameters\n",
        "num = 8 # digits from 0 to 7\n",
        "latent_dims = 3 # Latent z_A,z_B,z_C all are all same dimension size\n",
        "num_epochs = 300\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "variational_beta = 0.00001 #tuned 0.00001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FiF8-RkNLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder_MRF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1A = nn.Linear(num, latent_dims)\n",
        "        self.fc_muA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outA = nn.Linear(latent_dims,num)\n",
        "        \n",
        "        self.fc1B = nn.Linear(num, latent_dims)\n",
        "        self.fc_muB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outB = nn.Linear(latent_dims,num)\n",
        "\n",
        "        #Covariance: Sigma_{AB} = Sigma_{BA}^T\n",
        "        # Sigma_AB is the top right term\n",
        "        #self.covarianceAB = nn.Parameter(torch.zeros(latent_dims,latent_dims),requires_grad=True)\n",
        "        self.covarianceAB = torch.randn(size=(latent_dims,latent_dims))\n",
        "        self.covarianceAB = torch.nn.Parameter(self.covarianceAB,requires_grad=True)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        #self.covarianceAB = nn.Parameter(torch.rand(size=(latent_dims,latent_dims), requires_grad=True))\n",
        "        #print(self.covarianceAB)\n",
        "\n",
        "    def reparameterize(self, mu, logvar): #mu.size() = batch_size, 3\n",
        "        std = torch.exp(0.5*logvar) #batch_size,3 # e^{0.5log(\\Sigma^2)} = \\Sigma\n",
        "        eps = torch.randn_like(std) #batch_size,3\n",
        "        return mu + eps*std # batch_size,3\n",
        "\n",
        "\n",
        "    # Conditional of Multivariate Gaussian: matrix cookbook 353 and 354\n",
        "    def conditional(self, muA, logvarA, muB, logvarB, z, attribute):\n",
        "        #Convert logvarA vector to diagonal matrix\n",
        "        #log-space for numerical stability.\n",
        "        logvarA = torch.exp(0.5*logvarA)\n",
        "        logvarB = torch.exp(0.5*logvarB)\n",
        "        covarianceA = torch.diag_embed(logvarA) #batch_size,3,3\n",
        "        covarianceB = torch.diag_embed(logvarB)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        muA = muA.unsqueeze(2)\n",
        "        muB = muB.unsqueeze(2)\n",
        "        z = z.unsqueeze(2)\n",
        "        if attribute == 'A':\n",
        "          mu_cond = muA + torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                    torch.inverse(covarianceB)),\n",
        "                                   (z - muB)) # z is zB\n",
        "          logvar_cond = covarianceA - torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                      torch.inverse(covarianceB)),\n",
        "                                             torch.transpose(self.covarianceAB,0,1))\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims) # regularization\n",
        "        elif attribute == 'B':\n",
        "          mu_cond = muB + torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1),\n",
        "                                                    torch.inverse(covarianceA)), \n",
        "                                       (z - muA)) # z is zA\n",
        "          logvar_cond = covarianceB - torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1), \n",
        "                                                              torch.inverse(covarianceA)),\n",
        "                                                 self.covarianceAB)\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims)\n",
        "\n",
        "        # METHOD1: re-parameterization trick\n",
        "        eps = torch.randn_like(mu_cond) #64x3x1, 64x3x3 if use logvar_cond\n",
        "        sample = mu_cond + torch.matmul(logvar_cond,eps) #64x3x1\n",
        "        sample = sample.squeeze(2) #64x3\n",
        "\n",
        "        #METHOD 2 - random sampling, can't backprop\n",
        "        #mu_cond = mu_cond.squeeze(2)\n",
        "        #distrib = MultivariateNormal(loc=mu_cond, covariance_matrix=logvar_cond)\n",
        "        #sample = distrib.rsample() # 64x3\n",
        "        \n",
        "        return sample\n",
        "        # logvar_cond is not a diagonal covariance matrix\n",
        "        #VAE reparameterization trick with non-diagonal covariance?\n",
        "        #https://stats.stackexchange.com/questions/388620/variational-autoencoder-and-covariance-matrix\n",
        "\n",
        "    def encode(self, x, attribute):\n",
        "        if attribute == 'A':\n",
        "          h1 = torch.sigmoid(self.fc1A(x))\n",
        "          return self.fc_muA(h1), self.fc_logvarA(h1)\n",
        "        elif attribute == 'B':\n",
        "          h1 = torch.sigmoid(self.fc1B(x))\n",
        "          return self.fc_muB(h1), self.fc_logvarB(h1)\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "    def decode(self, z, attribute):\n",
        "        if z.size()[0] == latent_dims: #resize from [3] to [1,3] if fed only a single sample\n",
        "            z = z.view(1, latent_dims)\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        if attribute == 'A':\n",
        "          reconA = softmax(self.fc_outA(z))\n",
        "          return reconA\n",
        "        elif attribute == 'B':\n",
        "          reconB = softmax(self.fc_outB(z))\n",
        "          return reconB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "    \n",
        "    def forward(self, xA, xB, attribute):\n",
        "        muA, logvarA = self.encode(xA, attribute='A') #logvar is size [64,3]\n",
        "        #print(muA.size())\n",
        "        muB, logvarB = self.encode(xB, attribute='B')\n",
        "        if attribute == 'A':\n",
        "          zB = self.reparameterize(muB, logvarB)\n",
        "          zA = self.conditional(muA, logvarA, muB, logvarB, zB, attribute)\n",
        "          return self.decode(zA,attribute), muA, logvarA\n",
        "        elif attribute == 'B':\n",
        "          zA = self.reparameterize(muA, logvarA)\n",
        "          zB = self.conditional(muA, logvarA, muB, logvarB, zA, attribute)\n",
        "          return self.decode(zB,attribute), muB, logvarB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "    def forward_single_attribute(self, x, attribute):\n",
        "        mu, logvar = self.encode(x,attribute)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z, attribute), mu, logvar\n",
        "\n",
        "    def query_single_attribute(self, x_evidence, evidence_attribute):\n",
        "        if evidence_attribute =='A':\n",
        "          muA,logvarA = self.encode(x_evidence, evidence_attribute)\n",
        "          muB = torch.zeros(muA.size()) #100x3\n",
        "          logvarB = torch.ones(muA.size()) #100x3\n",
        "          #logvarB = torch.zeros(muA.size())\n",
        "          zA = self.reparameterize(muA, logvarA)\n",
        "          zB = self.conditional(muA, logvarA, muB, logvarB, zA, attribute='B')\n",
        "          return self.decode(zB,attribute='B')\n",
        "\n",
        "    def latent(self,x,attribute):\n",
        "        mu, logvar = self.encode(x,attribute)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z\n",
        "\n",
        "def vae_loss(batch_recon, batch_targets, mu, logvar):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  CE = criterion(batch_recon, batch_targets)\n",
        "  #print(CE)\n",
        "  KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\n",
        "  #print(KLd)\n",
        "  return CE,variational_beta*KLd, CE + variational_beta*KLd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Re5YHgVF-q",
        "colab_type": "text"
      },
      "source": [
        "# Multivariate Normal\n",
        "Koller Equation 7.3: \\\\\n",
        "$P(z_A,z_B) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_A \\\\ \\mu_B \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{A} & \\Sigma_{AB} \\\\ \\Sigma_{BA} & \\Sigma_{B} \\end{array} \\right] \\right) $ \n",
        "\n",
        "which is equivalent to the Matrix Cookbook (353 and 354) https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf: \\\\\n",
        "$P(z_A|z_B) = Normal_{z_A}(\\hat{\\mu}_A, \\hat{\\Sigma}_A)$ \\\\\n",
        "where: \\\\\n",
        "$\\hat{\\mu}_A = \\mu_A + \\Sigma_{AB} \\Sigma_{B}^{-1}(z_B - \\mu_B)$ \\\\\n",
        "$\\hat{\\Sigma}_A = \\Sigma_A - \\Sigma_{AB} \\Sigma_B^{-1} \\Sigma_{AB}^T$ \\\\\n",
        "\n",
        "$P(z_B|z_A) = Normal_{z_B}(\\hat{\\mu}_B, \\hat{\\Sigma}_B)$ \\\\\n",
        "where: \\\\\n",
        "$\\hat{\\mu}_B = \\mu_B + \\Sigma_{AB}^T \\Sigma_{A}^{-1}(z_A - \\mu_A)$ \\\\\n",
        "$\\hat{\\Sigma}_B = \\Sigma_B - \\Sigma_{AB}^T \\Sigma_A^{-1} \\Sigma_{AB}$ \\\\\n",
        "\n",
        "\n",
        "The output of the VAE encoders are assumed to be the mean and variance of the unary normal potentials in the MRF over the latent z's where:\n",
        "\n",
        "•\tMean: $\\mu_{A}$ and diagonal variance matrix: $\\Sigma_{A}$ are the outputs of the A encoder \\\\\n",
        "•\t$\\mu_{B}$,  $\\Sigma_{B}$ are the outputs of the B encoder \\\\\n",
        "\n",
        "\n",
        "The additional pairwise k-ary Normal potentials, which represent undirected graphical model structure between the latent A and latent B : \\\\\n",
        "•\t$\\Sigma_{AB}$ = $\\Sigma_{BA}^T$ \n",
        "\n",
        "If the latent space is dimension 3, each $\\mu \\in \\mathcal{R}^{1 \\times 3}$ and each $\\Sigma \\in \\mathcal{R}^{3 \\times 3}$.\n",
        "\n",
        "# Training the VAE-MRF\n",
        "In each epoch, break the training data into batches. Each batch contains samples of OHE input $x_A$ and $x_B$:\n",
        "- Feed in $x_A$ and $x_B$ to their respective encoders to:\n",
        "  - obtain $\\mu_A, \\Sigma_A$ from encoder A\n",
        "  - obtain $\\mu_B, \\Sigma_B$ from encoder B\n",
        "- To reconstruct $x_A$:\n",
        "  - Sample $z_B$ using $\\mu_B, \\Sigma_B$ (standard VAE reparameterization trick)\n",
        "  - Using $z_B,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_A$ from $P(z_A|z_B)$ (modified VAE reparameterization trick)\n",
        "  - Feed $z_A$ into the A decoder to obtain the reconstruction $\\hat{x}_A$ for $x_A$\n",
        "\n",
        "- To reconstruct $x_B$:\n",
        "  - Sample $z_A$ using $\\mu_A, \\Sigma_A$ (standard VAE reparameterization trick)\n",
        "  - Using $z_A,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$ (modified VAE reparameterization trick)\n",
        "  - Feed $z_B$ into the B decoder to obtain the reconstruction $\\hat{x}_B$ for $x_B$\n",
        "\n",
        "- Sum the losses (reconstruction error and KL-divergence) from both A and B  and backpropagate once per batch\n",
        "\n",
        "Repeat for each batch. \\\\\n",
        "\n",
        "Note that there is a different $\\mu_A$, $\\Sigma_A$ for each $x_A$ and a different $\\mu_B$, $\\Sigma_B$ for each $x_B$, while there is only one $\\Sigma_{AB}$ to be shared. \n",
        "\n",
        "# Training the VAE-MRF on A only\n",
        "- Feed $x_A$ into the A encoder to obtain $\\mu_A, \\Sigma_A$ \n",
        "- Since no input $x_B$ to the B encoder, assume $\\mu_B, \\Sigma_B$ come from the prior P(z) = Normal (0, Identity)\n",
        "- Sample $z_B$ using $\\mu_B, \\Sigma_B$ (standard VAE reparameterization trick)\n",
        "- Using $z_B,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_A$ from $P(z_A|z_B)$ (modified VAE reparameterization trick)\n",
        "-  Feed $z_A$ into the A decoder to obtain the reconstruction $\\hat{x}_A$ for $x_A$\n",
        "- Sum the losses (reconstruction error and KL-divergence) from A and backpropagate once per batch\n",
        "\n",
        "# Querying the VAE-MRF\n",
        "Once  the VAE-MRF is trained, to query P(B|A=0=(1,0,0,0,0,0,0,0))\n",
        "- Feed $x_A$ into the A encoder to obtain $\\mu_A, \\Sigma_A$\n",
        "- Sample $z_A$ using $\\mu_A, \\Sigma_A$ (standard VAE reparameterization trick)\n",
        "- Since no input $x_B$ to the B encoder, assume $\\mu_B, \\Sigma_B$ come from the prior P(z) = Normal (0, Identity)\n",
        "- Using $z_A, \\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$\n",
        "- Feed $z_B$ into the B decoder to obtain $\\hat{x}_B$ \\\\\n",
        "\n",
        "Repeat, feeding in evidence $x_A$ multiple times to the VAE-MRF to obtain a probability distribution $P(\\hat{x}_B|x_A)$\n",
        "\n",
        "# Extension to Two Datasets AB and BC (not yet implemented)\n",
        "$P(z_A,z_B,z_C) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_A \\\\ \\mu_B \\\\ \\mu_C \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{A} & \\Sigma_{AB} & 0 \\\\ \\Sigma_{BA} & \\Sigma_{B} & \\Sigma_{BC}  \\\\ 0 & \\Sigma_{CB} & \\Sigma_{C} \\end{array} \\right] \\right) $ \n",
        "\n",
        "In addition to the AB VAE-MRF: \\\\\n",
        "  - $\\mu_{C}$,  $\\Sigma_{C}$ are the outputs of the C encoder \\\\\n",
        "  -\t$\\Sigma_{BC}$ = $\\Sigma_{CB}^T$ \n",
        "\n",
        "## Training the ABC VAE-MRF\n",
        "First sample $x_B$ from either the AB or BC dataset. Then using $x_B$, sample $x_A$ from the AB dataset and sample $x_C$ from the BC dataset.\n",
        "\n",
        "- As given previously, feed $x_A, x_B$ to their respective encoders to obtain  $\\mu_A, \\Sigma_A,  \\mu_B, \\Sigma_B$ and obtain reconstructions $\\hat{x_A}, \\hat{x_B}$. Then sum the losses (reconstruction error and KL-divergence) from both A and B  and backpropagate once per batch\n",
        "\n",
        "- Feed in $x_C$ and $x_B$ to their respective encoders to:\n",
        "  - obtain $\\mu_C, \\Sigma_C$ from encoder C\n",
        "  - obtain $\\mu_B, \\Sigma_B$ from encoder B\n",
        "\n",
        "- To reconstruct $x_C$:\n",
        "  - Sample $z_B$ using $\\mu_B, \\Sigma_B$ (standard VAE reparameterization trick)\n",
        "  - Using $z_B,\\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_C$ from $P(z_C|z_B)$ (modified VAE reparameterization trick)\n",
        "  - Feed $z_C$ into the C decoder to obtain the reconstruction $\\hat{x}_C$ for $x_C$\n",
        "\n",
        "- To reconstruct $x_B$:\n",
        "  - Sample $z_C$ using $\\mu_C, \\Sigma_C$ (standard VAE reparameterization trick)\n",
        "  - Using $z_C,\\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_C)$ (modified VAE reparameterization trick)\n",
        "  - Feed $z_B$ into the B decoder to obtain the reconstruction $\\hat{x}_B$ for $x_B$\n",
        "\n",
        "- Sum the losses (reconstruction error and KL-divergence) from both B and C  and backpropagate once per batch\n",
        "\n",
        "## Querying the ABC VAE-MRF\n",
        "Once  the VAE-MRF is trained, to query P(C|A=0=(1,0,0,0,0,0,0,0))\n",
        "- Feed $x_A$ into the A encoder to obtain $\\mu_A, \\Sigma_A$\n",
        "- Sample $z_A$ using $\\mu_A, \\Sigma_A$ (standard VAE reparameterization trick)\n",
        "- Since no input $x_B, x_C$ to the B or C encoders, assume $\\mu_B, \\Sigma_B$ and $\\mu_C, \\Sigma_C$ come from the prior P(z) = Normal (0, Identity)\n",
        "- Using $z_A, \\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$\n",
        "- Using $z_B, \\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_C$ from $P(z_C|z_B)$\n",
        "- Feed $z_C$ into the C decoder to obtain $\\hat{x}_C$ \\\\\n",
        "\n",
        "Repeat, feeding in evidence $x_A$ multiple times to the VAE-MRF to obtain a probability distribution $P(\\hat{x}_B|x_A)$\n",
        "\n",
        "# Notes\n",
        "\n",
        "A symmetric matrix is positive definite if:\n",
        "\n",
        "- all the diagonal entries are positive, and\n",
        "- each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7LH-GQRW01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainVAE(VAE):\n",
        "  VAE.train() #set model mode to train\n",
        "  xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "  xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "  #print(xA.shape)\n",
        "\n",
        "  #sample2_OHE when do BC plate\n",
        "  \n",
        "  indsA = list(range(xA.shape[0]))\n",
        "  indsB = list(range(xB.shape[0]))\n",
        "  N = num_samples # 1000\n",
        "  freq = num_epochs // 10 # floor division\n",
        "\n",
        "  loss_hist = []\n",
        "  xA = Variable(torch.from_numpy(xA))\n",
        "  xB = Variable(torch.from_numpy(xB))\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "      #print('epoch' + str(epoch))\n",
        "      indsA = np.random.permutation(indsA)\n",
        "      xA = xA[indsA]\n",
        "      xA = xA.to(device)\n",
        "      indsB = np.random.permutation(indsB)\n",
        "      xB = xB[indsB]\n",
        "      xB = xB.to(device)\n",
        "      \n",
        "      loss = 0\n",
        "      CE = 0\n",
        "      KLd = 0\n",
        "      num_batches = N / batch_size\n",
        "      for b in range(0, N, batch_size):\n",
        "          #get the mini-batch\n",
        "          x_batchA = xA[b: b+batch_size]\n",
        "          x_batchB = xB[b: b+batch_size]\n",
        "          \n",
        "          #feed forward\n",
        "          batch_reconA,latent_muA,latent_logvarA = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='A')\n",
        "          batch_reconB,latent_muB,latent_logvarB = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='B')\n",
        "\n",
        "          # Error\n",
        "          #Convert x_batchA and x_batchB from OHE vectors to single scalar\n",
        "          # max returns index location of max value in each sample of batch \n",
        "          _, xA_batch_targets = x_batchA.max(dim=1)\n",
        "          _, xB_batch_targets = x_batchB.max(dim=1)\n",
        "          train_CE_A, train_KLd_A, train_loss_A = vae_loss(batch_reconA, xA_batch_targets, latent_muA, latent_logvarA)\n",
        "          train_CE_B, train_KLd_B, train_loss_B = vae_loss(batch_reconB, xB_batch_targets, latent_muB, latent_logvarB)\n",
        "          loss += train_loss_A.item() / N # update epoch loss\n",
        "          loss += train_loss_B.item() / N\n",
        "          CE += train_CE_A.item() / N\n",
        "          CE += train_CE_B.item() / N \n",
        "          KLd += train_KLd_A.item() / N\n",
        "          KLd += train_KLd_B.item() / N\n",
        "\n",
        "          #Backprop the error, compute the gradient\n",
        "          optimizer.zero_grad()\n",
        "          train_loss = train_loss_A + train_loss_B\n",
        "          train_loss.backward()\n",
        "          \n",
        "          #update parameters based on gradient\n",
        "          optimizer.step()\n",
        "          \n",
        "      #Record loss per epoch        \n",
        "      loss_hist.append(loss)\n",
        "      \n",
        "      if epoch % freq == 0:\n",
        "          #print(VAE.covarianceAB)\n",
        "          print('')\n",
        "          print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\n",
        "\n",
        "          #Test with all training data\n",
        "          VAE.eval()\n",
        "          train_reconA, train_muA, train_logvarA = VAE.forward(xA.float(),xB.float(), attribute='A')\n",
        "          train_reconB, train_muB, train_logvarB = VAE.forward(xA.float(),xB.float(), attribute='B')\n",
        "          _, xA_targets = xA.max(dim=1)\n",
        "          _, xB_targets = xB.max(dim=1)\n",
        "          CE_A,KLd_A,test_loss_A = vae_loss(train_reconA, xA_targets, train_muA, train_logvarA)\n",
        "          CE_B,KLd_B,test_loss_B = vae_loss(train_reconB, xB_targets, train_muB, train_logvarB)\n",
        "\n",
        "          CE = CE_A + CE_B\n",
        "          Kld = KLd_A + KLd_B\n",
        "          test_loss = test_loss_A + test_loss_B\n",
        "          print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\n",
        "      \n",
        "  print(\"\\nTraining finished!\")\n",
        "  #print(loss_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjRUnGgjnIvV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Focus on just AB Plate for now\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "VAE = VariationalAutoencoder_MRF()\n",
        "VAE = VAE.to(device)\n",
        "num_params = sum(p.numel() for p in VAE.parameters() if p.requires_grad)\n",
        "\n",
        "#for param in VAE.parameters():\n",
        "#    print(type(param.data), param.size())\n",
        "#print(list(VAE.parameters()))\n",
        "print(VAE.parameters)\n",
        "print(\"Number of parameters: %d\" % num_params) #8*3 + 3 = 27, 3*8 + 8 = 32 3*3+3 = 12 *2 = 24, 27+32+24=83\n",
        "\n",
        "# optimizer object\n",
        "optimizer = torch.optim.Adam(params = VAE.parameters(), lr = learning_rate)\n",
        "\n",
        "trainVAE(VAE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2a2hqTvnhHK",
        "colab_type": "text"
      },
      "source": [
        "It appears the model converges to a local minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKiDijtuUHt",
        "colab_type": "text"
      },
      "source": [
        "## Check encoder, decoders work on their own"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrqYmOIxeZvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = np.eye(num)[np.arange(num)]                        # Test data (one-hot encoded)\n",
        "x_test = Variable(torch.from_numpy(x_test))\n",
        "x_test = x_test.to(device)\n",
        "\n",
        "print(\"Print prediction results for A only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE.forward_single_attribute(x=x.float(), attribute='A')[0].cpu().detach().numpy(),decimals=2)))\n",
        "print(\"Print prediction results for B only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE.forward_single_attribute(x=x.float(), attribute='B')[0].cpu().detach().numpy(),decimals=2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRbLTCH8lrJe",
        "colab_type": "text"
      },
      "source": [
        "# Visualize Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0jQpqykvUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "xA = Variable(torch.from_numpy(xA))\n",
        "xB = Variable(torch.from_numpy(xB))\n",
        "\n",
        "np_zA = np.empty(0)\n",
        "for x in xA:\n",
        "  zA = VAE.latent(x.float(), attribute='A')\n",
        "  #print(z)\n",
        "  np_zA = np.concatenate((np_z, z.cpu().detach().numpy()))\n",
        "np_zA = np_z.reshape(1000,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO3_sY4bm2cT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "fig = plt.figure()\n",
        "ax = Axes3D(fig)\n",
        "t = np.arange(800)\n",
        "ax.scatter(np_z[:,0], np_z[:,1], np_z[:,2], c = t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vBTljFA9YMa",
        "colab_type": "text"
      },
      "source": [
        "# Query P(B|A=0)\n",
        "Feed nothing into B encoder, muB is zero, logVarB is standard diagonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuZdoZXu9biT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xA_evidence = x_test[0] #Evidence is A=0\n",
        "xA_evidence = xA_evidence.repeat(1000,1)\n",
        "print('A evidence input, first 5 of 100 samples:')\n",
        "print(xA_evidence[0:5]) #need to resize/ view for single sample, or make evidence a batch repeated\n",
        "\n",
        "print('B query output:')\n",
        "xB_query = VAE.query_single_attribute(x_evidence=xA_evidence.float(), evidence_attribute = 'A')\n",
        "#print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\n",
        "print(xB_query.size())\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0))\n",
        "\n",
        "\n",
        "#Taking max of each row in xB_query and counting times each element is max\n",
        "print('xB_query count of when each column is max:')\n",
        "_,indices_max =xB_query.max(dim=1) \n",
        "#print(indices_max.numpy())\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8gVDBBLmw7u",
        "colab_type": "text"
      },
      "source": [
        " Seems 1,4,5,6 have higher probability, which does not match ground truth or ppandas (0,1,2,3 equal prob of 0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQ15yUZnHsc",
        "colab_type": "text"
      },
      "source": [
        "# Query P(B|A=7)\n",
        "Feed nothing into B encoder, muB is zero, logVarB is standard diagonal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFsGFCqQmIZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xA_evidence = x_test[7] #Evidence is A=7\n",
        "xA_evidence = xA_evidence.repeat(1000,1)\n",
        "print('A evidence input, first 5 of 100 samples:')\n",
        "print(xA_evidence[0:5]) #need to resize/ view for single sample, or make evidence a batch repeated\n",
        "\n",
        "print('B query output:')\n",
        "xB_query = VAE.query_single_attribute(x_evidence=xA_evidence.float(), evidence_attribute = 'A')\n",
        "#print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\n",
        "print(xB_query.size())\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0))\n",
        "\n",
        "\n",
        "#Taking max of each row in xB_query and counting times each element is max\n",
        "print('xB_query count of when each column is max:')\n",
        "_,indices_max =xB_query.max(dim=1) \n",
        "#print(indices_max.numpy())\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gai-mGEZk9a5",
        "colab_type": "text"
      },
      "source": [
        "Again, Seems 1,4,5 have higher probability, which does not match ground truth or ppandas (4,5,6,7 equal prob of 0.25). Implies that the covarianceAB matrix does not have much effect compared to the standard mean + covariance prior of P(zB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oMp0BWBo3po",
        "colab_type": "text"
      },
      "source": [
        "#Query P(B|A= -1)\n",
        "Try feeding into B encoder negative ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN5B_9zMpBib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xA_evidence = x_test[7] #Evidence is A=0 or 7\n",
        "xA_evidence = xA_evidence.repeat(1000,1)\n",
        "xB = torch.tensor([-1,-1,-1,-1,-1,-1,-1,-1])\n",
        "#xB = torch.tensor([0,0,0,0,0,0,0,0])\n",
        "#xB = torch.tensor([0,0,0,0,0,0,0,1]) # if feed in valid input, get correct result\n",
        "xB = xB.repeat(1000,1)\n",
        "\n",
        "xB_query,_,_ = VAE.forward(xA_evidence.float(),xB.float(), attribute='B')\n",
        "print(xB_query.size())\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZJqFYQKqEWZ",
        "colab_type": "text"
      },
      "source": [
        "- No matter xA evidence, if B encoder always given -1's B decoder same xB\n",
        "- No matter xA evidence, if B encoder always given 0's B decoder returns same xB\n",
        "- If feed in valid xB as evidence, then get correct xB as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPKlzMCE8abG",
        "colab_type": "text"
      },
      "source": [
        "# Questions and Notes\n",
        "\n",
        "Requires alternating between AB and BC samples where B is the same.\n",
        "\n",
        "Have a separate plate for each dataset.\n",
        "In Bayesian network, need to learn P(B),P(A|B), P(C|B). \\\\\n",
        "In MRF need to learn factors $\\phi(A,B)$ and $\\phi(B,C)$.\n",
        "\n",
        "How to handle datasets with 3 dimensions.\n",
        "Latent edges between A,B,C (clique)?\n",
        "\n",
        "Do we need to incorporate the parition function Z? If want probabilities that sum to 1 then yes. But if just looking to have input into the decoders then normalizing isn't necessary?\n",
        "\n",
        "Koller Definition 4.3: \\\\\n",
        "$Z = \\sum_{AB,BC} \\phi(A,B) \\times \\phi(B,C)$ \\\\\n",
        "$P(A,B,C) = \\frac{1}{Z} \\phi(A,B) \\times \\phi(B,C)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgk-LlXB64eb",
        "colab_type": "text"
      },
      "source": [
        "# To Do\n",
        "\n",
        "- Query P(A|B=0)\n",
        "- Add BC Plate\n",
        "- Visualize latent space\n",
        "- Try more than 1 sample when sampling zA and zB\n",
        "- During training, try reconstructing A given only x_B and reconstructing B given only x_A. I believe feeding in A (and B) to reconstruct A during train time does not match what is required of the model during test time where we feed in only B to reconstruct A.\n",
        "  - However have to \n",
        "- Modifying variational_beta to lowest value that reconstructions were valid did not change ressults (0.0001), any higher variational_beta gave poor reconstructions.\n",
        "- Check if training on only A improves performance\n",
        "- Formalize in Overleaf\n",
        "- Answer general research questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCII451nHRR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ_f2Kmg7H9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}