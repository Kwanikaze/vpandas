{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRF_VAE",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPy5owqFya9t+p18mkkJk2x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwanikaze/vpandas/blob/master/MRF_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaO7CHX93gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNkadXIh0gD",
        "colab_type": "text"
      },
      "source": [
        "# Load Data and Create Sample Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE259FbtK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create two datasets from global df that are one-hot encoded\n",
        "def OHE_sample(sample_df, features_to_OHE: list):\n",
        "  for feature in features_to_OHE:\n",
        "    feature_OHE = pd.get_dummies(prefix = feature,data= sample_df[feature])\n",
        "    sample_df = pd.concat([sample_df,feature_OHE],axis=1)\n",
        "  sample_df.drop(features_to_OHE,axis=1,inplace=True)\n",
        "  print(sample_df)\n",
        "  return sample_df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykDGUc_-Q2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "3e5e1ac9-277f-479e-f88d-3477a9b72afa"
      },
      "source": [
        "# Load global relation\n",
        "df = pd.read_csv(\"data_8.csv\")\n",
        "print(df.shape)\n",
        "\n",
        "#Create two datasets containing AB and BC\n",
        "num_samples = 1000\n",
        "sample1_df = df[['A','B']].sample(n=num_samples, random_state=2)\n",
        "print(sample1_df.head())\n",
        "sample2_df = df[['B','C']].sample(n=num_samples, random_state=3)\n",
        "print(sample2_df.head())\n",
        "\n",
        "# Make A,B,C inputs all 8 bits\n",
        "#Does data need to respect Gaussian distribution?\n",
        "#Could add noise so not exactly OHE: 0.01...0.9...0.01\n",
        "sample1_OHE = OHE_sample(sample1_df,['A','B'])\n",
        "sample2_OHE = OHE_sample(sample2_df,['B','C'])\n",
        "\n",
        "# Could onvert pandas dataframes to list of lists of lists\n",
        "# [ [[OHE A1],[OHE B1]], [[OHE A2],[OHE B2]], ...  ]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5056, 3)\n",
            "      A  B\n",
            "4333  7  6\n",
            "2638  6  4\n",
            "2254  4  4\n",
            "3116  5  5\n",
            "3998  6  6\n",
            "      B  C\n",
            "4616  7  6\n",
            "2276  4  6\n",
            "3448  5  4\n",
            "4064  6  5\n",
            "1204  2  3\n",
            "      A_0  A_1  A_2  A_3  A_4  A_5  A_6  ...  B_1  B_2  B_3  B_4  B_5  B_6  B_7\n",
            "4333    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2638    0    0    0    0    0    0    1  ...    0    0    0    1    0    0    0\n",
            "2254    0    0    0    0    1    0    0  ...    0    0    0    1    0    0    0\n",
            "3116    0    0    0    0    0    1    0  ...    0    0    0    0    1    0    0\n",
            "3998    0    0    0    0    0    0    1  ...    0    0    0    0    0    1    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "1857    0    1    0    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "3813    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "604     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "621     1    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1322    0    1    0    0    0    0    0  ...    0    1    0    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n",
            "      B_0  B_1  B_2  B_3  B_4  B_5  B_6  ...  C_1  C_2  C_3  C_4  C_5  C_6  C_7\n",
            "4616    0    0    0    0    0    0    0  ...    0    0    0    0    0    1    0\n",
            "2276    0    0    0    0    1    0    0  ...    0    0    0    0    0    1    0\n",
            "3448    0    0    0    0    0    1    0  ...    0    0    0    1    0    0    0\n",
            "4064    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "1204    0    0    1    0    0    0    0  ...    0    0    1    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "3358    0    0    0    0    0    1    0  ...    0    0    0    0    0    1    0\n",
            "1496    0    0    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4025    0    0    0    0    0    0    1  ...    0    0    0    0    1    0    0\n",
            "4689    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    1\n",
            "2155    0    0    0    1    0    0    0  ...    0    0    1    0    0    0    0\n",
            "\n",
            "[1000 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWt2iUw9xE",
        "colab_type": "text"
      },
      "source": [
        "# Global Relation Bayesian Network Ground Truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZqS2rxNrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "684c36be-f84c-4904-bd79-a81b08353f48"
      },
      "source": [
        "!pip install pgmpy==0.1.9\n",
        "import pgmpy\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "def groundTruth(df,evidence):\n",
        "    \"\"\"\n",
        "    Extracts ground truth from global relation\n",
        "    \"\"\"\n",
        "    model = BayesianModel([('B', 'A'), ('B', 'C')])\n",
        "    model.fit(df)\n",
        "    nx.draw(model, with_labels=True)\n",
        "    plt.show()\n",
        "    print('\\n Global Relation Ground Truth')\n",
        "    #for var in model.nodes():\n",
        "    #    print(model.get_cpds(var))\n",
        "    inference = VariableElimination(model)\n",
        "    \n",
        "    #q = inference.query(variables=['A','B','C'])\n",
        "    #joint_prob = q.values.flatten()\n",
        "    #print(joint_prob)\n",
        "    #print('\\n P(A,B,C) \\n Ground Truth')\n",
        "    #print(q)\n",
        "    q = inference.query(variables=['C'], evidence=evidence)\n",
        "    print(q)\n",
        "\n",
        "print('\\n P(C|A=0) \\n Ground Truth')\n",
        "groundTruth(df,{'A':0})\n",
        "\n",
        "print('\\n P(C|B=0) \\n Ground Truth')\n",
        "groundTruth(df,{'B':0})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pgmpy==0.1.9 in /usr/local/lib/python3.6/dist-packages (0.1.9)\n",
            "\n",
            " P(C|A=0) \n",
            " Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : 100%|██████████| 1/1 [00:00<00:00, 277.73it/s]\n",
            "Eliminating: B: 100%|██████████| 1/1 [00:00<00:00, 195.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| C    |   phi(C) |\n",
            "+======+==========+\n",
            "| C(0) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(1) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(2) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(3) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(4) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(5) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(6) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(7) |   0.0000 |\n",
            "+------+----------+\n",
            "\n",
            " P(C|B=0) \n",
            " Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : 100%|██████████| 1/1 [00:00<00:00, 157.25it/s]\n",
            "Eliminating: A: 100%|██████████| 1/1 [00:00<00:00, 260.47it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| C    |   phi(C) |\n",
            "+======+==========+\n",
            "| C(0) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(1) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(2) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(3) |   0.2500 |\n",
            "+------+----------+\n",
            "| C(4) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(5) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(6) |   0.0000 |\n",
            "+------+----------+\n",
            "| C(7) |   0.0000 |\n",
            "+------+----------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3YIf_-iAm8",
        "colab_type": "text"
      },
      "source": [
        "# VAE-MRF Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UMLBM0iE4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VAE Parameters\n",
        "num = 8 # digits from 0 to 7\n",
        "latent_dims = 3 # Latent z_A,z_B,z_C all are all same dimension size\n",
        "num_epochs = 2000\n",
        "batch_size = 64\n",
        "learning_rate = 1e-3\n",
        "use_gpu = True\n",
        "variational_beta = 0.00001 #tuned"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FiF8-RkNLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalAutoencoder_MRF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1A = nn.Linear(num, latent_dims)\n",
        "        self.fc_muA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarA = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outA = nn.Linear(latent_dims,num)\n",
        "        \n",
        "        self.fc1B = nn.Linear(num, latent_dims)\n",
        "        self.fc_muB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_logvarB = nn.Linear(latent_dims, latent_dims)\n",
        "        self.fc_outB = nn.Linear(latent_dims,num)\n",
        "\n",
        "        #Covariance: Sigma_{AB} = Sigma_{BA}^T\n",
        "        # Sigma_AB is the top right term\n",
        "        #self.covarianceAB = nn.Parameter(torch.zeros(latent_dims,latent_dims),requires_grad=True)\n",
        "        self.covarianceAB = torch.randn(size=(latent_dims,latent_dims))\n",
        "        self.covarianceAB = torch.nn.Parameter(self.covarianceAB,requires_grad=True)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        #self.covarianceAB = nn.Parameter(torch.rand(size=(latent_dims,latent_dims), requires_grad=True))\n",
        "        #print(self.covarianceAB)\n",
        "\n",
        "    def reparameterize(self, mu, logvar): #mu.size() = batch_size, 3\n",
        "        std = torch.exp(0.5*logvar) #batch_size,3 \n",
        "        eps = torch.randn_like(std) #batch_size,3\n",
        "        #print('eps size')\n",
        "        #print(eps.size())\n",
        "        return mu + eps*std # batch_size,3\n",
        "\n",
        "\n",
        "    # Conditional of Multivariate Gaussian: matrix cookbook 353 and 354\n",
        "    def conditional(self, muA, logvarA, muB, logvarB, z, attribute):\n",
        "        #Convert logvarA vector to diagonal matrix\n",
        "        logvarA = torch.exp(0.5*logvarA)\n",
        "        logvarB = torch.exp(0.5*logvarB)\n",
        "        covarianceA = torch.diag_embed(logvarA) #batch_size,3,3\n",
        "        covarianceB = torch.diag_embed(logvarB)\n",
        "        #self.covarianceAB = torch.nn.Parameter(0.5* torch.exp(self.covarianceAB),requires_grad=True)\n",
        "        muA = muA.unsqueeze(2)\n",
        "        muB = muB.unsqueeze(2)\n",
        "        z = z.unsqueeze(2)\n",
        "        if attribute == 'A':\n",
        "          mu_cond = muA + torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                    torch.inverse(covarianceB)),\n",
        "                                   (z - muB)) # z is zB\n",
        "          logvar_cond = covarianceA - torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                      torch.inverse(covarianceB)),\n",
        "                                             torch.transpose(self.covarianceAB,0,1))\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims) # regularization\n",
        "        elif attribute == 'B':\n",
        "          mu_cond = muB + torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1),\n",
        "                                                    torch.inverse(covarianceA)), \n",
        "                                       (z - muA)) # z is zA\n",
        "          logvar_cond = covarianceB - torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1), \n",
        "                                                              torch.inverse(covarianceA)),\n",
        "                                                 self.covarianceAB)\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims)\n",
        "        #print('mu_cond, logvar_cond, eps, xx size')\n",
        "        #print(mu_cond.size()) # 64x3x1\n",
        "        #print(mu_cond)\n",
        "        #print(logvar_cond.size()) #64x3x3\n",
        "        #print(logvar_cond)\n",
        "\n",
        "        # METHOD1: re-parameterization trick\n",
        "        eps = torch.randn_like(mu_cond) #64x3x1, 64x3x3 if use logvar_cond\n",
        "        #print(eps.size())\n",
        "        #xx = eps*logvar_cond #64x3x3\n",
        "        #xx = torch.matmul(logvar_cond,eps) #64x3x1\n",
        "        #print(xx.size())\n",
        "        sample = mu_cond + torch.matmul(logvar_cond,eps)\n",
        "        sample = sample.squeeze(2) #64x3\n",
        "\n",
        "        #METHOD 2 - random sampling, can't backprop\n",
        "        #mu_cond = mu_cond.squeeze(2)\n",
        "        #distrib = MultivariateNormal(loc=mu_cond, covariance_matrix=logvar_cond)\n",
        "        #sample = distrib.rsample() # 64x3\n",
        "        \n",
        "        #print(sample.size())\n",
        "        return sample\n",
        "        #return self.reparameterize(mu_cond, logvar_cond) # logvar_cond is not a diagonal covariance matrix\n",
        "        #VAE reparameterization trick with non-diagonal covariance?\n",
        "        #https://stats.stackexchange.com/questions/388620/variational-autoencoder-and-covariance-matrix\n",
        "\n",
        "    def encode(self, x, attribute):\n",
        "        if attribute == 'A':\n",
        "          h1 = torch.sigmoid(self.fc1A(x))\n",
        "          return self.fc_muA(h1), self.fc_logvarA(h1)\n",
        "        elif attribute == 'B':\n",
        "          h1 = torch.sigmoid(self.fc1B(x))\n",
        "          return self.fc_muB(h1), self.fc_logvarB(h1)\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "    def decode(self, z, attribute):\n",
        "        if z.size()[0] == latent_dims: #resize from [3] to [1,3] if fed only a single sample\n",
        "            z = z.view(1, latent_dims)\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "        if attribute == 'A':\n",
        "          reconA = softmax(self.fc_outA(z))\n",
        "          return reconA\n",
        "        elif attribute == 'B':\n",
        "          reconB = softmax(self.fc_outB(z))\n",
        "          return reconB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "    \n",
        "    def forward(self, xA, xB, attribute):\n",
        "        muA, logvarA = self.encode(xA, attribute='A') #logvar is size [64,3]\n",
        "        muB, logvarB = self.encode(xB, attribute='B')\n",
        "        if attribute == 'A':\n",
        "          zB = self.reparameterize(muB, logvarB)\n",
        "          zA = self.conditional(muA, logvarA, muB, logvarB, zB, attribute)\n",
        "          return self.decode(zA,attribute), muA, logvarA\n",
        "        elif attribute == 'B':\n",
        "          zA = self.reparameterize(muA, logvarA)\n",
        "          zB = self.conditional(muA, logvarA, muB, logvarB, zA, attribute)\n",
        "          return self.decode(zB,attribute), muB, logvarB\n",
        "        print('ERROR')\n",
        "        return -100\n",
        "\n",
        "def vae_loss(batch_recon, batch_targets, mu, logvar):\n",
        "  #print('batch_targets, batch_recon size')\n",
        "  #print(batch_targets.size())\n",
        "  #print(batch_recon.size())\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  CE = criterion(batch_recon, batch_targets)\n",
        "  #print(CE)\n",
        "  KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\n",
        "  #print(KLd)\n",
        "  return CE,variational_beta*KLd, CE + variational_beta*KLd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Re5YHgVF-q",
        "colab_type": "text"
      },
      "source": [
        "Koller Equation 7.3: \\\\\n",
        "$P(X,Y) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_X \\\\ \\mu_Y \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{XX} & \\Sigma_{XY} \\\\ \\Sigma_{YX} & \\Sigma_{YY} \\end{array} \\right] \\right) $ \n",
        "\n",
        "From Koller Theorem 7.4: \\\\\n",
        "$P(Y|X) = Normal (\\beta_0 + \\beta^TX, \\sigma^2)$ \\\\\n",
        "such that \\\\\n",
        "$\\beta_0 = \\mu_Y - \\Sigma_{YX} \\Sigma^{-1}_{XX}\\mu_X$ \\\\\n",
        "$\\beta = \\Sigma^{-1}_{XX} \\Sigma_{YX}$ \\\\\n",
        "$\\sigma^2 = \\Sigma_{YY} - \\Sigma_{YX}\\Sigma^{-1}_{XX}\\Sigma_{XY}$\n",
        "\n",
        "which is equivalent to the Matrix Cookbook (353 and 354).\n",
        "\n",
        "A symmetric matrix is positive definite if:\n",
        "\n",
        "- all the diagonal entries are positive, and\n",
        "- each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7LH-GQRW01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainVAE(VAE):\n",
        "  VAE.train() #set model mode to train\n",
        "  xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "  xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "  #print(xA.shape)\n",
        "\n",
        "  #sample2_OHE when do BC plate\n",
        "  \n",
        "  indsA = list(range(xA.shape[0]))\n",
        "  indsB = list(range(xB.shape[0]))\n",
        "  N = num_samples # 1000\n",
        "  freq = num_epochs // 10 # floor division\n",
        "\n",
        "  loss_hist = []\n",
        "  xA = Variable(torch.from_numpy(xA))\n",
        "  xB = Variable(torch.from_numpy(xB))\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "      #print('epoch' + str(epoch))\n",
        "      indsA = np.random.permutation(indsA)\n",
        "      xA = xA[indsA]\n",
        "      xA = xA.to(device)\n",
        "      indsB = np.random.permutation(indsB)\n",
        "      xB = xB[indsB]\n",
        "      xB = xB.to(device)\n",
        "      \n",
        "      loss = 0\n",
        "      CE = 0\n",
        "      KLd = 0\n",
        "      num_batches = N / batch_size\n",
        "      for b in range(0, N, batch_size):\n",
        "          #get the mini-batch\n",
        "          x_batchA = xA[b: b+batch_size]\n",
        "          x_batchB = xB[b: b+batch_size]\n",
        "          \n",
        "          #feed forward\n",
        "          batch_reconA,latent_muA,latent_logvarA = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='A')\n",
        "          batch_reconB,latent_muB,latent_logvarB = VAE.forward(x_batchA.float(),x_batchB.float(),attribute='B')\n",
        "          #print('batch_recon size')\n",
        "          #print(batch_reconA.size())\n",
        "          # Error\n",
        "          #Convert x_batchA and x_batchB from OHE vectors to single scalar\n",
        "          # max returns index location of max value in each sample of batch \n",
        "          _, xA_batch_targets = x_batchA.max(dim=1)\n",
        "          _, xB_batch_targets = x_batchB.max(dim=1)\n",
        "          train_CE_A, train_KLd_A, train_loss_A = vae_loss(batch_reconA, xA_batch_targets, latent_muA, latent_logvarA)\n",
        "          train_CE_B, train_KLd_B, train_loss_B = vae_loss(batch_reconB, xB_batch_targets, latent_muB, latent_logvarB)\n",
        "          #print(batch_reconA.size())\n",
        "          #print(xA_batch_targets.size())\n",
        "          loss += train_loss_A.item() / N # update epoch loss\n",
        "          loss += train_loss_B.item() / N\n",
        "          CE += train_CE_A.item() / N\n",
        "          CE += train_CE_B.item() / N \n",
        "          KLd += train_KLd_A.item() / N\n",
        "          KLd += train_KLd_B.item() / N\n",
        "\n",
        "          #Backprop the error, compute the gradient\n",
        "          optimizer.zero_grad()\n",
        "          train_loss = train_loss_A + train_loss_B\n",
        "          train_loss.backward()\n",
        "          \n",
        "          #update parameters based on gradient\n",
        "          optimizer.step()\n",
        "          \n",
        "      #Record loss per epoch        \n",
        "      loss_hist.append(loss)\n",
        "      \n",
        "      if epoch % freq == 0:\n",
        "          print(VAE.covarianceAB)\n",
        "          print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\n",
        "          \n",
        "          #Test with all training data\n",
        "          VAE.eval()\n",
        "          train_reconA, train_muA, train_logvarA = VAE.forward(xA.float(),xB.float(), attribute='A')\n",
        "          train_reconB, train_muB, train_logvarB = VAE.forward(xA.float(),xB.float(), attribute='B')\n",
        "          _, xA_targets = xA.max(dim=1)\n",
        "          _, xB_targets = xB.max(dim=1)\n",
        "          CE_A,KLd_A,test_loss_A = vae_loss(train_reconA, xA_targets, train_muA, train_logvarA)\n",
        "          CE_B,KLd_B,test_loss_B = vae_loss(train_reconB, xB_targets, train_muB, train_logvarB)\n",
        "\n",
        "          CE = CE_A + CE_B\n",
        "          Kld = KLd_A + KLd_B\n",
        "          test_loss = test_loss_A + test_loss_B\n",
        "          print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\n",
        "      \n",
        "  print(\"\\nTraining finished!\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCII451nHRR",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "Requires alternating between AB and BC samples where B is the same. What if B is not the same in both datasets? How to train?\n",
        "\n",
        "Have a separate plate for each.\n",
        "In Bayesian network, need to learn P(B),P(A|B), P(C|B). \\\\\n",
        "In MRF need to learn factors $\\phi(A,B)$ and $\\phi(B,C)$.\n",
        "\n",
        "We want to query P(C|A), therefore at test time there will be no input to the B encoder.\n",
        "\n",
        "Do we need to incorporate the parition function Z? If want probabilities that sum to 1 then yes. But if just looking to have input into the decoders then normalizing isn't necessary?\n",
        "\n",
        "Koller Definition 4.3: \\\\\n",
        "$Z = \\sum_{AB,BC} \\phi(A,B) \\times \\phi(B,C)$ \\\\\n",
        "$P(A,B,C) = \\frac{1}{Z} \\phi(A,B) \\times \\phi(B,C)$ \n",
        "\n",
        "To learn $\\phi(A,B)$ where X = A and Y=B, need to re-construct A and B, have separate loss terms for the A decoder and the B decoder and backpropogate to learn the mean vectors, variance matrices and covariance matrices.\n",
        "\n",
        "Need to work in log-space for numerical stability.\n",
        "\n",
        "Assume the A encoder outputs $\\mu_A, \\Sigma_{AA}$ and the B encoder outputs $\\mu_B, \\Sigma_{BB}$.\n",
        "\n",
        "The latent variables have structure by learning $\\Sigma_{AB}, \\Sigma_{BA} = \\Sigma_{AB}^T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjRUnGgjnIvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "6a147c76-ce33-4f50-b9fa-78ff4148cc6d"
      },
      "source": [
        "# Focus on just AB Plate for now\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "VAE = VariationalAutoencoder_MRF()\n",
        "VAE = VAE.to(device)\n",
        "num_params = sum(p.numel() for p in VAE.parameters() if p.requires_grad)\n",
        "\n",
        "#for param in VAE.parameters():\n",
        "#    print(type(param.data), param.size())\n",
        "#print(list(VAE.parameters()))\n",
        "#print(VAE.parameters)\n",
        "print(\"Number of parameters: %d\" % num_params) #8*3 + 3 = 27, 3*8 + 8 = 32 3*3+3 = 12 *2 = 24, 27+32+24=83\n",
        "\n",
        "# optimizer object\n",
        "optimizer = torch.optim.Adam(params = VAE.parameters(), lr = learning_rate)\n",
        "\n",
        "trainVAE(VAE)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 175\n",
            "Parameter containing:\n",
            "tensor([[ 1.2833, -1.6478,  0.7694],\n",
            "        [-0.1047,  0.0675, -0.0198],\n",
            "        [ 1.3801, -0.0344, -0.3366]], requires_grad=True)\n",
            "Epoch 1/2000\t CE: 0.06697, KLd: 0.00001, Train loss=0.06697\t\t CE: 4.17411, KLd: 0.00001, Test loss: 4.18109Parameter containing:\n",
            "tensor([[ 0.1588, -0.2949,  0.1419],\n",
            "        [ 0.1934,  0.0683,  0.0136],\n",
            "        [ 0.1455, -0.1191, -0.0922]], requires_grad=True)\n",
            "Epoch 201/2000\t CE: 0.04619, KLd: 0.00018, Train loss=0.04637\t\t CE: 2.87854, KLd: 0.00018, Test loss: 3.06124Parameter containing:\n",
            "tensor([[ 0.0066, -0.0951,  0.1167],\n",
            "        [ 0.1324,  0.0226,  0.0154],\n",
            "        [ 0.0464, -0.0971, -0.0456]], requires_grad=True)\n",
            "Epoch 401/2000\t CE: 0.04090, KLd: 0.00035, Train loss=0.04125\t\t CE: 2.55588, KLd: 0.00035, Test loss: 2.91056Parameter containing:\n",
            "tensor([[-0.0792, -0.1452,  0.1479],\n",
            "        [ 0.1622,  0.0584,  0.1002],\n",
            "        [ 0.1214, -0.1370, -0.0751]], requires_grad=True)\n",
            "Epoch 601/2000\t CE: 0.04081, KLd: 0.00024, Train loss=0.04105\t\t CE: 2.55073, KLd: 0.00024, Test loss: 2.78675Parameter containing:\n",
            "tensor([[-0.0548, -0.1976,  0.1370],\n",
            "        [ 0.1308,  0.0854,  0.1361],\n",
            "        [ 0.1946, -0.1112, -0.0861]], requires_grad=True)\n",
            "Epoch 801/2000\t CE: 0.04079, KLd: 0.00014, Train loss=0.04093\t\t CE: 2.54924, KLd: 0.00014, Test loss: 2.69156Parameter containing:\n",
            "tensor([[ 0.0251, -0.2357,  0.1093],\n",
            "        [ 0.1380,  0.0984,  0.1552],\n",
            "        [ 0.2209, -0.0383, -0.1300]], requires_grad=True)\n",
            "Epoch 1001/2000\t CE: 0.04078, KLd: 0.00012, Train loss=0.04090\t\t CE: 2.54894, KLd: 0.00012, Test loss: 2.66481Parameter containing:\n",
            "tensor([[ 0.1328, -0.2073,  0.1022],\n",
            "        [ 0.1310,  0.1399,  0.1242],\n",
            "        [ 0.1835,  0.0384, -0.1506]], requires_grad=True)\n",
            "Epoch 1201/2000\t CE: 0.04078, KLd: 0.00011, Train loss=0.04088\t\t CE: 2.54871, KLd: 0.00011, Test loss: 2.65424Parameter containing:\n",
            "tensor([[ 0.1246, -0.2098,  0.0715],\n",
            "        [ 0.1193,  0.1058,  0.1222],\n",
            "        [ 0.1682,  0.0178, -0.1647]], requires_grad=True)\n",
            "Epoch 1401/2000\t CE: 0.04078, KLd: 0.00010, Train loss=0.04088\t\t CE: 2.54857, KLd: 0.00010, Test loss: 2.64851Parameter containing:\n",
            "tensor([[ 0.1195, -0.2220,  0.0909],\n",
            "        [ 0.1172,  0.1106,  0.1345],\n",
            "        [ 0.1745,  0.0565, -0.1483]], requires_grad=True)\n",
            "Epoch 1601/2000\t CE: 0.04078, KLd: 0.00010, Train loss=0.04087\t\t CE: 2.54842, KLd: 0.00010, Test loss: 2.64542Parameter containing:\n",
            "tensor([[ 0.0961, -0.1991,  0.1095],\n",
            "        [ 0.1154,  0.1272,  0.1030],\n",
            "        [ 0.1757, -0.0074, -0.1455]], requires_grad=True)\n",
            "Epoch 1801/2000\t CE: 0.04077, KLd: 0.00009, Train loss=0.04087\t\t CE: 2.54848, KLd: 0.00009, Test loss: 2.64214\n",
            "Training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKiDijtuUHt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrqYmOIxeZvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}