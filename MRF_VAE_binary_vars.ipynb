{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRF_VAE_binary_vars",
      "provenance": [],
      "collapsed_sections": [
        "tvSWt2iUw9xE",
        "_bTvWAZ9UARW",
        "t5whHhIl14l5"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMvZi4+1I18H55Xe4OKXIDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwanikaze/vpandas/blob/master/MRF_VAE_binary_vars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaO7CHX93gN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6980d845-2e52-4295-a109-becba5eec4b3"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "!pip install pgmpy==0.1.9\n",
        "import pgmpy\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "!pip install -i https://test.pypi.org/simple/ PPandas==0.0.1.7.1\n",
        "!pip install python-intervals\n",
        "!pip install geopandas\n",
        "!pip install geovoronoi\n",
        "import ppandas\n",
        "from ppandas import PDataFrame"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pgmpy==0.1.9 in /usr/local/lib/python3.6/dist-packages (0.1.9)\n",
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: PPandas==0.0.1.7.1 in /usr/local/lib/python3.6/dist-packages (0.0.1.7.1)\n",
            "Requirement already satisfied: python-intervals in /usr/local/lib/python3.6/dist-packages (1.10.0.post1)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.6/dist-packages (0.8.2)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (3.0.0.post1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: fiona in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.8.18)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from pyproj>=2.2.0->geopandas) (2020.12.5)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.1.2)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (20.3.0)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.15.0)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (0.7.1)\n",
            "Requirement already satisfied: geovoronoi in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from geovoronoi) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from geovoronoi) (1.4.1)\n",
            "Requirement already satisfied: shapely>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from geovoronoi) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNkadXIh0gD"
      },
      "source": [
        "# Load Data and Create Sample Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE259FbtK1"
      },
      "source": [
        "# Function to create OHE dataset for specified attributes given a global df\n",
        "def OHE_sample(sample_df, features_to_OHE: list):\n",
        "  for feature in features_to_OHE:\n",
        "    feature_OHE = pd.get_dummies(prefix = feature,data= sample_df[feature])\n",
        "    sample_df = pd.concat([sample_df,feature_OHE],axis=1)\n",
        "  sample_df.drop(features_to_OHE,axis=1,inplace=True)\n",
        "  print(sample_df)\n",
        "  return sample_df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykDGUc_-Q2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2f4d86-5403-4681-cb0f-1315027bb4e1"
      },
      "source": [
        "#Hardcode 2x2 P(A,B)\n",
        "# Load global relation from github\n",
        "from numpy import genfromtxt\n",
        "data_2 = genfromtxt('https://raw.githubusercontent.com/Kwanikaze/vpandas/master/data_2.csv?token=ADHT6UHKQYDJPLSDJROGZATACHP5Q', delimiter=',',skip_header=1)\n",
        "data_2_1000 = np.tile(data_2, (100, 1))\n",
        "mean = np.mean(data_2_1000, axis=0)\n",
        "cov = np.cov(data_2_1000, rowvar=0)\n",
        "print(\"Mean Vector\")\n",
        "print(mean)\n",
        "print(\"Covariance Matrix\")\n",
        "print(cov)\n",
        "#print(data_2.shape)\n",
        "#print(data_2_1000.shape)\n",
        "df = pd.DataFrame(pd.np.tile(data_2, (100, 1)))\n",
        "df.columns=['A','B']\n",
        "df=df.astype(int)\n",
        "#print(df)\n",
        "#df.to_csv('data_2_1000rows.csv',index=False)\n",
        "\n",
        "\n",
        "#df = pd.read_csv(\"data_2_1000rows.csv\") # 3columns A,B,C that each contain values 0 to 1, block diagonal\n",
        "print(df.shape)\n",
        "\n",
        "#Create two datasets containing AB and BC\n",
        "num_samples = 500\n",
        "sample1_df = df[['A','B']].sample(n=num_samples, random_state=2)\n",
        "print(sample1_df.shape)\n",
        "print(sample1_df.head())\n",
        "#sample2_df = df[['B','C']].sample(n=num_samples, random_state=3)\n",
        "#print(sample2_df.head())\n",
        "\n",
        "# Make A,B,C inputs all 8 bits\n",
        "#Could add noise so not exactly OHE: 0.01...0.9...0.01\n",
        "sample1_OHE = OHE_sample(sample1_df,['A','B'])\n",
        "#sample2_OHE = OHE_sample(sample2_df,['B','C'])\n",
        "\n",
        "# Could onvert pandas dataframes to list of lists of lists\n",
        "# [ [[OHE A1],[OHE B1]], [[OHE A2],[OHE B2]], ...  ]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Vector\n",
            "[0.6 0.5]\n",
            "Covariance Matrix\n",
            "[[0.24024024 0.1001001 ]\n",
            " [0.1001001  0.25025025]]\n",
            "(1000, 2)\n",
            "(500, 2)\n",
            "     A  B\n",
            "37   1  1\n",
            "726  1  1\n",
            "846  1  1\n",
            "295  1  0\n",
            "924  1  0\n",
            "     A_0  A_1  B_0  B_1\n",
            "37     0    1    0    1\n",
            "726    0    1    0    1\n",
            "846    0    1    0    1\n",
            "295    0    1    1    0\n",
            "924    0    1    1    0\n",
            "..   ...  ...  ...  ...\n",
            "194    0    1    1    0\n",
            "136    0    1    0    1\n",
            "581    1    0    1    0\n",
            "662    1    0    1    0\n",
            "671    1    0    1    0\n",
            "\n",
            "[500 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWt2iUw9xE"
      },
      "source": [
        "# Global Relation Bayesian Network Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up-Ps6PEoQB4"
      },
      "source": [
        "P(A,B) = \r\n",
        "*   P(A=0,B=0) = 0.3\r\n",
        "*P(A=0,B=1) = 0.1\r\n",
        "*P(A=1,B=0) = 0.2\r\n",
        "*P(A=1,B=1) = 0.4\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZqS2rxNrH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "04e936d7-f4d0-49e7-be6d-49b2f3c6355c"
      },
      "source": [
        "def groundTruth(df,query_attribute,evidence):\n",
        "    \"\"\"\n",
        "    Extracts ground truth from global relation\n",
        "    \"\"\"\n",
        "    model = BayesianModel([('B', 'A')])\n",
        "    model.fit(df)\n",
        "    nx.draw(model, with_labels=True)\n",
        "    plt.show()\n",
        "    print('\\n Global Relation Ground Truth')\n",
        "    #for var in model.nodes():\n",
        "    #    print(model.get_cpds(var))\n",
        "    inference = VariableElimination(model)\n",
        "    \n",
        "    #q = inference.query(variables=['A','B','C'])\n",
        "    #joint_prob = q.values.flatten()\n",
        "    #print(joint_prob)\n",
        "    #print('\\n P(A,B,C) \\n Ground Truth')\n",
        "    #print(q)\n",
        "    q = inference.query(variables=[query_attribute], evidence=evidence)\n",
        "    print(q)\n",
        "\n",
        "print('\\n P(B|A=0) Ground Truth')\n",
        "groundTruth(df,query_attribute = 'B', evidence = {'A':0})\n",
        "\n",
        "print('\\n P(A|B=0) Ground Truth')\n",
        "groundTruth(df,query_attribute = 'B', evidence = {'A':1})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " P(B|A=0) Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| B    |   phi(B) |\n",
            "+======+==========+\n",
            "| B(0) |   0.7500 |\n",
            "+------+----------+\n",
            "| B(1) |   0.2500 |\n",
            "+------+----------+\n",
            "\n",
            " P(A|B=0) Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| B    |   phi(B) |\n",
            "+======+==========+\n",
            "| B(0) |   0.3333 |\n",
            "+------+----------+\n",
            "| B(1) |   0.6667 |\n",
            "+------+----------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bTvWAZ9UARW"
      },
      "source": [
        "# ppandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bto996MFUCnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b527bf1-8bc9-44ac-90a1-27640293f769"
      },
      "source": [
        "def ppandas_query(sample1_df,num_samples,query_attribute,evidence):\n",
        "    pd1 = PDataFrame(['B'],sample1_df)\n",
        "    q = pd1.query(['A','B'])\n",
        "    cols = q.columns.tolist()\n",
        "    q = q.rename(columns={q.columns[2]:'Probability(A,B)'})\n",
        "    #Reorder columns\n",
        "    q = q[['A','B','Probability(A,B)']]\n",
        "    q= q.sort_values(by=['A','B'])\n",
        "    #print(q)\n",
        "    #Sort rows in dataframe by descending order\n",
        "    print(\"\\n ppandas P({}|{}) , n={} \\n \".format(query_attribute,evidence,num_samples))\n",
        "    q1 = pd1.query([query_attribute],evidence_vars=evidence)\n",
        "    print(q1)\n",
        "    q1 = pd1.map_query([query_attribute],evidence_vars=evidence)\n",
        "    #pd_join.visualise()\n",
        "    return q1\n",
        "\n",
        "q1 = ppandas_query(sample1_df,num_samples,query_attribute='B',evidence={'A':0})\n",
        "q1 = ppandas_query(sample1_df,num_samples,query_attribute='B',evidence={'A':1})\n",
        "#print(ppandas_C)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ppandas P(B|{'A': 0}) , n=500 \n",
            " \n",
            "     B  Probability(B)\n",
            "0  0.0        0.770732\n",
            "1  1.0        0.229268\n",
            "\n",
            " ppandas P(B|{'A': 1}) , n=500 \n",
            " \n",
            "     B  Probability(B)\n",
            "0  0.0        0.318644\n",
            "1  1.0        0.681356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3YIf_-iAm8"
      },
      "source": [
        "# VAE-MRF Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Re5YHgVF-q"
      },
      "source": [
        "A Gaussian Markov Random Field is a Gaussian Process with a linear kernel (covariance function). $k(x,x') = x \\cdot x'$\n",
        "\n",
        "\n",
        "Pg 16: \n",
        "\"Fortunately,\n",
        "in probabilistic terms this operation is extremely simple, corresponding to conditioning the joint Gaussian prior distribution on the observations\"\n",
        "\n",
        "## Multivariate Normal\n",
        "Koller Equation 7.3: \\\\\n",
        "$P(z_A,z_B) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_A \\\\ \\mu_B \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{A} & \\Sigma_{AB} \\\\ \\Sigma_{BA} & \\Sigma_{B} \\end{array} \\right] \\right) $ \n",
        "\n",
        "which is equivalent to the Matrix Cookbook (353 and 354) https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf: \\\\\n",
        "$P(z_A|z_B) = Normal_{z_A}(\\hat{\\mu}_A, \\hat{\\Sigma}_A)$ \\\\\n",
        "where: \\\\\n",
        "$\\hat{\\mu}_A = \\mu_A + \\Sigma_{AB} \\Sigma_{B}^{-1}(z_B - \\mu_B)$ \\\\\n",
        "$\\hat{\\Sigma}_A = \\Sigma_A - \\Sigma_{AB} \\Sigma_B^{-1} \\Sigma_{AB}^T$ \\\\\n",
        "\n",
        "$P(z_B|z_A) = Normal_{z_B}(\\hat{\\mu}_B, \\hat{\\Sigma}_B)$ \\\\\n",
        "where: \\\\\n",
        "$\\hat{\\mu}_B = \\mu_B + \\Sigma_{AB}^T \\Sigma_{A}^{-1}(z_A - \\mu_A)$ \\\\\n",
        "$\\hat{\\Sigma}_B = \\Sigma_B - \\Sigma_{AB}^T \\Sigma_A^{-1} \\Sigma_{AB}$ \\\\\n",
        "\n",
        "\n",
        "The output of the VAE encoders are assumed to be the mean and variance of the unary normal potentials in the MRF over the latent z's where:\n",
        "\n",
        "•\tMean: $\\mu_{A}$ and diagonal variance matrix: $\\Sigma_{A}$ are the outputs of the A encoder \\\\\n",
        "•\t$\\mu_{B}$,  $\\Sigma_{B}$ are the outputs of the B encoder \\\\\n",
        "\n",
        "\n",
        "The additional pairwise k-ary Normal potentials, which represent undirected graphical model structure between the latent A and latent B : \\\\\n",
        "•\t$\\Sigma_{AB}$ = $\\Sigma_{BA}^T$ \n",
        "\n",
        "If the latent space is dimension 3, each $\\mu \\in \\mathcal{R}^{1 \\times 3}$ and each $\\Sigma \\in \\mathcal{R}^{3 \\times 3}$.\n",
        "\n",
        "\n",
        "#Three Options to learn Gaussian MRF (in Stage 2):\n",
        "## 1. Fully Emperical\n",
        "Emperically estimate $\\mu_A, \\mu_B, \\Sigma_A$ and $\\Sigma_B$ by taking sample mean of the mu and logvar output of the entire population from marginal encoders.\n",
        "\n",
        "Emperically estimate $\\Sigma_{AB}$ by using sampled z_A and z_B of the entire population (N=num_samples=500) and the sample covariance formula. Note could also use to Q formula to calculate entire covariance matrix and estimate $\\Sigma_A$ and $\\Sigma_B$ as well, but would be less accurate than directly using sample mean of the logvar output from the marginal encoders.\n",
        "\n",
        "$\\Sigma_{AB} = \\frac{1}{N-1} (\\sum_{i=1}^N (z_{i,A} - \\mu_A) (z_{i,B} - \\mu_B))$\n",
        "\n",
        "Note we use more reliable $\\mu_A, \\mu_B$ instead of sample mean of $z_A, z_B$\n",
        "https://www.itl.nist.gov/div898/handbook/pmc/section5/pmc541.htm\n",
        "\n",
        "## 2. Fully Learned\n",
        "Learn $\\Sigma_A$ and $\\Sigma_B$ by gradient descent which involves doing gradient descent through an inverse.\n",
        "\n",
        "Learn $\\Sigma_{AB} by gradient descent\n",
        "\n",
        "## 3. Half Emperical Half Learned\n",
        "Emperically estimate $\\Sigma_A$ and $\\Sigma_B$ using Option 1 to avoid gradient descent through an inverse.\n",
        "\n",
        "Learn $\\Sigma_{AB}$ using Option 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training the VAE-MRF\n",
        "Assuming no missing data. Training is done in two stages. The first stage learns marginal VAEs for each attribute. The second stage learns the covariance matrix $\\Sigma_{AB}$ to capture the intervariable dependencies between A and B.\n",
        "\n",
        "##Stage 1 - Marginal VAEs (Identical to Stage 1 VAEM)\n",
        "Train A and B VAEs separately.\n",
        "In each epoch, break the training data into batches. Each batch contains samples of OHE input $x_A$ or $x_B$:\n",
        "- Feed in $x_A$ or $x_B$ to their respective encoders to obtain either:\n",
        "  - $\\mu_A, \\Sigma_A$ from encoder A\n",
        "  - $\\mu_B, \\Sigma_B$ from encoder B\n",
        "\n",
        "- To reconstruct $x_A$ ($x_B$):\n",
        "  - Sample $z_A$ ($z_B$) using $\\mu_A, \\Sigma_A$ ($\\mu_B, \\Sigma_B$) through standard VAE reparameterization trick\n",
        "  - Feed $z_A$  ($z_B$) into the **A** (**B**) decoder to obtain the reconstruction $\\hat{x}_A$ ($\\hat{x}_B$)\n",
        "\n",
        "- Sum the losses (reconstruction error and KL-divergence) from either A or B  and backpropagate once per batch.\n",
        "\n",
        "For marginal VAEs, fix the parameters: encoder $\\phi$ and decoder $\\theta$.\n",
        "\n",
        "## Stage 2 - Intervariable Dependency CRF\n",
        "In each epoch, break the training data into batches. Each batch contains samples of OHE input $x_A$ and $x_B$. By reconstructing $x_A, x_B$ from $x_A$ and $x_B$, learn $\\Sigma_{AB}$:\n",
        "  - Feed entire batch of $x_A$ to marginal A encoder to obtain Monte Carlo emperical $\\mu_A, \\Sigma_A$\n",
        "  - Feed entire batch of $x_B$ to marginal B encoder to obtain Monte Carlo emperical $\\mu_B, \\Sigma_B$\n",
        "  - If memory allows - feed in entire train population of $x_A$ and $x_B$ for more reliable emperical estimates (as is done with 2 binary variables A, B and 500 samples)\n",
        "  - To reconstruct a specific $x_A$:\n",
        "      - Feed specific corresponding $x_B$ to marginal encoder to obtain sample $z_B$ (standard VAE reparameterization trick)\n",
        "      - Using $z_B,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_A$ from $P(z_A|z_B)$ (modified VAE reparameterization trick)\n",
        "      - Feed $z_A$ into the A decoder to obtain the reconstruction $\\hat{x}_A$\n",
        "\n",
        "  - To reconstruct $x_B$:\n",
        "    - Feed specific corresponding $x_A$ to marginal encoder to obtain sample $z_A$ (standard VAE reparameterization trick) \n",
        "    - Using $z_A,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$ (modified VAE reparameterization trick)\n",
        "    - Feed $z_B$ into the B decoder to obtain the reconstruction $\\hat{x}_B$\n",
        "\n",
        "\n",
        "Sum the losses (reconstruction error and KL-divergence) from both A and B and backpropagate once per batch.\n",
        "Repeat for each batch. \\\\\n",
        "\n",
        "Note that $\\mu_A$, $\\Sigma_A$, $\\mu_B$, $\\Sigma_B$ are fixed for each batch.  There is only one $\\Sigma_{AB}$ to be shared. \n",
        "\n",
        "# Stage 2 - Intervariable Dependency CRF (missing data at train time scenario, not implemented)\n",
        "Training the VAE-MRF on x_A only: \n",
        "- Feed entire batch of $x_A$ to marginal A encoder to obtain emperical $\\mu_A, \\Sigma_A$\n",
        "-  Feed entire batch of $x_B$ to marginal B encoder to obtain emperical $\\mu_B, \\Sigma_B$ (If no $x_B$ assume prior P(zB) = Normal (0, Identity))\n",
        "- Sample $z_B$ using $\\mu_B, \\Sigma_B$ (standard VAE reparameterization trick)\n",
        "- Using $z_B,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_A$ from $P(z_A|z_B)$ (modified VAE reparameterization trick)\n",
        "-  Feed $z_A$ into the A decoder to obtain the reconstruction $\\hat{x}_A$ for $x_A$\n",
        "- Sum the losses (reconstruction error and KL-divergence) from A and backpropagate once per batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UMLBM0iE4y"
      },
      "source": [
        "# VAE Parameters\n",
        "num = 2 # digits from 0 to 1\n",
        "latent_dims = 1 # Latent z_A, z_B same dimension size\n",
        "num_epochs = 300\n",
        "batch_size = 25\n",
        "learning_rate = 1e-2 #1e-4\n",
        "use_gpu = True\n",
        "variational_beta = 0.0001 #tuned 0.001\n",
        "\n",
        "batch_size_list = [25,50]\n",
        "learning_rate_list = [1e-2, 1e-3, 1e-4]\n",
        "variational_beta_list = [0.1,0.001,0.0001]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifSVkjMe-lJj"
      },
      "source": [
        "def vae_loss(batch_recon, batch_targets, mu, logvar):\r\n",
        "  criterion = nn.CrossEntropyLoss()\r\n",
        "  CE = criterion(batch_recon, batch_targets)\r\n",
        "  #print(CE)\r\n",
        "  KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\r\n",
        "  #print(KLd)\r\n",
        "  return CE,variational_beta*KLd, CE + variational_beta*KLd\r\n",
        "\r\n",
        "#Train marginal VAE\r\n",
        "def trainVAE(VAE, sample1_OHE, attribute: str):\r\n",
        "  print(\"\\nTraining marginal VAE for \" + attribute+ \" started!\")\r\n",
        "  VAE.train() #set model mode to train\r\n",
        "  optimizer = torch.optim.Adam(params = VAE.parameters(), lr = learning_rate)\r\n",
        "  x = sample1_OHE.filter(like=attribute, axis=1).values\r\n",
        "  #sample2_OHE when do BC plate\r\n",
        "  \r\n",
        "  inds = list(range(x.shape[0]))\r\n",
        "  N = num_samples\r\n",
        "  freq = num_epochs // 10 # floor division\r\n",
        "\r\n",
        "  loss_hist = []\r\n",
        "  x = Variable(torch.from_numpy(x))\r\n",
        "  \r\n",
        "  for epoch in range(num_epochs):\r\n",
        "      VAE.train()\r\n",
        "      #print('epoch' + str(epoch))\r\n",
        "      inds = np.random.permutation(inds)\r\n",
        "      x = x[inds]\r\n",
        "      x = x.to(device)\r\n",
        "      \r\n",
        "      loss = 0\r\n",
        "      CE = 0\r\n",
        "      KLd = 0\r\n",
        "      #num_batches = N / batch_size\r\n",
        "      for b in range(0, N, batch_size):\r\n",
        "          #get the mini-batch\r\n",
        "          x_batch = x[b: b+batch_size]\r\n",
        "          #feed forward\r\n",
        "          batch_recon,latent_mu,latent_logvar = VAE.forward(x_batch.float())\r\n",
        "          # Error\r\n",
        "          #Convert x_batch from OHE vectors to single scalar\r\n",
        "          # max returns index location of max value in each sample of batch \r\n",
        "          _, x_batch_targets = x_batch.max(dim=1)\r\n",
        "          train_CE, train_KLd, train_loss = vae_loss(batch_recon, x_batch_targets, latent_mu, latent_logvar)\r\n",
        "          loss += train_loss.item() / N # update epoch loss\r\n",
        "          CE += train_CE.item() / N\r\n",
        "          KLd += train_KLd.item() / N\r\n",
        "\r\n",
        "          #Backprop the error, compute the gradient\r\n",
        "          optimizer.zero_grad()\r\n",
        "          train_loss = train_loss\r\n",
        "          train_loss.backward()\r\n",
        "\r\n",
        "          #update parameters based on gradient\r\n",
        "          optimizer.step()\r\n",
        "          \r\n",
        "      #Record loss per epoch        \r\n",
        "      loss_hist.append(loss)\r\n",
        "      \r\n",
        "      if epoch % freq == 0:\r\n",
        "          print('')\r\n",
        "          print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\r\n",
        "\r\n",
        "          #Test with all training data\r\n",
        "          VAE.eval()\r\n",
        "          train_recon, train_mu, train_logvar = VAE.forward(x.float())\r\n",
        "          _, x_targets = x.max(dim=1)\r\n",
        "          CE_,KLd,test_loss = vae_loss(train_recon, x_targets, train_mu, train_logvar)\r\n",
        "          print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\r\n",
        "\r\n",
        "          #print('Visualize ' + attribute + 'predictions')\r\n",
        "          #print(train_recon[0:5])\r\n",
        "          #print(x_targets[0:5])\r\n",
        "\r\n",
        "  print(\"\\nTraining marginal VAE for \" + attribute+ \" finished!\")\r\n",
        "  #print(loss_hist)\r\n",
        "\r\n",
        "#Each attribute has a marginal VAE\r\n",
        "class marginal_VAE(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.latent_dims = latent_dims\r\n",
        "        self.fc1 = nn.Linear(num, latent_dims)\r\n",
        "        self.fc_mu = nn.Linear(latent_dims, latent_dims)\r\n",
        "        self.fc_logvar = nn.Linear(latent_dims, latent_dims)\r\n",
        "        self.fc_out = nn.Linear(latent_dims,num)\r\n",
        "    \r\n",
        "    #accepts OHE input of an attribute, returns mu and log variance\r\n",
        "    def encode(self, x):\r\n",
        "        h1 = torch.sigmoid(self.fc1(x))\r\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\r\n",
        "\r\n",
        "    #Given mu and logvar generates latent z \r\n",
        "    def reparameterize(self, mu, logvar):\r\n",
        "        std = torch.exp(0.5*logvar) \r\n",
        "        eps = torch.randn_like(std)\r\n",
        "        return mu + eps*std\r\n",
        "\r\n",
        "    #Decodes latent z into reconstruction with dimension equal to num\r\n",
        "    def decode(self, z):\r\n",
        "        if z.size()[0] == self.latent_dims: #resize from [1] to [1,1]\r\n",
        "          z = z.view(1, self.latent_dims)\r\n",
        "        softmax = nn.Softmax(dim=1)  #normalizes reconstruction to range [0,1] and sum to 1\r\n",
        "        recon = softmax(self.fc_out(z))\r\n",
        "        return recon\r\n",
        "    \r\n",
        "    #Given x, returns: reconstruction x_hat, mu, log_var\r\n",
        "    def forward(self, x):\r\n",
        "        mu, logvar = self.encode(x)\r\n",
        "        z = self.reparameterize(mu, logvar)\r\n",
        "        return self.decode(z), mu, logvar\r\n",
        "    \r\n",
        "    #Given x, returns latent z\r\n",
        "    def latent(self, x):\r\n",
        "        mu, logvar = self.encode(x)\r\n",
        "        z = self.reparameterize(mu, logvar)\r\n",
        "        return z\r\n",
        "    \r\n",
        "    # ignore latent_mu, latent_logvar, instead generate z values from standard normal\r\n",
        "    def sample(self, num_samples):\r\n",
        "      z = torch.randn(num_samples, self.latent_dims)\r\n",
        "      z = z.to(device)\r\n",
        "      samples = self.decode(z)\r\n",
        "      return samples"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FiF8-RkNLB"
      },
      "source": [
        "class VariationalAutoencoder_MRF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.latent_dims = latent_dims\n",
        "        #Marginal VAEs\n",
        "        self.VAE_A = marginal_VAE()\n",
        "        self.VAE_B = marginal_VAE()\n",
        "        #Emperical mu and logvar\n",
        "        self.muA_emp = 0\n",
        "        self.muB_emp = 0\n",
        "        self.logvarA_emp = 0\n",
        "        self.logvarB_emp = 0\n",
        "        #Sigma_{AB} = Sigma_{BA}^T\n",
        "        self.covarianceAB = torch.randn(size=(self.latent_dims,self.latent_dims))\n",
        "        self.covarianceAB = torch.nn.Parameter(self.covarianceAB,requires_grad=True)\n",
        "        #print(self.covarianceAB)\n",
        "\n",
        "\n",
        "    #Stage 1 - Train Marginal VAEs and then freeze parameters\n",
        "    def train_marginals(self):\n",
        "        trainVAE(self.VAE_A,sample1_OHE, 'A')\n",
        "        trainVAE(self.VAE_B,sample1_OHE, 'B')\n",
        "\n",
        "    #Finds emperical mean and logvar of each attribute for entire population\n",
        "    def emp_mu_logvar(self,x,attribute):\n",
        "      if attribute == 'A':\n",
        "        muA, logvarA = self.encode(x, attribute)\n",
        "        #print(\"muA of each sample\")\n",
        "        #print(muA)\n",
        "        self.muA_emp = torch.mean(muA,0,keepdim=True) #need to modify if muA is more than 1 dimension\n",
        "\n",
        "        self.logvarA_emp = torch.mean(logvarA,0,keepdim=True)\n",
        "        print('Emperical A mu and logvar')\n",
        "        print(self.muA_emp)\n",
        "        print(self.logvarA_emp)\n",
        "      elif attribute == 'B':\n",
        "        muB, logvarB = self.encode(x, attribute)\n",
        "        self.muB_emp = torch.mean(muB,0,keepdim=True)\n",
        "        self.logvarB_emp = torch.mean(logvarB,0,keepdim=True)\n",
        "        print('Emperical B mu and logvar')\n",
        "        print(self.muB_emp)\n",
        "        print(self.logvarB_emp)\n",
        "      return\n",
        "\n",
        "    def emp_covariance(self,xA,xB):\n",
        "      np_zA = np.empty(0)\n",
        "      #Need to vectorize instead of for loop\n",
        "      for x in xA:\n",
        "        zA = self.latent(x.float(), attribute='A')\n",
        "        np_zA = np.concatenate((np_zA, zA.cpu().detach().numpy()))\n",
        "      np_zA = np_zA.reshape(num_samples,latent_dims)\n",
        "      \n",
        "      np_zB = np.empty(0)\n",
        "      for x in xB:\n",
        "        zB = self.latent(x.float(), attribute='B')\n",
        "        np_zB = np.concatenate((np_zB, zB.cpu().detach().numpy()))\n",
        "      np_zB = np_zB.reshape(num_samples,latent_dims)\n",
        "\n",
        "      #zA = self.latent(xA,'A')\n",
        "      #zB = self.latent(xB,'B')\n",
        "      #Combine pytorch tensors to single numpy array\n",
        "      #zA_np = zA.cpu().detach().numpy()\n",
        "      #print(\"zA_np\")\n",
        "      #print(zA_np)\n",
        "      #zB_np = zB.cpu().detach().numpy()\n",
        "      z_obs = np.concatenate((np_zA, np_zB),axis=1) #(num_samples,2)\n",
        "      print(z_obs)\n",
        "      print(np.shape(z_obs))\n",
        "      print(\"Means of zA,zB\")\n",
        "      print(np.mean(z_obs,axis=0))\n",
        "      print(\"Covariance Matrix zAzB\")\n",
        "      print(np.cov(z_obs,rowvar=0))\n",
        "      # q entry formula\n",
        "      #covarianceAB_emp = (1 / (num_samples-1)) *\n",
        "      # Q matrix formula\n",
        "      #covarianceAB_emp = \n",
        "\n",
        "    # Conditional of Multivariate Gaussian: matrix cookbook 353 and 354\n",
        "    # Attribute is the attribute of the returned z_cond \n",
        "    def conditional(self, muA, logvarA, muB, logvarB, z, attribute):\n",
        "        #log-space for numerical stability.\n",
        "        logvarA = torch.exp(0.5*logvarA)\n",
        "        logvarB = torch.exp(0.5*logvarB)\n",
        "        covarianceA = torch.diag_embed(logvarA) #Convert logvar vector to diagonal matrix\n",
        "        covarianceB = torch.diag_embed(logvarB) #batch_size,3,3\n",
        "        #self.covarianceAB = torch.nn.Parameter(torch.log(self.covarianceAB))\n",
        "        muA = muA.unsqueeze(2)\n",
        "        muB = muB.unsqueeze(2)\n",
        "        z = z.unsqueeze(2)\n",
        "        if attribute == 'A':\n",
        "          mu_cond = muA + torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                    torch.inverse(covarianceB)),\n",
        "                                      (z - muB)) # z is zB\n",
        "          logvar_cond = covarianceA - torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                                torch.inverse(covarianceB)),\n",
        "                                                  torch.transpose(self.covarianceAB,0,1))\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims) # regularization\n",
        "        elif attribute == 'B':\n",
        "          mu_cond = muB + torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1),\n",
        "                                                    torch.inverse(covarianceA)), \n",
        "                                       (z - muA)) # z is zA\n",
        "          logvar_cond = covarianceB - torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1), \n",
        "                                                              torch.inverse(covarianceA)),\n",
        "                                                 self.covarianceAB)\n",
        "              # logvar_cond is not a diagonal covariance matrix\n",
        "          #logvar_cond = logvar_cond + 20*torch.eye(latent_dims)\n",
        "\n",
        "        # METHOD1: re-parameterization trick to sample z_cond\n",
        "        eps = torch.randn_like(mu_cond) #64x3x1, 64x3x3 if use logvar_cond\n",
        "        z_cond = mu_cond + torch.matmul(logvar_cond,eps) #64x3x1\n",
        "        z_cond = z_cond.squeeze(2) #64x3\n",
        "        return z_cond\n",
        "\n",
        "    #return mu, logvar\n",
        "    def encode(self, x, attribute):\n",
        "      if attribute == 'A':\n",
        "        return self.VAE_A.encode(x)\n",
        "      elif attribute =='B':\n",
        "        return self.VAE_B.encode(x)\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "    \n",
        "    #return reconstruction\n",
        "    def decode(self, z, attribute):\n",
        "      if attribute == 'A':\n",
        "        return self.VAE_A.decode(z)\n",
        "      elif attribute =='B':\n",
        "        return self.VAE_B.decode(z)\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "    \n",
        "    #Given xA, xB and attribute to reconstruct, return reconstruction\n",
        "    def forward(self, xA, xB, attribute):\n",
        "      muA, logvarA = self.encode(xA, attribute='A') #logvar is size [64,3]\n",
        "      muB, logvarB = self.encode(xB, attribute='B')\n",
        "      # Take batch emperical average of mus and logvars\n",
        "      #size_placeholder = muA.size() #[batch_size,latent_dims]\n",
        "      #muA_emp = torch.mean(muA,0,keepdim=True).repeat(size_placeholder,1) #(batchsize,latent_dims) all repeated values of avg\n",
        "      #logvarA_emp = torch.mean(logvarA,0,keepdim=True).repeat(size_placeholder,1)\n",
        "      #muB_emp = torch.mean(muB,0,keepdim=True).repeat(size_placeholder,1)\n",
        "      #logvarB_emp = torch.mean(logvarB,0,keepdim=True).repeat(size_placeholder,1)\n",
        "      #print(logvarA)\n",
        "      if attribute == 'A':\n",
        "        zB = self.VAE_B.reparameterize(muB, logvarB)\n",
        "        zA = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zB, attribute)\n",
        "        return self.decode(zA,attribute), self.muA_emp, self.logvarA_emp #should error use emperical avg or not?\n",
        "      elif attribute == 'B':\n",
        "        zA = self.VAE_A.reparameterize(muA, logvarA)\n",
        "        zB = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zA, attribute)\n",
        "        return self.decode(zB,attribute), self.muB_emp, self.logvarB_emp\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "\n",
        "    def latent(self,x,attribute):\n",
        "      if attribute == 'A':\n",
        "          return self.VAE_A.latent(x)\n",
        "      elif attribute == 'B':\n",
        "        return self.VAE_B.latent(x)\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "\n",
        "    #Given x, returns: reconstruction x_hat, mu, log_var\n",
        "    def forward_single_attribute(self, x, attribute):\n",
        "      if attribute == 'A':\n",
        "        return self.VAE_A.forward(x)\n",
        "      elif attribute == 'B':\n",
        "        return self.VAE_B.forward(x)\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "\n",
        "    def query_single_attribute(self, x_evidence, evidence_attribute):\n",
        "      if evidence_attribute =='A':\n",
        "        muA,logvarA = self.encode(x_evidence, evidence_attribute)\n",
        "        #muB = torch.zeros(muA.size()) #100x3\n",
        "        #logvarB = torch.ones(muA.size()) #100x3\n",
        "        zA = self.VAE_A.reparameterize(muA, logvarA)\n",
        "        print(\"zA\")\n",
        "        print(zA)\n",
        "        #Use emperical mus and logvars\n",
        "        zB = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zA, attribute='B')\n",
        "        print(\"zB\")\n",
        "        print(zB)\n",
        "        return self.decode(zB,attribute='B')\n",
        "\n",
        "      elif evidence_attribute =='B':\n",
        "        muB,logvarB = self.encode(x_evidence, evidence_attribute)\n",
        "        zB = self.VAE_B.reparameterize(muB, logvarB)\n",
        "        zA = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zB, attribute='A')\n",
        "        return self.decode(zA,attribute='A')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7LH-GQRW01"
      },
      "source": [
        "def trainVAE_MRF(VAE_MRF, emperical):\n",
        "  VAE_MRF.train() #set model mode to train\n",
        "  xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "  xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "  #print(xA.shape)\n",
        "\n",
        "  #sample2_OHE when do BC plate\n",
        "  \n",
        "  indsA = list(range(xA.shape[0]))\n",
        "  indsB = list(range(xB.shape[0]))\n",
        "  N = num_samples # 1000\n",
        "  freq = num_epochs // 10 # floor division\n",
        "\n",
        "  loss_hist = []\n",
        "  xA = Variable(torch.from_numpy(xA))\n",
        "  xB = Variable(torch.from_numpy(xB))\n",
        "  \n",
        "  #Calculate mu_emp,logvar_emp for all attributes (entire sample)\n",
        "  VAE_MRF.emp_mu_logvar(xA.float(),attribute='A')\n",
        "  VAE_MRF.emp_mu_logvar(xB.float(),attribute='B')\n",
        "  if emperical:\n",
        "      VAE_MRF.emp_covariance(xA.float(),xB.float())\n",
        "  else:\n",
        "    for epoch in range(num_epochs):\n",
        "        VAE_MRF.train()\n",
        "        #print('epoch' + str(epoch))\n",
        "        indsA = np.random.permutation(indsA)\n",
        "        xA = xA[indsA]\n",
        "        xA = xA.to(device)\n",
        "        indsB = np.random.permutation(indsB)\n",
        "        xB = xB[indsB]\n",
        "        xB = xB.to(device)\n",
        "\n",
        "        loss = 0\n",
        "        CE = 0\n",
        "        KLd = 0\n",
        "        num_batches = N / batch_size\n",
        "        for b in range(0, N, batch_size):\n",
        "            #get the mini-batch\n",
        "            x_batchA = xA[b: b+batch_size]\n",
        "            x_batchB = xB[b: b+batch_size]\n",
        "            \n",
        "            #feed forward, should latent mu and logvar be the same for every recon, yes since want to learn covariance accurately?\n",
        "            batch_reconA,latent_muA,latent_logvarA = VAE_MRF.forward(x_batchA.float(),x_batchB.float(),attribute='A')\n",
        "            batch_reconB,latent_muB,latent_logvarB = VAE_MRF.forward(x_batchA.float(),x_batchB.float(),attribute='B')\n",
        "\n",
        "            # Error\n",
        "            #Convert x_batchA and x_batchB from OHE vectors to single scalar\n",
        "            # max returns index location of max value in each sample of batch \n",
        "            _, xA_batch_targets = x_batchA.max(dim=1)\n",
        "            _, xB_batch_targets = x_batchB.max(dim=1)\n",
        "            train_CE_A, train_KLd_A, train_loss_A = vae_loss(batch_reconA, xA_batch_targets, latent_muA, latent_logvarA)\n",
        "            train_CE_B, train_KLd_B, train_loss_B = vae_loss(batch_reconB, xB_batch_targets, latent_muB, latent_logvarB)\n",
        "            loss += train_loss_A.item() / N # update epoch loss\n",
        "            loss += train_loss_B.item() / N\n",
        "            CE += train_CE_A.item() / N\n",
        "            CE += train_CE_B.item() / N \n",
        "            KLd += train_KLd_A.item() / N\n",
        "            KLd += train_KLd_B.item() / N\n",
        "\n",
        "            #Backprop the error, compute the gradient\n",
        "            optimizer.zero_grad()\n",
        "            train_loss = train_loss_A + train_loss_B\n",
        "            train_loss.backward()\n",
        "            \n",
        "            #update parameters based on gradient\n",
        "            optimizer.step()\n",
        "            \n",
        "        #Record loss per epoch        \n",
        "        loss_hist.append(loss)\n",
        "        \n",
        "        if epoch % freq == 0:\n",
        "            print('')\n",
        "            print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\n",
        "\n",
        "            #Test with all training data\n",
        "            VAE_MRF.eval()\n",
        "            train_reconA, train_muA, train_logvarA = VAE_MRF.forward(xA.float(),xB.float(), attribute='A')\n",
        "            train_reconB, train_muB, train_logvarB = VAE_MRF.forward(xA.float(),xB.float(), attribute='B')\n",
        "            _, xA_targets = xA.max(dim=1)\n",
        "            _, xB_targets = xB.max(dim=1)\n",
        "            CE_A,KLd_A,test_loss_A = vae_loss(train_reconA, xA_targets, train_muA, train_logvarA)\n",
        "            CE_B,KLd_B,test_loss_B = vae_loss(train_reconB, xB_targets, train_muB, train_logvarB)\n",
        "\n",
        "            CE = CE_A + CE_B\n",
        "            Kld = KLd_A + KLd_B\n",
        "            test_loss = test_loss_A + test_loss_B\n",
        "            print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\n",
        "        \n",
        "    print(\"\\nTraining MRF finished!\")\n",
        "  #print(loss_hist)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjRUnGgjnIvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ea23f8-e20b-4c2f-89bf-d111b547e67c"
      },
      "source": [
        "# Focus on just AB Plate for now\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "VAE_MRF = VariationalAutoencoder_MRF()\n",
        "VAE_MRF = VAE_MRF.to(device)\n",
        "\n",
        "VAE_MRF.train_marginals() #Stage 1, then freeze marginal VAEs\n",
        "print('Parameters for Marginal VAEs fixed')\n",
        "for param in VAE_MRF.VAE_A.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in VAE_MRF.VAE_B.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "num_params = sum(p.numel() for p in VAE_MRF.parameters() if p.requires_grad)\n",
        "print(\"Number of parameters: %d\" % num_params) #8*3 + 3 = 27, 3*8 + 8 = 32 3*3+3 = 12 *2 = 24, 27+32+24=83\n",
        "\n",
        "#for param in VAE_MRF.parameters():\n",
        "#    print(type(param.data), param.size())\n",
        "#print(list(VAE_MRF.parameters()))\n",
        "#print(VAE_MRF.parameters)\n",
        "\n",
        "\n",
        "# optimizer object\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, VAE_MRF.parameters()), lr = learning_rate)\n",
        "print(\"CovarianceAB before training\")\n",
        "print(VAE_MRF.covarianceAB.cpu().detach().numpy())\n",
        "num_epochs = 2000\n",
        "trainVAE_MRF(VAE_MRF,emperical=True)\n",
        "print(\"CovarianceAB after emperically estimating or learning by gradient descent\")\n",
        "print(VAE_MRF.covarianceAB.cpu().detach().numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training marginal VAE for A started!\n",
            "\n",
            "Epoch 1/300\t CE: 0.02777, KLd: 0.00001, Train loss=0.02778\t\t CE: 0.02777, KLd: 0.00598, Test loss: 0.69085\n",
            "Epoch 31/300\t CE: 0.01259, KLd: 0.00023, Train loss=0.01281\t\t CE: 0.01259, KLd: 0.11394, Test loss: 0.42879\n",
            "Epoch 61/300\t CE: 0.01255, KLd: 0.00020, Train loss=0.01275\t\t CE: 0.01255, KLd: 0.09849, Test loss: 0.41242\n",
            "Epoch 91/300\t CE: 0.01256, KLd: 0.00018, Train loss=0.01274\t\t CE: 0.01256, KLd: 0.09041, Test loss: 0.40413\n",
            "Epoch 121/300\t CE: 0.01254, KLd: 0.00017, Train loss=0.01272\t\t CE: 0.01254, KLd: 0.08677, Test loss: 0.40041\n",
            "Epoch 151/300\t CE: 0.01256, KLd: 0.00017, Train loss=0.01273\t\t CE: 0.01256, KLd: 0.08881, Test loss: 0.40234\n",
            "Epoch 181/300\t CE: 0.01254, KLd: 0.00016, Train loss=0.01270\t\t CE: 0.01254, KLd: 0.08001, Test loss: 0.39350\n",
            "Epoch 211/300\t CE: 0.01255, KLd: 0.00015, Train loss=0.01270\t\t CE: 0.01255, KLd: 0.07749, Test loss: 0.39106\n",
            "Epoch 241/300\t CE: 0.01255, KLd: 0.00014, Train loss=0.01269\t\t CE: 0.01255, KLd: 0.07192, Test loss: 0.38580\n",
            "Epoch 271/300\t CE: 0.01254, KLd: 0.00014, Train loss=0.01268\t\t CE: 0.01254, KLd: 0.07088, Test loss: 0.38425\n",
            "Training marginal VAE for A finished!\n",
            "\n",
            "Training marginal VAE for B started!\n",
            "\n",
            "Epoch 1/300\t CE: 0.02958, KLd: 0.00001, Train loss=0.02960\t\t CE: 0.02958, KLd: 0.00737, Test loss: 0.72149\n",
            "Epoch 31/300\t CE: 0.01260, KLd: 0.00021, Train loss=0.01281\t\t CE: 0.01260, KLd: 0.10335, Test loss: 0.41826\n",
            "Epoch 61/300\t CE: 0.01257, KLd: 0.00019, Train loss=0.01275\t\t CE: 0.01257, KLd: 0.09362, Test loss: 0.40751\n",
            "Epoch 91/300\t CE: 0.01255, KLd: 0.00018, Train loss=0.01273\t\t CE: 0.01255, KLd: 0.08844, Test loss: 0.40215\n",
            "Epoch 121/300\t CE: 0.01255, KLd: 0.00017, Train loss=0.01272\t\t CE: 0.01255, KLd: 0.08458, Test loss: 0.39804\n",
            "Epoch 151/300\t CE: 0.01254, KLd: 0.00015, Train loss=0.01270\t\t CE: 0.01254, KLd: 0.07588, Test loss: 0.38969\n",
            "Epoch 181/300\t CE: 0.01254, KLd: 0.00015, Train loss=0.01269\t\t CE: 0.01254, KLd: 0.07735, Test loss: 0.39118\n",
            "Epoch 211/300\t CE: 0.01255, KLd: 0.00015, Train loss=0.01270\t\t CE: 0.01255, KLd: 0.07607, Test loss: 0.38947\n",
            "Epoch 241/300\t CE: 0.01254, KLd: 0.00015, Train loss=0.01269\t\t CE: 0.01254, KLd: 0.07493, Test loss: 0.38839\n",
            "Epoch 271/300\t CE: 0.01254, KLd: 0.00015, Train loss=0.01269\t\t CE: 0.01254, KLd: 0.07400, Test loss: 0.38746\n",
            "Training marginal VAE for B finished!\n",
            "Parameters for Marginal VAEs fixed\n",
            "Number of parameters: 1\n",
            "CovarianceAB before training\n",
            "[[-2.406624]]\n",
            "Emperical A mu and logvar\n",
            "tensor([[0.0511]])\n",
            "tensor([[-2.4894]])\n",
            "Emperical B mu and logvar\n",
            "tensor([[0.0252]])\n",
            "tensor([[-2.4330]])\n",
            "[[ 1.00147331 -1.45288944]\n",
            " [ 1.18880093 -1.00915647]\n",
            " [ 1.65212321 -1.24907017]\n",
            " [ 0.13029468  1.59519279]\n",
            " [ 1.15728402  1.26667774]\n",
            " [ 0.71015656 -1.56499779]\n",
            " [-1.2025888   0.74033964]\n",
            " [ 1.16225648 -0.94856942]\n",
            " [-1.17205095  1.27006233]\n",
            " [-1.61393571  1.26380968]\n",
            " [ 1.13292086 -1.14316392]\n",
            " [ 1.47612929  1.52501607]\n",
            " [ 1.08405566  1.07203484]\n",
            " [ 0.74778354 -0.9769361 ]\n",
            " [-1.62212634  1.06936669]\n",
            " [-1.70768821  1.21962929]\n",
            " [ 1.30397201 -1.44379818]\n",
            " [-1.17747915 -0.88205075]\n",
            " [-1.43477392  1.06163847]\n",
            " [-1.3151418   0.79444116]\n",
            " [-1.17948973 -1.25259042]\n",
            " [ 1.23686993 -1.85072803]\n",
            " [-1.69510078  1.53635144]\n",
            " [ 1.10881782 -1.1292156 ]\n",
            " [ 1.33881354 -1.24793005]\n",
            " [ 0.82667917 -1.44185162]\n",
            " [ 0.28079432 -1.01352239]\n",
            " [-1.22205639  1.19352579]\n",
            " [ 0.62995708 -1.42179751]\n",
            " [ 1.46334171 -0.79077578]\n",
            " [-1.19605339  0.92871106]\n",
            " [-0.99797428 -0.60717851]\n",
            " [ 1.03016937  0.83558381]\n",
            " [ 1.09440744 -0.68881512]\n",
            " [-1.56503582  1.1720345 ]\n",
            " [ 1.14058614 -0.99008965]\n",
            " [-1.36336637  1.06673241]\n",
            " [ 1.40525293 -1.41927552]\n",
            " [ 0.80914921 -1.19681275]\n",
            " [ 0.75156718 -1.05629528]\n",
            " [ 0.53405237 -0.52051055]\n",
            " [ 1.08437657  1.46622765]\n",
            " [-1.28968513  1.23528218]\n",
            " [-0.83115065 -1.14191389]\n",
            " [ 0.61631453 -1.33613729]\n",
            " [-1.23400772  0.88761365]\n",
            " [ 1.5721972  -1.15230215]\n",
            " [ 0.82194561 -0.72438294]\n",
            " [ 1.44214845  1.36754155]\n",
            " [-0.70492369 -0.73417127]\n",
            " [ 0.93858159  0.95352483]\n",
            " [ 0.69669431 -1.01438689]\n",
            " [ 0.71232057  0.97254896]\n",
            " [-1.3146528   1.10717297]\n",
            " [-1.27432334  1.11221933]\n",
            " [ 0.68919384  1.19739783]\n",
            " [-0.8348946  -1.21984243]\n",
            " [ 1.53577328  1.53165472]\n",
            " [ 1.42170668  1.31168377]\n",
            " [-1.54991221 -0.87208849]\n",
            " [-0.99649978  0.98744112]\n",
            " [ 0.62060857 -1.08663356]\n",
            " [-1.2072978   0.87211055]\n",
            " [ 0.88022625 -1.09209096]\n",
            " [ 0.79092526  1.65896869]\n",
            " [ 1.26449013 -1.04001999]\n",
            " [ 0.7931506   1.10225379]\n",
            " [ 1.15609932 -0.52813643]\n",
            " [ 0.72386968 -1.56622863]\n",
            " [-1.06440341  1.22692466]\n",
            " [ 1.34186029 -1.26356566]\n",
            " [ 0.91812259 -0.87448031]\n",
            " [-0.90543365  0.9297291 ]\n",
            " [ 1.20521355 -1.40581751]\n",
            " [ 0.66214311 -0.81694353]\n",
            " [ 1.18631494 -1.14501441]\n",
            " [ 0.64961195 -0.78039527]\n",
            " [-1.23912442  1.22823906]\n",
            " [-1.40736759  1.36793017]\n",
            " [-1.18081069  1.56567907]\n",
            " [ 0.796143   -1.16704702]\n",
            " [-2.15126944  1.04739332]\n",
            " [-1.38718104  1.20012939]\n",
            " [ 1.23880637 -1.0309912 ]\n",
            " [ 0.77234185 -0.53299856]\n",
            " [-0.98167866 -0.70464218]\n",
            " [ 0.82967889 -1.6543442 ]\n",
            " [ 0.65843642 -1.49001551]\n",
            " [ 0.8210516  -1.27571297]\n",
            " [ 0.1792174   1.2022016 ]\n",
            " [ 1.00182927 -1.3867873 ]\n",
            " [-0.96496499  0.99368548]\n",
            " [-1.04592729  0.808074  ]\n",
            " [-1.57419825  1.27892745]\n",
            " [-1.06521404 -1.72250724]\n",
            " [ 0.17395318 -0.88289332]\n",
            " [ 0.40060365  1.02100825]\n",
            " [-1.21242523  1.26433432]\n",
            " [ 0.71795201  1.04317379]\n",
            " [ 0.78634751 -1.13757789]\n",
            " [-1.50599694 -1.53422821]\n",
            " [ 0.84515268 -0.99381018]\n",
            " [-1.45841026  1.06985283]\n",
            " [ 1.07718635 -0.77904236]\n",
            " [ 1.03455043 -1.26902556]\n",
            " [ 0.99264425  0.77388918]\n",
            " [-1.71215522  1.17581224]\n",
            " [ 0.53486878 -1.0874002 ]\n",
            " [-1.28141832  0.72864127]\n",
            " [ 0.70422125  0.93768024]\n",
            " [-0.91480154  1.02101922]\n",
            " [ 1.20824111 -0.92240906]\n",
            " [ 1.54617608 -0.78916395]\n",
            " [-0.97840106  1.384305  ]\n",
            " [-1.41659462  1.44051957]\n",
            " [-1.14234018 -1.56748331]\n",
            " [-1.3580308   0.9803434 ]\n",
            " [ 0.26572835 -1.01499856]\n",
            " [ 1.08363187  1.203408  ]\n",
            " [-1.60138512  0.94049937]\n",
            " [-1.10839081  0.90360975]\n",
            " [ 0.76095587  1.31281054]\n",
            " [ 0.89336598  1.30869794]\n",
            " [ 1.25786209  0.64904886]\n",
            " [ 0.87345392 -1.65594935]\n",
            " [-1.03317356  1.25220764]\n",
            " [ 0.93769443 -1.82917476]\n",
            " [-1.62524819 -0.86452484]\n",
            " [-0.86546671 -0.99215889]\n",
            " [ 1.50088322 -1.20864666]\n",
            " [ 0.69139135  1.04631245]\n",
            " [-1.5209806   0.9049015 ]\n",
            " [-0.64335704  0.89080268]\n",
            " [ 1.20634031 -1.33857059]\n",
            " [-1.32892621  1.16524351]\n",
            " [ 1.20193505 -1.04679   ]\n",
            " [-0.82772005  1.27906859]\n",
            " [ 0.79119259 -0.71106577]\n",
            " [-1.46163106  1.40062952]\n",
            " [-2.17477846 -1.30694211]\n",
            " [-1.17119122  1.67088604]\n",
            " [ 0.75597024 -1.50477386]\n",
            " [-1.42046726  1.07947266]\n",
            " [ 1.08565021  1.08920038]\n",
            " [-0.9857052   1.48856115]\n",
            " [ 0.68724751 -1.15676689]\n",
            " [ 0.88808501  0.75208569]\n",
            " [-1.42116928 -1.1784724 ]\n",
            " [ 0.93317783 -0.91912216]\n",
            " [ 1.12520552 -0.85448325]\n",
            " [ 0.41330361  1.50927234]\n",
            " [ 1.55414617 -1.24753737]\n",
            " [ 0.89047533 -1.19322526]\n",
            " [ 0.67286921  1.36468887]\n",
            " [ 1.10443318  1.519822  ]\n",
            " [ 0.72742194 -0.85425556]\n",
            " [-1.86263013  1.02440405]\n",
            " [ 0.7032637   0.94988102]\n",
            " [ 1.18220687 -1.37170053]\n",
            " [ 0.96690512 -0.77588964]\n",
            " [-1.49315023 -0.64772284]\n",
            " [ 1.48886979  1.15611649]\n",
            " [-1.32136226  1.06111383]\n",
            " [ 1.20977592  0.67783523]\n",
            " [ 1.18325698  1.19303012]\n",
            " [ 0.97344387 -0.78054559]\n",
            " [-0.97112358  1.01779842]\n",
            " [-1.52636337  1.33903182]\n",
            " [ 0.92625302  1.2970612 ]\n",
            " [ 0.86097097  1.14325523]\n",
            " [ 1.52708364 -0.99295807]\n",
            " [ 0.71569854 -0.85396004]\n",
            " [ 1.50720453  1.44184124]\n",
            " [ 0.99260646 -1.57075608]\n",
            " [ 0.80349755  1.18272126]\n",
            " [-1.5420351   1.3881588 ]\n",
            " [-0.57972699 -0.32940185]\n",
            " [ 0.83006907 -1.93199849]\n",
            " [-0.66534871  1.19281209]\n",
            " [ 0.6495353  -1.64723456]\n",
            " [-1.43117404  1.51360476]\n",
            " [-1.51972854  1.13327193]\n",
            " [-0.97732913  1.88085771]\n",
            " [-1.04788029  1.21654713]\n",
            " [-1.1443615   1.41329312]\n",
            " [ 0.66812778 -1.03746581]\n",
            " [-1.4426471   1.29951358]\n",
            " [ 1.21007574 -1.103091  ]\n",
            " [ 1.00927472 -1.01402366]\n",
            " [ 0.6995998  -1.05544019]\n",
            " [ 1.01015842  1.44778895]\n",
            " [-1.67866433  1.02485192]\n",
            " [ 0.81422019 -0.8843205 ]\n",
            " [ 1.06999159 -1.24918854]\n",
            " [ 0.74129343  1.12283874]\n",
            " [ 0.93827003 -1.35962093]\n",
            " [-1.2049197   0.80873233]\n",
            " [-1.44373214  0.82528663]\n",
            " [ 1.16625905 -0.92727602]\n",
            " [ 0.81133628 -1.16057801]\n",
            " [ 0.45953971  1.55089819]\n",
            " [ 1.12995565 -1.04070628]\n",
            " [ 0.69773233 -1.61144078]\n",
            " [-1.56260729  0.77141148]\n",
            " [-0.91678083  1.25233841]\n",
            " [-1.48762465  1.63787329]\n",
            " [-1.24602544  0.89650917]\n",
            " [-1.38738406  1.35423899]\n",
            " [ 1.07518911 -1.30827308]\n",
            " [ 0.65131921  1.21895528]\n",
            " [ 0.59977221 -1.19799185]\n",
            " [ 0.51643157  1.11539936]\n",
            " [ 0.64374965 -1.03925931]\n",
            " [-1.04640019  1.2365098 ]\n",
            " [-1.0341444  -0.96131968]\n",
            " [-1.31329179 -0.96019   ]\n",
            " [ 1.30656338 -1.39362478]\n",
            " [ 0.98935091 -0.74812913]\n",
            " [ 0.70676225  1.15418088]\n",
            " [-1.53529978  1.39012671]\n",
            " [ 1.30918348 -1.50013399]\n",
            " [-1.18822479  1.05808711]\n",
            " [-1.52844048 -1.23677742]\n",
            " [ 0.83208311  1.46175659]\n",
            " [-0.92971766 -0.7808913 ]\n",
            " [ 0.53803205 -1.33454669]\n",
            " [-1.32037508  1.01282358]\n",
            " [ 1.16630054 -1.05665839]\n",
            " [ 0.65161598 -1.00228012]\n",
            " [-1.47693992  0.96681869]\n",
            " [ 0.66508579 -1.12104928]\n",
            " [ 1.93491507  1.42451274]\n",
            " [ 1.51953125 -0.7076292 ]\n",
            " [ 0.90078062 -1.02442229]\n",
            " [ 1.62561846  0.90965575]\n",
            " [-0.93482053  1.5570873 ]\n",
            " [-1.03689539  1.54727745]\n",
            " [ 0.79266524 -0.9733938 ]\n",
            " [-1.29604483  1.21720946]\n",
            " [ 1.01155114 -0.86382878]\n",
            " [ 0.82338572  1.16506863]\n",
            " [ 0.93734717 -0.74835223]\n",
            " [ 0.71087593 -1.08962321]\n",
            " [ 1.14685118 -0.98364067]\n",
            " [-1.67863059  1.07920694]\n",
            " [ 0.99688673  1.03439009]\n",
            " [ 1.03281772  0.76471078]\n",
            " [ 0.65256429 -0.92330098]\n",
            " [-1.32818043  1.05561566]\n",
            " [-1.32616162  1.45796776]\n",
            " [-1.3262639  -1.15866959]\n",
            " [ 1.06521535 -2.05526972]\n",
            " [ 1.09861064 -1.55371237]\n",
            " [-1.33687162  1.12943065]\n",
            " [ 1.45951998 -1.71130085]\n",
            " [ 0.74167478 -0.88140899]\n",
            " [ 1.32424092 -1.40639734]\n",
            " [-1.14180624 -0.99035543]\n",
            " [-1.44018972 -1.34851944]\n",
            " [-1.37990963  1.36153531]\n",
            " [ 0.85163587 -0.83497298]\n",
            " [ 0.97952294 -1.219015  ]\n",
            " [ 0.86692548 -1.6329757 ]\n",
            " [-1.87347639  0.95598638]\n",
            " [-1.25326991  1.39812899]\n",
            " [-1.37163794  1.26747489]\n",
            " [ 1.27997696  1.08330488]\n",
            " [ 1.20174181  0.62186229]\n",
            " [ 0.44974625 -1.06482995]\n",
            " [ 0.77067912 -0.91692388]\n",
            " [-1.04424047  1.3178854 ]\n",
            " [ 0.8695913  -1.50648046]\n",
            " [ 0.81578678 -1.39359689]\n",
            " [ 1.07619512 -1.04305041]\n",
            " [ 1.2209785   0.99417281]\n",
            " [-1.45048225  0.84227747]\n",
            " [ 1.14807177 -1.01752031]\n",
            " [ 1.59367204 -1.60439372]\n",
            " [ 0.73254883  1.08963478]\n",
            " [-1.14353788  0.80234653]\n",
            " [ 1.26994228  1.91203094]\n",
            " [-1.44644642  0.97320497]\n",
            " [-0.94784582  1.48498392]\n",
            " [ 0.99337804 -1.35674226]\n",
            " [-1.46378994  1.02048469]\n",
            " [ 1.09047866 -0.90871984]\n",
            " [ 0.78318167 -1.62555242]\n",
            " [ 0.52037024  1.51224017]\n",
            " [-0.59070224  1.47508168]\n",
            " [-1.23503458 -1.16877747]\n",
            " [ 0.98557079 -1.17266858]\n",
            " [ 1.2328558  -0.55816108]\n",
            " [ 0.9464258   1.01680231]\n",
            " [-0.91281289  1.03300655]\n",
            " [ 0.48003829  1.4934783 ]\n",
            " [ 0.71416914  0.87171125]\n",
            " [-1.43374693  1.52833509]\n",
            " [ 1.1119225  -0.96477348]\n",
            " [-0.97400635  0.64087307]\n",
            " [ 0.69700104 -1.02732861]\n",
            " [-1.72405815  0.69655097]\n",
            " [ 0.99762809  1.17106342]\n",
            " [ 1.42710459 -1.38760817]\n",
            " [-1.13080966 -1.52508616]\n",
            " [-1.55808091 -1.2697885 ]\n",
            " [-0.98243755  1.18640184]\n",
            " [-1.56411433  0.92958236]\n",
            " [ 0.64109814 -0.95771837]\n",
            " [ 0.97541046 -1.46808815]\n",
            " [-1.50290525 -1.14050579]\n",
            " [ 0.71842706 -1.18376565]\n",
            " [ 0.47674853 -1.39958978]\n",
            " [ 1.01083088 -1.07746077]\n",
            " [-1.20511591  1.16551566]\n",
            " [-1.32671809  1.95868635]\n",
            " [-1.018875    1.28065908]\n",
            " [-1.56552315 -0.44728529]\n",
            " [ 0.69808042  1.1290257 ]\n",
            " [-1.79490566  0.92379361]\n",
            " [-1.26464403  1.06327307]\n",
            " [-1.36223352 -1.00170338]\n",
            " [-1.40974355 -0.89346498]\n",
            " [ 0.90400004 -1.63573015]\n",
            " [ 1.07188737 -1.6561662 ]\n",
            " [ 1.12418723 -1.53035808]\n",
            " [-1.4151566   0.45347232]\n",
            " [ 1.15029514 -0.77900624]\n",
            " [-1.10026908  1.04280567]\n",
            " [-1.3097316  -0.8140294 ]\n",
            " [ 0.90238303 -0.67303348]\n",
            " [ 0.6934799   0.90162635]\n",
            " [ 0.93062383 -1.46393967]\n",
            " [-1.46531332 -1.20144486]\n",
            " [ 0.85643446  1.78719807]\n",
            " [-1.53369534  1.37063348]\n",
            " [-1.1284591   1.08094597]\n",
            " [-1.03803349  1.09905553]\n",
            " [ 0.86205757 -0.78188872]\n",
            " [-1.06623268 -1.13556111]\n",
            " [-0.6291005   1.29941845]\n",
            " [ 0.69428289 -1.64399552]\n",
            " [-1.52693415  1.23308384]\n",
            " [-1.2432884   1.38028002]\n",
            " [ 0.73137689  0.97523141]\n",
            " [-1.29628766  1.41204059]\n",
            " [ 0.97003669 -1.10935569]\n",
            " [ 0.85073113 -1.61873078]\n",
            " [ 0.62760019  1.41242373]\n",
            " [-1.05688703 -0.91553676]\n",
            " [ 0.70487958 -1.30334616]\n",
            " [ 0.9940694  -0.92619085]\n",
            " [-1.42988265  1.23838079]\n",
            " [ 1.22498858  1.48030186]\n",
            " [ 1.01233459 -1.12504435]\n",
            " [ 1.38444459  1.17864096]\n",
            " [ 1.06527925  1.05250192]\n",
            " [ 0.56123984 -0.90360439]\n",
            " [-0.7403062   1.22024488]\n",
            " [-1.20955253  1.25959706]\n",
            " [-1.60340524  1.47126102]\n",
            " [ 0.88296503 -1.31177998]\n",
            " [ 1.11471903 -1.74375725]\n",
            " [-1.00987518  1.67912316]\n",
            " [ 0.61862183 -1.12599385]\n",
            " [ 1.06837153 -0.75436234]\n",
            " [-0.95970154  1.30484319]\n",
            " [ 0.4691636  -1.28528452]\n",
            " [ 0.77543336 -1.93291259]\n",
            " [ 0.75973481 -1.24549365]\n",
            " [ 1.18383527 -1.22965848]\n",
            " [ 0.52538085 -1.73715115]\n",
            " [-1.64220595  0.64278549]\n",
            " [-1.57315719  1.10500324]\n",
            " [ 0.88346791 -1.04744077]\n",
            " [ 1.59138227 -0.84312236]\n",
            " [ 1.02478957 -0.89188838]\n",
            " [-1.66874003  0.99579269]\n",
            " [ 0.93432254 -0.97915661]\n",
            " [ 1.01079834 -1.36783051]\n",
            " [ 1.41908669 -1.05110991]\n",
            " [ 1.24862456 -0.75690728]\n",
            " [ 1.47120452 -1.07710409]\n",
            " [ 0.86835837 -1.61713135]\n",
            " [ 0.49500626  1.36679304]\n",
            " [-1.24887633  0.90227795]\n",
            " [ 1.17450082 -1.53190672]\n",
            " [-1.54766762 -0.66040039]\n",
            " [ 0.83179981 -0.85829103]\n",
            " [ 1.45678425 -1.70260239]\n",
            " [-1.20595372 -1.16226792]\n",
            " [-1.69545174  1.22710323]\n",
            " [-1.2428503   1.40474653]\n",
            " [-1.16804504 -0.80374336]\n",
            " [ 1.35287416 -1.03278112]\n",
            " [ 1.3109827  -0.80093324]\n",
            " [ 1.31946969 -1.07711339]\n",
            " [-1.1495893   1.07788384]\n",
            " [ 1.27285278 -1.04023206]\n",
            " [ 0.85622907  0.85599327]\n",
            " [-1.61926615  1.25334918]\n",
            " [ 0.54512334 -0.94465977]\n",
            " [-1.39193869  1.15904558]\n",
            " [-1.05540752  1.0978173 ]\n",
            " [ 0.64939368 -0.94797087]\n",
            " [-1.45551598  1.07548165]\n",
            " [-1.03839922 -1.64428711]\n",
            " [-1.32457471 -0.65425706]\n",
            " [ 1.12498283 -0.98378688]\n",
            " [-1.86661172  1.06060457]\n",
            " [ 1.26393116 -1.30854309]\n",
            " [ 1.24781084  1.09548533]\n",
            " [-1.3125993   1.28559721]\n",
            " [ 0.57875907  1.05067647]\n",
            " [-1.23005974  1.28414559]\n",
            " [-1.51697993  0.6885879 ]\n",
            " [-1.27362871  1.14260149]\n",
            " [ 1.41867626 -1.2215457 ]\n",
            " [ 1.0863781  -0.77736866]\n",
            " [-1.36742127  1.09790981]\n",
            " [-1.18454754 -0.72577834]\n",
            " [ 1.29001284 -0.53671396]\n",
            " [ 1.02539384 -1.67902803]\n",
            " [ 0.99221826 -1.30749953]\n",
            " [ 0.81683636 -1.04024589]\n",
            " [ 0.53464282 -1.31507349]\n",
            " [ 1.37009621 -0.85635579]\n",
            " [ 0.7547611   1.36027694]\n",
            " [ 1.06779826 -0.98290366]\n",
            " [ 0.75110584 -1.15939403]\n",
            " [ 1.20846808  1.91293216]\n",
            " [ 0.82711208 -0.90220362]\n",
            " [-1.01748157  1.40078676]\n",
            " [-1.10978866  0.98218143]\n",
            " [-1.98535001  0.93073487]\n",
            " [ 1.41611695 -1.04252148]\n",
            " [ 1.60238147 -0.96239823]\n",
            " [ 1.66351199  1.55539274]\n",
            " [-0.62917221  1.30114615]\n",
            " [ 1.5665729  -0.93473476]\n",
            " [-1.21909392  0.78311706]\n",
            " [-1.4546268  -1.55951822]\n",
            " [ 0.89498174 -1.23638391]\n",
            " [ 0.99340624  0.90441096]\n",
            " [ 0.6411345   0.76250207]\n",
            " [ 0.88581395  0.93737036]\n",
            " [ 1.1679399   1.18953598]\n",
            " [ 0.95936114  1.06160164]\n",
            " [-1.74821651 -1.15805769]\n",
            " [ 0.71738398  1.07729721]\n",
            " [-1.16050756  1.25016773]\n",
            " [-1.6304462   1.76929402]\n",
            " [ 0.44042504  1.15594327]\n",
            " [ 1.35049629  1.4186213 ]\n",
            " [ 1.40442407  1.240695  ]\n",
            " [ 0.76435316 -1.42490005]\n",
            " [ 1.41841459  1.01867759]\n",
            " [-1.33558035  0.90335238]\n",
            " [-2.0323143  -0.80078673]\n",
            " [-1.030568    0.55484551]\n",
            " [-1.16260576  1.4081676 ]\n",
            " [ 0.82948053 -0.94842428]\n",
            " [ 1.07977402 -1.20718884]\n",
            " [ 0.80918622 -1.01773989]\n",
            " [-1.54538929  1.285411  ]\n",
            " [ 0.68446869  1.0758673 ]\n",
            " [ 1.01744556  1.32841372]\n",
            " [-1.10880411 -1.06879652]\n",
            " [ 1.01371574 -0.61250854]\n",
            " [-1.30818844  0.7866261 ]\n",
            " [ 1.2055155   1.24375963]\n",
            " [-1.18247259  1.17068887]\n",
            " [-1.08541     1.58885896]\n",
            " [-1.84648156 -1.07639217]\n",
            " [ 1.09165704  1.43191886]\n",
            " [-1.66307926  0.97870827]\n",
            " [ 1.27854586 -1.12673628]\n",
            " [-1.19341242  0.87429416]\n",
            " [ 1.30030727  0.82139754]\n",
            " [ 1.21142852 -1.25140214]\n",
            " [-1.31714737  1.0271672 ]\n",
            " [ 1.16295254 -1.32040513]\n",
            " [ 1.45870245  1.67067218]\n",
            " [-1.4350456   1.07584345]\n",
            " [ 1.23451042  0.76990402]\n",
            " [-1.03101432  1.33858609]\n",
            " [ 1.19364166 -1.19240785]\n",
            " [-1.00253856  1.60604334]\n",
            " [ 1.49855638 -1.12613738]\n",
            " [ 0.92086035  1.46527147]\n",
            " [ 1.22760987 -0.90161681]\n",
            " [-1.10039425  1.31230676]\n",
            " [ 1.22916007 -1.08973944]\n",
            " [ 1.01796448  1.64782417]\n",
            " [-1.45592022  1.47740459]\n",
            " [-1.0072819  -1.70441711]\n",
            " [ 0.9520784   1.27328444]\n",
            " [ 1.28661168 -1.48161566]\n",
            " [-1.50825477  0.88028157]\n",
            " [-1.4881053   1.39757442]\n",
            " [-0.89854008  1.38999522]]\n",
            "(500, 2)\n",
            "Means of zA,zB\n",
            "[0.04889741 0.02995574]\n",
            "Covariance Matrix zAzB\n",
            "[[ 1.34016535 -0.58722704]\n",
            " [-0.58722704  1.41914445]]\n",
            "CovarianceAB after emperically estimating or learning by gradient descent\n",
            "[[-2.406624]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npx9A56U40hH"
      },
      "source": [
        "Why does Emperical A and B logvar differ from zAzB Covariance Matrix?\r\n",
        "\r\n",
        "\r\n",
        "Even after adjusting by 2*exp(logvar), still does not equal covariance matrix estimates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKa-T94O5mXH"
      },
      "source": [
        "# P(A,B) Multivariate Gaussian Ground Truth (1000 data points)\r\n",
        "## Mean: \r\n",
        "[0.6 0.5]\r\n",
        "\r\n",
        "##Covariance: \r\n",
        "\r\n",
        "[[0.24024024, 0.1001001 ]\r\n",
        "\r\n",
        "[0.1001001,  0.25025025]]\r\n",
        "\r\n",
        "# Hardcode Ground truth logvar,mu, Covariance\r\n",
        "Cannot hardcode since marginal encoders weights and biases generate sample specific mu and logvar that are widely different than ground truth emperical estimates. Therefore zA  and zB (not 0 or 1) differs from actual xA and xB (0 or 1).\r\n",
        "\r\n",
        "While mu and logvar of each sample (0 or 1) stays the same, the sampled z differs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxjVeK606_Ub"
      },
      "source": [
        "VAE_MRF.eval()\r\n",
        "hardcode = False\r\n",
        "if hardcode == True:\r\n",
        "  with torch.no_grad():\r\n",
        "      #VAE_MRF.covarianceAB = torch.nn.Parameter(torch.tensor(0.1001001))\r\n",
        "      #VAE_MRF.covarianceAB = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(VAE_MRF.covarianceAB,0),0))\r\n",
        "      VAE_MRF.covarianceAB = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.1001001),0),0))\r\n",
        "      VAE_MRF.muA_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.6),0),0))\r\n",
        "      VAE_MRF.muB_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.5),0),0))\r\n",
        "      VAE_MRF.logvarA_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.24024024),0),0))\r\n",
        "      VAE_MRF.logvarB_emp =torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.25025025),0),0))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpNuirlCG4b1"
      },
      "source": [
        "## Visualizing Multivariate Normal\r\n",
        "https://peterroelants.github.io/posts/multivariate-normal-primer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrqYmOIxeZvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94286e5-9c42-4c01-f0e6-b6e25c943291"
      },
      "source": [
        "x_test = np.eye(num)[np.arange(num)]  # Test data (one-hot encoded)\n",
        "duplicates = 10\n",
        "x_test = np.repeat(x_test, [duplicates,duplicates],axis=0)\n",
        "x_test = Variable(torch.from_numpy(x_test))\n",
        "x_test = x_test.to(device)\n",
        "\n",
        "print(\"Print prediction results for A only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE_MRF.forward_single_attribute(x=x.float(), attribute='A')[0].cpu().detach().numpy(),decimals=2)))\n",
        "\n",
        "print(\"Print prediction results for B only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE_MRF.forward_single_attribute(x=x.float(), attribute='B')[0].cpu().detach().numpy(),decimals=2)))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Print prediction results for A only:\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "Print prediction results for B only:\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRbLTCH8lrJe"
      },
      "source": [
        "# Visualize Latent Space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKiDijtuUHt"
      },
      "source": [
        "## Check encoder, decoders work on their own\r\n",
        "It appears the marginal VAEs converges to a local minimum with accurate reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0jQpqykvUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "d909e116-fad3-448a-d5de-74a8e924cf77"
      },
      "source": [
        "xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "xA = Variable(torch.from_numpy(xA))\n",
        "xB = Variable(torch.from_numpy(xB))\n",
        "\n",
        "np_zA = np.empty(0)\n",
        "for x in xA:\n",
        "  zA = VAE_MRF.latent(x.float(), attribute='A')\n",
        "  #print(z)\n",
        "  np_zA = np.concatenate((np_zA, zA.cpu().detach().numpy()))\n",
        "np_zA = np_zA.reshape(num_samples,latent_dims)\n",
        "\n",
        "if latent_dims==1:\n",
        "  plt.plot(np_zA, 'o', color='black');\n",
        "elif latent_dims ==2:\n",
        "  plt.plot(np_zA[:,0], np_zA[:,1],'o', color='black');\n",
        "elif latent_dims ==3:\n",
        "  from mpl_toolkits.mplot3d import Axes3D\n",
        "  fig = plt.figure()\n",
        "  ax = Axes3D(fig)\n",
        "  #t = np.arange(1000)\n",
        "  ax.scatter(np_zA[:,0], np_zA[:,1], np_zA[:,2])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2df4xtV3Xfv2vmvWd6n2mAa4cY7LljK1bCo2oJHhHcIETzSOS4EQ6SkUCj1KiupoyKlEqtIlsjoTbSqEGVShuB2owIKuodEdq0EZblyPUP8lfEj3HinyHGz3TmYcvBjkvcPx6lOG/3jznncd55Z++91v5xzr73rI90NHPvPXef/fO71l57n3PJGANFURRl+VkZOgOKoihKP6jgK4qijAQVfEVRlJGggq8oijISVPAVRVFGwomhM+DimmuuMevr60NnQ1EUZWF47LHH/soYc23XZ0UL/vr6Og4ODobOhqIoysJAREe2zzSkoyiKMhJU8BVFUUaCCr6iKMpIUMFXFEUZCSr4iqIoI0EFX1GUpWN/fx/r6+tYWVnB+vo69vf3h85SERS9LVNRFEXK/v4+tra2cOHCBQDA0dERtra2AACbm5tDZm1w1MNXFGWp2NnZuST2NRcuXMDOzs5AOSoHFXxFUZaK8+fPi94fEyr4idHYoaIMy9ramuj9MaGCn5A6dnh0dARjzKXYoYq+ovTH7u4uJpPJZe9NJhPs7u4OlKNyUMFPiMYOFWV4Njc3sbe3h9lsBiLCbDbD3t7e6BdsARX8pGjsUBkjJYYxNzc3cXh4iIsXL+Lw8FDFvkIFPyEaO1TGhoYxFwsV/ITkjB2W6EUpioYxFwsV/ITkih3m9qJCjIkaIAXQMCaXYsaLMabY45ZbbjGKMbPZzAC44pjNZtFpz+dzM5lMLkt3MpmY+Xye9DvKcMznczObzQwRmdlslrSdcvbNZaHv8QLgwFg0dXBRdx0q+McQUeegIqLotEMGrA7yxSG32JRs/HMaOgl9jxcVfCGldJSanB3GZkyOJ3+y76QwQMqPSdEP+xCb0sZLnadSDFHf40UFX0BJHaWPPNkEgYis6YeKSInCwKXvvKdq87Ea55JmoUvn4QP4AoCXATxt+fwDAF4D8Hh1fIqT7hCCX1JHaZJLcObzuVUUbGXuEqOTJ0+a6XRqzV+JhpTLEHlP1Q9TpLOIhrokQ7d0MXwA7wfwbo/g3y9NdwjBL6mj9IUtpOMqc1MEptOpOXXqlLNDl2pIOQyR91T9MFZsFtVQ991mPqPYp9HMLvjH18B6yYLPrfBFFqZQYsvM+X6IgJXiWfbhBLTLOp1Ok/XDmHpc1PHQp6EqzSiWIvivAngCwB8BeKcjnS0ABwAO1tbWklSApEFKa7w+iC0zRxClwhGapxxGIrfo2UJkvllTHyzyjDdVX/ClU5pRLEHw/zaAq6v/bwfwHCfNVB5+iNiU4Fly6KtTu87h1K/U6K6urooHUS5jndsJsNXfdDodvB+mFrNFGlvG8Nq+NKM4uOB3nHsI4BrfeakEv7QGSUUp01ZuPrhGpZ0Wt81yelo5hark/pmyjy3i7JnTp9TDv/KznwJA1f/vAXC+fu06hvLwF4U+y+W7VipBtF2HU7aShdNFKf3T1oa527bkccjpU6UZsuyCD+BLAF4C8CMALwC4G8AnAHyi+vyTAJ7BcQz/awD+PifdVIJfWoOkIkbgpLsKQjzulGXitNkiCooxZfTP1J58V99aRIPM7VMlhap68fBzHEPs0lkkQgXON7i7Ppfu1U9dptXVVdaC7dDCGcoQN3Y1r5dqV5CrDRbNIM/n8856kfaprrbN2d4q+B3knr72QajA+Qae7fO26OcQU5ex4bTH9vb2pQXf1dVVs729XdQe6RLwrZNIvG+J4Vgkg2yro+l0Khb7djorKyvRRsSFCn4L2za406dPZ22IHISIlWuAG+MOq/iulUI8m96gzcjYvCbp9sYYEVpUQ+EK0Um87xDDkcv4pm6LVLORVHUtQQW/haQRUjZEKdi2PK6urhpj8oWKpNjyMZ1OO69j8y5dZSmlrH3iMuiS8kjGUTMk55pdh9574QtRSo1BqvUGbl2HpG1DBb+FpBFSNkTN0J6hq6x1/iQDzxWfjTGY0nYKadPQgb1o8egmLkMq6ZfS9plMJmZ7ezt5jN/V90Jn7erhL5HgD+nhp9yzHgqnM3Ovz5nWhxpMaTtJD1fMuf7cVu6SdpxI+0qq2UlI+9hmlzH9J8QxqOvJVm+p6ogb9nI9nVaKCn4LSewxtJFtHYkrtjELl768pAxHcAZ91w4bjkjZ8tm16FXXESeGL/ncZoxD7gTmtI2UmDBIjmvnOOqFUumY4oztdl9oPvF1e3s7idPVzHvXgwaJyGxvbwel3cUoBV+yONTVCM2OJr2uawDGPHdGKs6uvKSaQYTEg7e3t9k7ftrt5IvT2wycq07rUIbt87YxPnnyZFS7+NpGgs+ByB0+nM+7ty7ajhAPf3V1VbzwniIfOe9az9kmoxP8kMGUqhF8A5Dj4XNElONJpnziorS8tuvO5/bn77v22nMGta9cPmPLMca2Ol1ZWRH1GU4/4ODKc+6F5eaY4bR/6jWZtiHm9kVJPqTt4XI4bKGj1OI/OsFPNZhC8IkGZxByO65vBuPq8KmQbs0Lnb2kmPXY0phOp87Pm/3GlQcJoWsAbYFwGfWc3r/Uq86xAN9VV65wW/2dOlQTeg1Jnbi2BecyyBiT4LuEzjcoU1hbjmj4vABbiMk1gCRGI7Xhk4hQ6OzFd28AN8TVFY5ZXV29lGdfvabqWyGzL6mg5PT+JTM7SRgn1vt2jf96BikJQ0nGinS2m8sxxVgE3+d11PvMud9NuSrvWg+wDWSbCHEec+AaODnikk1cMXrOoOjyqmzf4zxyoQlnoLsWyG3fr2cJXUiF2oZrhtLlqLgEJdb7l6zdSESwa9umpK5c7WszdMDx9s3Y3x+Q7rnPtdMLYxF8jpjYOnGK6W9T0LoaUxqusF3bJ5Suaa1LmFLQNaCauxBCY/Gu70kGJndQ2rys+Xx+hTCcOnUqqVBL824TCJcTE+v9S8rkG5ftLYnSOHjze742dY210Bm+ZP2geaTY6dXFaATfN5hd0/XYAcCNaUrCFbaB7Ou0KYQxFM401WUYfTd4uQZJM936PK5ht6WZYqEtlScXEgKw5TXG+6/T5c6IOWMjtG459dMcx6m9al/ZfNt+c4zR0Qi+r8FdnTh2AHCFRBKucHmZtsFmm9JKQx/1dVyDr+vzEC80hXjWdeAbRFzDLDFEPkKE2lZXqRb5Qr3/dhrctvMZ6xTlc/WNeoZpa4vV1dXkRqY9O7GdF3ptG6MRfFf83CfAsQMgJlRgu7brxg/btNd1bekA9T2fRPI8m9hpao1rwHLr3bdAzlkjkWALczVFQZJWqAfMTSuVgeq6nqtPcWcWtvLbvr+ysnLp/K41gvaRwsh0OTi5YvZtRiP4xnQ/HteYuM7kEpn6HM5iIOd5NK6O2b4TsB3b9ImeZFHKVV8ub+306dPZ9353pe+re9egkq6RuL7ra98ug9K+Ga5pwLraOZUn6KqPXG3oKkfslmbO7K3tSLnGDKeeJcbRdm7qdbXsgg/gCwBehv0nDgnA7wA4B+BJAO/mpJvCw28OJulimyvddgdybfeTLj4ZwwsRSeOD3I5pTFjopD5S3ZLuao92+r76kninHANf58MnQNx8nj592lrnITt6YunTwNT4xNPVxhIHqLmw7BsfnBs2JWsZXTrB1SEufQj++wG82yH4twP4o0r43wvg65x0U8Xw687QruyTJ0+yK9rl1XJ3xEg6R44bVboO6cIwZ0917NQ/BJdRtoVPbKLmM/CcEIRta2rKtsu94yqEGEPhGx++MdEeS6nGkC/GLilzH3e/o4AfMf9dAB9rvH4WwHW+NFPt0nFN1SUVLe1AzakoxztpdhrJ80lSi4Zt8Vd6ZyV3wKfyJrtCIq7wic879y0yuvqD7bOQZ8m4DmldhdS1JGwVGwoKidHbxjPnfOkRO7PqI46PAgT/fgDva7x+BMCG5dwtAAcADtbW1kQFdYl6aEVzpoi+rYIcoWxP2VOEa9rpdeWzPZ10LXxzQidd122Htdr1myte7OoPHAfA12dCZ0EpPX2JwxJS15IF5xROlTT/rvFsC6GkqPNQJyV3HRmzYILfPFLG8EMqmivWZ86cEV+XczRjjV27SbhGoe6QnOmkr566QhWSg3sDT4oB4BJsjgPgy5tLDH3t2jSCMbM5iWcYUte+/su9lyUVnNlyk1wzZc59Oc1r1w5TTgenBgUIfi8hHWNkcdn27gju7pyuDm1bqIwRR86+Z87uj5qY7aW2G9AAmKuvvlpUTokXLWnrdhu4DBzH+HEGp6v+OX3Gdp2QuvQRUtecMtV5sI2XHGsNXOHsay2sWQ++xVnXOE6xSI4CBP8f4vJF229w0kz9Ayi2ipbuv5cMvBgPv8sz9z2Tx9VpOB6e6xzf9yUGUpInW1l9Imnb3WLbVdW1WyLlrqqueuja0cN5eF6zn/qQesXN73HWHJprVX3sQmmXy9U2MeMvpD191+yq69ReP3rYpfMlAC8B+BGAFwDcDeATAD5RfU4APgfgeQBPgRHOMRkEv4sQgXM1eHtW0eVJcreDSYSJA9djDb0BjeupSr1oSbu1D8mzXZqeqNTjCvUkuwSAUzbOj4Jz2iTmcQhdZXA9XC6VByshNOwW05a+xfw2oU6PDYzpxispvsYP6Ry1WNsWP313+3EWSEM7Q9uT7FpQDQlxtaemrvoJ9aIl7eYaYK7v1nn3hcfa+Y7ZfRNSNpeRbS6Uh9xYxPXsu+qF2y657yNol6fZvzh320qPphMmHbep1z4wVsFPPeWbTqfm7NmzrE7NeW5I7La/2Lrh3MnbPN+3AMbZo86N5/q8Vq4gSTxoX7u2ZwApRKMZ1vHlr6tcMSGLWM++K7woHU99ef229Z7Y9rP1DcnMXD38BILPDRVIOnjXLfAhR925XSJgTFgskDuAOPl37Vn35SsmLulbYJe2Fyd9rmcqXdCX1Bm3T0puRpLWj6RcsePJl16qhUxfX0yxk6drg4Vk7W2hYvi5jhjBl1jNZseSDE7XeTFT/KZoShYXJZ1GkhepuHFiyyFt56rT+rlJrllBl4fnuinP1Ta+sJAkzNM1W+OE3lx1JS1PjdSAuMZTXbbQvp9KBH1aIAlD5bpTNpVxM2akgh8aF/MNoOb3XQM6NE7YnuLbPIV2B5F2RIkYSQetK94N2J9V72s7bru04YgHVzh9s69mfdsMdkqxiA0ttetNakBc9R5ijLj1KxHI0BvoukJOqb3xHGBsgs+Jj7u+y91pcvbs2c7Pzp49eymtkE7PKR93kNsGJDcvLg/f5+Vy8hsbVuC0K1ecOXVax2olIUNXyCCFWLRnA11bUbm/kyA1IK56DzHcPq/bdj+Iqx45xkOSXkpvPAcYk+BLxcWWBvdxx2fPnu18HHOTkJCBixTpcdLwPXdme3vb+l3OXntbHqWi42tX7myvOZBt34nZuhn6HSld15CMC9f6kqTepYabs+jvckBsfZ0j6KWLuIRRCb6tM4T+4pMv9MH5tZr5nH8Tl2txpybF1reuQeDbpdM1IDjeky+fvhh2n88s99Vx7O6oIZHMfF3jiCuKvrb39bfQ+0FseVkWQfcxKsFPPVAl01KXwLq8YUk6xqT7UewUgyDFjhrfrMb2qGFufkPiriFGYhGQzHZSxKpjfnIz1Mnoi1KNyKgEP7QzSDuXVLy4C3ic/Ja2cBRTd758d5WViDpDZyF5lF63bhvbonRpg7+NZHzkdghCKaH/l5AHG6MSfF9DcOObIfu+uzylGqnh8M1IFkFgfLOjXOGYJjH11Fx0d80wXGscpbWRNJSX6pq2eghtn6H7fymzjC5GJfjG2DuDbWD6tjQ2B35zgVbS4Cn2Ny8aKQZFTIgulRfmK4ft89AwVG4x4+zq6WMGU7KX7KPkNZ7RCb6NlF62tLO6RKHv3yvtixQDOsZopPLCfINbYsw5ocU+RZBTR7nyVLKX7KPkvKvgV6T2siVejy9sVNrUPxWxZYsRm1ReWKiHz7l2u376+M3TJpw6spUvZOeb9Nqljo2SZycq+BW2jjudTntpvFI7b+mE1lsqL4yzLsR9No/Pc+YailRw6sjlKMWME9+1SxZVY8odzyr4FWP1svuAU3991zFHMLh58p3X/rzr0Rqhj3PI6eFz6siXz9C8+a7tctCWYazmGg+jFvyugbgMnaUkuMI6hLfm2yGSM0++AZ3iBjrutWK+65uJxMw+XNdOWT+lkbPvZRd8ALfh+HdqzwG4p+PzjwN4BcDj1fFPOOmmeB5+yVPCZYETFkgVXuk730Ncn+vBcreNpmA+D38+VSiSGVCJXr/LmOXse1kFH8Aqjn+68CYApwA8AeBM65yPA/isNO1YwR96QPfF0OEozuKby1sbanAOtbUuhVBz4v+p+3nfDpRkjaMvr18SAnTVVc6+l1vwbwXwYOP1vQDubZ0ziOCXvFc2FSXMYmI8fFt++zBiQzgErgVeSTk53m+Oft6Xc9E0is3HaUt+rGRIg+frW4vs4d8J4PON17/eFvdK8F8C8CSAPwBwgyO9LQAHAA7W1taiCr6oHr5kUJVQxtAYvi2/fRkxW544D7ALJVV7ceLbpfdzG77NFUPtbHIZWe76Q52nhY3hMwV/CuCq6v9/CuBRTtpjjOFL81zKLIa7S4czOPs0YvN596Owc/WT3PcG9NHPc3v5vvbPfe+CrXw+I8t9vLPvOrHkFnxvSKd1/iqA1zhp59ilU7LYGyMXuxI8fAmc/PZtxKQCE9OHUrXXfG5/THDsDVG+6+Z2oqTtnzJPrrQ4YbRmnxnK2cwt+CcAfAfAjfjxou07W+dc1/j/wwC+xkk79T78RWDIzt4HMfu+cxkxV52nrt+U6cU+NjqEPtom5BqpjLLr2pxwUgl3CWcV/OP0cTuAb+N4t85O9d5vAfhQ9f+/AfBMZQy+CuBnOemOUfCH7Ox9EbLve6jnyeQQuJTt1Xfb9zH7GtKJ4cTefbH8ocku+LmOMQr+onnsuehTyFx1XsoaSSn0Nfsayonhli/HzC9VeVXwF4xF89j7IHed2NJftDWS3Cy7QyIpX6o+mbpOVfCVhWZIkVl2gQth2R2SvsuX2qlwCT4df14mGxsb5uDgYOhsKAOzvr6Oo6OjK96fzWY4PDzMfv39/X3s7Ozg/PnzWFtbw+7uLjY3N7NfVxkHKysr6NJhIsLFixfF6RHRY8aYjc7PVPCV0kk9IBSlJFI7NC7BXxGnpig9s7a2JnpfURaJ3d1dTCaTy96bTCbY3d1Nfi0VfKV4+hwQitI3m5ub2Nvbw2w2AxFhNpthb28vS9hQQzrKQqBxdEXhoTF8RVGUkaAxfEVRFEUFX1EUZSyo4CuKoowEFXxFUZSRoIKvKIoyElTwFUVRRoIKvqIoykhQwVcURRkJSQSfiG4jomeJ6BwR3dPx+VVE9OXq868T0XqK6yqKoih8ogWfiFYBfA7ArwA4A+BjRHSmddrdAL5vjPlpAJ8B8OnY6yqKoigyUnj47wFwzhjzHWPM/wPw+wDuaJ1zB4AvVv//AYCzREQJrq0oiqIwSSH4bwfw3cbrF6r3Os8xxrwO4DUA067EiGiLiA6I6OCVV15JkD1FURQFKHDR1hizZ4zZMMZsXHvttUNnR1EUZWlIIfgvArih8fr66r3Oc4joBICfAPBqgmsriqIoTFII/jcB3ExENxLRKQAfBXBf65z7ANxV/X8ngEdNyc9lVhRFWUJOxCZgjHmdiD4J4EEAqwC+YIx5hoh+C8e/nn4fgN8D8F+I6ByA/41jo6AoiqL0SLTgA4Ax5gEAD7Te+1Tj//8L4CMprqUoiqKEUdyiraIoipIHFXxFUZSRoIKvKIoyElTwFUVRRoIKvqIoykhQwVcURRkJKviKoigjQQVfURRlJKjgK4qijAQVfEVRlJGggq8oijISVPAVRVFGggq+oijKSFDBVxRFGQkq+IqiKCNBBV9RFGUkRAk+Eb2FiB4ioueqv2+2nPc3RPR4dbR//lBRFEXpgVgP/x4AjxhjbgbwSPW6ix8YY95VHR+KvKaiKIoSQKzg3wHgi9X/XwTwa5HpKYqiKJmIFfy3GmNeqv7/SwBvtZz3BiI6IKKvEZHTKBDRVnXuwSuvvBKZPUVRFKXG+yPmRPQwgJ/q+Gin+cIYY4jIWJKZGWNeJKKbADxKRE8ZY57vOtEYswdgDwA2NjZs6SmKoihCvIJvjPmg7TMi+h4RXWeMeYmIrgPwsiWNF6u/3yGiPwbwcwA6BV9RFEXJQ2xI5z4Ad1X/3wXgK+0TiOjNRHRV9f81AH4BwJ9HXlcZIfv7+1hfX8fKygrW19exv78/dJYUZaGIFfzfBvBLRPQcgA9Wr0FEG0T0+eqcdwA4IKInAHwVwG8bY1TwFRH7+/vY2trC0dERjDE4OjrC1tYW9vf31RAoChMyptww+cbGhjk4OBg6G0oBrK+v4+jo6Ir3p9MpfvCDH+DChQuX3ptMJtjb28Pm5mafWVSUIiCix4wxG12f6Z22ykJw/vz5zvdfffXVy8QeAC5cuICdnZ3O85U06KxqMVHBVxaCtbU10fk2A6HE4wqv5bqey7io8RFgjCn2uOWWW8yyMp/PzWw2M0RkZrOZmc/nQ2epdyR1MJ/PzWQyMQAuHZPJxEyn08veq4/ZbNZfQUbGbDbrrc5t7V73Fd/nYwTAgbFo6uCi7jqWVfC1k4bVQZeB0Lrsn+p+mysOIkp+LZ9x6dP4LAoq+IUxxk7aFuuUnrnOlvqlz/7rMy59Gp9FQQW/MMbWSbu8cNuxrHWwTPQ5q1IPX45L8HXRdgBsC5DShclFYWdn54qdNDaWtQ6Wic3NTezt7WE2m4GIMJvNsm2D3d3dxWQyuey9yWSC3d1d1udKC5slKOFYVg9/bHFn24ymfSxzHShuXGE5X8hOQ3qXAw3plMeYOqlt2j2dTkdTB4qdZXGAShnTKvjKoCzLgFbysAxx+FR9PIXRUMEfIdKOk9s7KcX7UcpjGTYxpDBaqYyGCv7IkHYc9cCVmiEMM1csS3YaUhitVDMdFfyRIe04nPNLHmylsah1NZTh51y3dKckhVinmumo4I8MacfxnV/6YCuJvuoqh1EZMpbuK0/pcf4U7a4evgr+ZXAHeWoPv+TBVpo33Udd5TIqUkehz7p3be0tpf1j60Nj+Cr4l5B0htQx/FIX1UqcefRRV7mMiiTdvuvelrd2fYc8jynmvNQUvUsHwEcAPAPgIoANx3m3AXgWwDkA93DTH4Pg5/LaU+7SKdXDt+VrdXV1MNHvo65sni6AqHS3t7fZAtp3n+gyMDbj6hoTHCMV4jCVMsswJq/gvwPAzwD4Y5vgA1jF8Q+W3wTgFIAnAJzhpL/sgi/pWJw4e65OV6InbYx7mj9U/vqoq9XVVauhS5lvIjLb29ud56eeyXD6b/scW9vb8sA1UiXPdDhkE/xLibgF/1YADzZe3wvgXk66Qwt+7lu6JR3LdW4fna40L8YYe53k9jZ95K6rGA/fljdfX2x/L/XTTkMelW0zfLY8cI2UxJiVOPsdWvDvBPD5xutfB/BZR1pbAA4AHKytreWtGQe+TphCZDkdqx5o9ftd18vd6UoUe2P8T+EcYo0hV10105UKXTMNW5919cWu7508edKcOnUqiZMREq50tbttVpLDwy9xfStK8AE8DODpjuOOxjnJBL95DOnh+xo9hchyvCpb3LIpJjk7XYlT1iYhnl79vdTCnKuufALXvk6IFx/yWapnIUn7b+jMzlaP0+n0srrrmr3U9cud6Uyn06C6SEGU4HMOj+APHtIJGdx9/PCCTyByeCRS+pqyxghw6l1JoeSqK1u6taGr/85mM7O9vZ3Mi+d8LwXSevM9fdWVL5egd9Vd0yDYZjpdDsepU6cGc4qGFvwTAL4D4Eb8eNH2nZx0Uwh+6Iq7rTNJPfyYdQDuQMvphfcxZU2Rf4nByCXMuerKtzjtEr9m2TgzSunMIAXS9vd5+HXeYr4vGfMrKytZ60dKNsEH8GEALwD4IYDvofLkAbwNwAON824H8G0c79bZ4aafQvAlndU3dZbG8GOFTJr39mBNEbbwDY7mdDiUvmYRNbmEeSgPn3u4vH8XfW0KaHrern7FCXG58sj9fYZ2vwj9Xt9kE/zcRwrBT7HiXp9fLwY1ZwHN6XS7c8UKQMxASzVIOYMrdvra98JXLmHuM4bP9ezb7bS9vR3kBOReuI+ZibsMX1ebpvbwJdd2lSVV3Y5a8CWD22fB68ZItXeeQ2hniJ0ddH3O7dzSPPft4ef0WPvYpVOnKxWgnHUaS2wfkIw17gyh3S9Cv+ciR18cteDbFlqm0yk7VtnsPJKOGdOJY4XDZbxCw06uNGMetDbETqDcHmsfSAQoxNnok1jnSDrWXLu7VldXnc5PyPea32/2u5T3M9SMWvCNubySp9PpFfuH687W9Vm7EWI9iRQxU45YuYxXyE4gX5q+aa+vAy+DAEuQxKx96Ug8/T63C/a5kN6noyG9f6Y2ED59aR+hY2D0gt/ENzhOnjxpTp8+bRVJX8dsd/KQmKnrGlxj0DRkrvxKDdjJkyevOLcZwx/ilvuU3+sjXU49SuEuKEqvEVP/fW+VDclryHc4GhCyxtI1Zmw3kblQwW/AGRh1w3d1BFfHTBWacImm1Bi4OpMxRjyl9HmmsZ5a+1qcnVDtdvK1kWuA29KzGdGQ9uXMlHz10s6jxMvP6TX7yui6tqttSpoB+upF0haccSotqwp+A05j+LxRmyiE3vLOzWN9TakxsB3T6dR600jMbg7umklMPdiuNZlMnHc/Sh+X0fUIgdj25ayFSOrXddNQyDW49R9SxpCZ3hBrPJw82caHdPum75D2LxX8BhwvWFrBvjSlndzVwUOMgfQ4ffp01ADzhZW4aflEI5UnVbd3aHrSXVfSbYRNfDO8pgjFLghKbvzjzjhCZnop00pN7Gwrdf8yRgX/ClyNknqKHtoxbR6E65nltnzUzzxJJYzc/LuMICct30BPZeBCb6yR1ouvTjjx9ZtFMqAAAA6zSURBVD42DXDr33WN0Ju8JGUGIE4rFEno0DXbas52uYu46uEL4TaWZJGkmaarsVJOPX159g3wWIGUhLp8d4FyvJbQWKktdHP11Vc7B1SIUUwhoHWec8TGYxZdXQ8R4+QnVdzddo2Q+HYI0tBh29N33ZzZrCPXZhEJoxZ8aWNxfi2Juzga+stLtoHiEoymIesybinDH11lku5MqAXKld/aC7LF/yWLs9vb296dMdwYfm04OSLGcQykIaE+HnPQ1ZZdRqmPmwvnc/sD3KSzzhADJB07Mfc6pDCSoxb8HF4bJ83QQega0CGzCZ8QN6eYMfUjqWfXribpc9a5A8Q1G/ClxzVAnLaMFS1JmUORzCKkM46usnAMWKy4xhhK6cxY2p6pGbXg54jL+ry1HNPX2cz+tENXvjmzAs65vutw67npJcZcT0qKNpOKRk7HICelrRNIzou9juS7vl1fQzFqwZc0lq9z+9JMYdldg20+n3sFpJ3v2MHLvY6tTly3m0uMcX29nNPy0Jmdrd1zOgYpac9gpI/7jZlxSHYCxYirq919SEKHsTPRFIxW8H0LT/N5+K8l5bDsnPzY1h5s+baJ1MrKijUW3+yYvoXOmDqRevgx9S5Zd5EaKJtzkNMxSAW3XnJ5rpI6ihHN2B9+b2uJ5FEYfay7NBml4HMXnkIbw9X5Qjqma+D5bg5y5Xs+776NH/BvA7R91/Y9abmlYhMroJwFVFd9Sq/f90B3YWsbnwNRi2KuPPdVR67y5c5n34Z/lILfl+fQlVZIx3CFRLpEvD7fteWrxjWoXZ3OFQ5LhS9M1SxXyrs3ubOLZv2HtG2fU3lXHmz7xTl1wN1CG1rOVHXkSidWdGO+n7Lfcsgm+AA+AuAZABdh+YnD6rxDAE8BeNyVmfYRI/h9V3KNS0hcnTlnfkO3BPZVh7Y6a++zTukpSdYr2jOsPgU8xfVczgSn/L76LWEm48tDbB5jxsLSePgA3gHgZ+D4TVvzY8G/Rpp+Xx5+SkJvxMqZX5cRck3X+6rD+Zy3z9rlqYaIYlNMfeLnKnMuI5BKSGNuuIuZneYea9I8SNqpfW7MYyqWLoZfouAP5XVwQgW2sFKu/Lri+K7ruB7jkBpb3nzPbUl1C7/P47d5cjnbLZWQhnr43IXJoWbTufLQ1abS+0O60uxrZliC4P8vAH8K4DEAW560tgAcADhYW1uLKvgQ8VNOqMAlHrnya9uxZBORrnJIHj0hxSZKvvUC1/ekdRmyayund5tKxFwzI+4dtV1p+mZHnFBQqv6esh1S9qkhiBJ8AA8DeLrjuKNxjk/w3179/UkATwB4v++6JtLDH5Iuzzi1GITiExHXbpiceZ/P5537v307iUJ33LjywfHYffWU4tECKX/+rmtmVOefs/DfTsvn1HAWsyX1zNnnnmq3XQkzlhiiBJ9z+AS/de6/AvAvOecuquC7hGDoO/FcnlDM7CQW124dl8BxQmhSD80nMpx64oqyy/uO+VUsVxliw1CuEBHX++V45NJ8SmcMtvRj4/Wueu9jhjCo4AM4DeCNjf//BMBtnHRzPQ8/xLORNJbL6xx6GugaRBzxzOXhh3rLkt02qYwvp57Onj0blZYtTLKyshLt7caGP1J4wJw0cobLXOmHPjLBVe99rilmE3wAHwbwAoAfAvgegAer998G4IHq/5twHMZ5AsdbOHe46acWfJc4+DyHFM9QGTKU08RmvEJ3GKUgJgTG3VGRqk04YaT2ltKYtCTtwVmHiBXsFP2bk0bu0IrvMSZSb9xVpj41IZvg5z5SC77PM7NVvrSxStiXHIKrfnJOQV3X5gpnE4nXn/OmLc5glnr4rmtwQ00x+bVdR9q/OWkM5eGHpu8yIH2uC6jgV/i8KVvlhzTWEDuEYhnSUKXeFZQzRMU1KNw7VCU7aFzX8JXX1f9DBDu2f4eslaTsj6nTVw+/MMHvy8NfZIY0VKmv7QpvxA5ujkGReMxd5ZZuffTdUe3KZ4kOSXsrseSBZZJrpOpzSx/Dz32UEsPv8wYkJQ0+LzyleMT0j5TbDkPCQyVuNWwL/SKNuVHs0sl1lLBLJ1WogdPYixgGKhWX+OWo12bbcX8VK/W2w5AdWKXNUn2Gesj8Lsr4VMGPIMVA4QzsRV3oLZWhbp6RtGMOEXaFhxahf3HWIYZgUerPGBV8KxyLnUI4OAPbdk7KRxGPiaE8Wsl1+zZKi+Ch+jZW1PnuuxyLMkMyRgW/E67FTtHQnIFd8s1aucg5cIfyyCQi7vJmSxXk3LjqxLZ7qbR2HRoV/A64Qp5COGI8/FK9iFj6EOTSPUFfvLoPISvN67fVSb3Ivggzt6FRwe9AYrFjBwU3hm8b+CV6EbEs0gCSELoQa2v7nCG9UuPSrvG2CGszQ6OC30HfgsMxGimfjlg6izRFlhLiIAwR0utrDKScRQzpKJQ2G7Khgt9BiRa7xDzlYlk9/FCGCOn1YXRT9+kxjZFQVPAtlGixS8xTDnTgXs4QIb0+jG6Oa5Q+RiQ30+Uohwr+klP6ALCxqPnORd8hvT6M7jKH7rrg1mnOulfBX2LUU14ehmjL3EZ3bKE7bnlz1osK/hIztgG17CzbrGdsDgl3RpNz5uMS/BUoC8358+dF7ytls7m5icPDQ1y8eBGHh4fY3NwcOktRbG5uYm9vD7PZDESE2WyGvb29hS+XjbW1Ndb73PNSEyX4RPRviegviOhJIvpDInqT5bzbiOhZIjpHRPfEXFO5nKE6jqJwWTYj5mJ3dxeTyeSy9yaTCXZ3d4POS47N9eccAH4ZwInq/08D+HTHOasAnsfxTx2ewvFPHZ7hpK8hHT9jmzIrSumMYpcOjn/fdr/j/VtR/dZt9fpeAPdy0lTB57Fscd9S0XpWFgGX4J9IOFn4xwC+3PH+2wF8t/H6BQA/b0uEiLYAbAEaluCyubm51NPkEtjf38fW1hYuXLgAADg6OsLW1hYAaN0rC4M3hk9EDxPR0x3HHY1zdgC8DmA/NkPGmD1jzIYxZuPaa6+NTU5RkrCzs3NJ7GsuXLiAnZ2dgXKkKHK8gm+M+aAx5u90HF8BACL6OIBfBbBZTSfavAjghsbr66v3FGVhKGk31P7+PtbX17GysoL19XXs70f7WcpIiN2lcxuA3wTwIWPMBctp3wRwMxHdSESnAHwUwH0x11WUvillN1QdWjo6OoIx5lJoSUVf4RC7D/+zAN4I4CEiepyI/hMAENHbiOgBADDGvA7gkwAeBPAtAP/VGPNM5HUVpVcG20bXYlFCSzoLKRTbam4Jh+7SUUqihF06i/BsGu7vPwxdl8sKHLt0yHSG3ctgY2PDHBwcDJ0NRSmG9fV1HB0dXfH+bDbD4eFh/xnqwJfH9o4n4Hi2tMx34PYJET1mjNno+kwfraAoC0QpoSUXvgXuRQlLLSMq+IqyQCzCs2l8C9wl7XgaGyr4irJglP5sGt8spJQdT2NEBV9RlKT4ZiGLEJZaVnTRVlGU3tnf38fOzg7Onz+PtbU17O7uFjdTWVRci7Yq+IqiKEuE7tJRFEVRVPAVRVHGggq+oijKSFDBVxRFGQkq+IqiKCOh6F06RPQKgCsfysHjGgB/lTA7i4CWeRxomcdBaJlnxpjOX48qWvBjIKID29akZUXLPA60zOMgR5k1pKMoijISVPAVRVFGwjIL/t7QGRgALfM40DKPg+RlXtoYvqIoinI5y+zhK4qiKA1U8BVFUUbC0gk+Ed1GRM8S0Tkiumfo/KSCiL5ARC8T0dON995CRA8R0XPV3zdX7xMR/U5VB08S0buHy3k4RHQDEX2ViP6ciJ4hot+o3l/achPRG4joG0T0RFXmf129fyMRfb0q25eJ6FT1/lXV63PV5+tD5j8GIloloj8jovur10tdZiI6JKKniOhxIjqo3svat5dK8IloFcDnAPwKgDMAPkZEZ4bNVTL+M4DbWu/dA+ARY8zNAB6pXgPH5b+5OrYA/Mee8pia1wH8C2PMGQDvBfDPqvZc5nL/EMAvGmP+HoB3AbiNiN4L4NMAPmOM+WkA3wdwd3X+3QC+X73/meq8ReU3AHyr8XoMZf4Hxph3Nfbb5+3bxpilOQDcCuDBxut7Adw7dL4Slm8dwNON188CuK76/zoAz1b//y6Aj3Wdt8gHgK8A+KWxlBvABMCfAvh5HN9xeaJ6/1I/B/AggFur/09U59HQeQ8o6/WVwP0igPsB0AjKfAjgmtZ7Wfv2Unn4AN4O4LuN1y9U7y0rbzXGvFT9/5cA3lr9v3T1UE3bfw7A17Hk5a5CG48DeBnAQwCeB/DXxpjXq1Oa5bpU5urz1wBM+81xEv49gN8EcLF6PcXyl9kA+J9E9BgRbVXvZe3bJ0JzqpSFMcYQ0VLusSWiqwH8dwD/3Bjzf4jo0mfLWG5jzN8AeBcRvQnAHwL42YGzlBUi+lUALxtjHiOiDwydnx55nzHmRSL6SQAPEdFfND/M0beXzcN/EcANjdfXV+8tK98jousAoPr7cvX+0tQDEZ3EsdjvG2P+R/X20pcbAIwxfw3gqzgOZ7yJiGoHrVmuS2WuPv8JAK/2nNVYfgHAh4joEMDv4zis8x+w3GWGMebF6u/LODbs70Hmvr1sgv9NADdXq/unAHwUwH0D5ykn9wG4q/r/LhzHuOv3/1G1sv9eAK81pokLAx278r8H4FvGmH/X+Ghpy01E11aePYjob+F4zeJbOBb+O6vT2mWu6+JOAI+aKsi7KBhj7jXGXG+MWcfxmH3UGLOJJS4zEZ0mojfW/wP4ZQBPI3ffHnrhIsNCyO0Avo3juOfO0PlJWK4vAXgJwI9wHL+7G8dxy0cAPAfgYQBvqc4lHO9Weh7AUwA2hs5/YJnfh+M455MAHq+O25e53AD+LoA/q8r8NIBPVe/fBOAbAM4B+G8Arqref0P1+lz1+U1DlyGy/B8AcP+yl7kq2xPV8UytVbn7tj5aQVEUZSQsW0hHURRFsaCCryiKMhJU8BVFUUaCCr6iKMpIUMFXFEUZCSr4iqIoI0EFX1EUZST8f161UseBRv3AAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vBTljFA9YMa"
      },
      "source": [
        "# Query P(B|A=0).\n",
        "##Ground truth: \n",
        "\n",
        "P(B=0|A=0) = 0.75\n",
        "\n",
        "P(B=1|A=0) = 0.25\n",
        "\n",
        "\n",
        "Feed nothing into B encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuZdoZXu9biT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed7bcfa-2ee8-4fd6-c7fb-8589df487277"
      },
      "source": [
        "xA_evidence = x_test[0] #Evidence is A=0\n",
        "xA_evidence = xA_evidence.repeat(10000,1)\n",
        "print('A evidence input, first 5 rows')\n",
        "print(xA_evidence[0:5]) #need to resize/ view for single sample, or make evidence a batch repeated\n",
        "\n",
        "xB_query = VAE_MRF.query_single_attribute(x_evidence=xA_evidence.float(), evidence_attribute = 'A')\n",
        "print('B query output:')\n",
        "print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\n",
        "print(xB_query.size())\n",
        "\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0))\n",
        "\n",
        "#Taking max of each row in xB_query and counting times each element is max\n",
        "print('xB_query count of when each column is max:')\n",
        "_,indices_max =xB_query.max(dim=1) \n",
        "#print(indices_max.numpy())\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A evidence input, first 5 rows\n",
            "tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 0.]], dtype=torch.float64)\n",
            "zA\n",
            "tensor([[-1.4849],\n",
            "        [-1.3566],\n",
            "        [-1.3780],\n",
            "        ...,\n",
            "        [-1.5776],\n",
            "        [-1.2095],\n",
            "        [-2.0132]])\n",
            "zB\n",
            "tensor([[  3.5339],\n",
            "        [ 26.7430],\n",
            "        [ 18.3762],\n",
            "        ...,\n",
            "        [ 39.6758],\n",
            "        [ 25.4887],\n",
            "        [-15.9103]], grad_fn=<SqueezeBackward1>)\n",
            "B query output:\n",
            "[[1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]]\n",
            "torch.Size([10000, 2])\n",
            "xB_query mean of each column:\n",
            "tensor([0.7181, 0.2819], grad_fn=<MeanBackward1>)\n",
            "xB_query count of when each column is max:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 7187, 1: 2813}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQ15yUZnHsc"
      },
      "source": [
        "# Query P(B|A=1), \n",
        "## Groundtruth: \n",
        "\n",
        "P(B=0|A=1) = 1/3\n",
        "\n",
        "P(B=1|A=1) = 2/3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFsGFCqQmIZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a3015a-9904-497b-e6c4-14215f673e60"
      },
      "source": [
        "xA_evidence = x_test[1+duplicates] #Evidence is A=1\n",
        "xA_evidence = xA_evidence.repeat(10000,1)\n",
        "print('A evidence input, first 5 rows')\n",
        "print(xA_evidence[0:5]) #need to resize/ view for single sample, or make evidence a batch repeated\n",
        "\n",
        "print('B query output:')\n",
        "\n",
        "xB_query = VAE_MRF.query_single_attribute(x_evidence=xA_evidence.float(), evidence_attribute = 'A')\n",
        "#\n",
        "print(\"xB reconstruction\")\n",
        "print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\n",
        "#print(xB_query.size())\n",
        "\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0))\n",
        "\n",
        "#Taking max of each row in xB_query and counting times each element is max\n",
        "print('xB_query count of when each column is max:')\n",
        "_,indices_max =xB_query.max(dim=1) \n",
        "#print(indices_max.numpy())\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A evidence input, first 5 rows\n",
            "tensor([[0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [0., 1.]], dtype=torch.float64)\n",
            "B query output:\n",
            "zA\n",
            "tensor([[1.1546],\n",
            "        [0.9756],\n",
            "        [0.6979],\n",
            "        ...,\n",
            "        [0.7830],\n",
            "        [1.2027],\n",
            "        [1.0915]])\n",
            "zB\n",
            "tensor([[  9.0486],\n",
            "        [-13.8331],\n",
            "        [ 10.7607],\n",
            "        ...,\n",
            "        [  2.9541],\n",
            "        [-36.4415],\n",
            "        [  3.5269]], grad_fn=<SqueezeBackward1>)\n",
            "xB reconstruction\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]]\n",
            "xB_query mean of each column:\n",
            "tensor([0.3490, 0.6510], grad_fn=<MeanBackward1>)\n",
            "xB_query count of when each column is max:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 3490, 1: 6510}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8gVDBBLmw7u"
      },
      "source": [
        "Notice that the VAE_MRF can answer the query, but not as accurately as ppandas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5whHhIl14l5"
      },
      "source": [
        "# Query P(B|A=1,B=1)\r\n",
        "Feed both A and B, correctly identifies correct B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPPraAN111xK",
        "outputId": "41162f53-0894-4dac-842e-6324aed8a957"
      },
      "source": [
        "xA_evidence = x_test[1] #Evidence is A=1\r\n",
        "xA_evidence = xA_evidence.repeat(1000,1)\r\n",
        "\r\n",
        "xB_evidence = x_test[1] #Evidence is A=1\r\n",
        "xB_evidence = xB_evidence.repeat(10000,1)\r\n",
        "\r\n",
        "xB_query,_,_ = VAE_MRF.forward(xA_evidence.float(),xB_evidence.float(), 'A')\r\n",
        "print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\r\n",
        "print(xB_query.size())\r\n",
        "\r\n",
        "#Averaging all xB_query\r\n",
        "print('xB_query mean of each column:')\r\n",
        "print(torch.mean(xB_query,0))\r\n",
        "\r\n",
        "#Taking max of each row in xB_query and counting times each element is max\r\n",
        "print('xB_query count of when each column is max:')\r\n",
        "_,indices_max =xB_query.max(dim=1) \r\n",
        "#print(indices_max.numpy())\r\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\r\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.3 0.7]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]\n",
            " [0.  1. ]]\n",
            "torch.Size([10000, 2])\n",
            "xB_query mean of each column:\n",
            "tensor([0.6867, 0.3133], grad_fn=<MeanBackward1>)\n",
            "xB_query count of when each column is max:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 6866, 1: 3134}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oMp0BWBo3po"
      },
      "source": [
        "#Query P(A|B= -1)\n",
        "Try feeding into B encoder negative ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN5B_9zMpBib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45097fc4-6e4a-4169-9acd-97e5c2038e89"
      },
      "source": [
        "xA_evidence = x_test[0] #Evidence is A=0\n",
        "xA_evidence = xA_evidence.repeat(1000,1)\n",
        "xB = torch.tensor([0,0])\n",
        "#xB = torch.tensor([0,0,0,0,0,0,0,0])\n",
        "#xB = torch.tensor([0,0,0,0,0,0,0,1]) # if feed in valid input, get correct result\n",
        "xB = xB.repeat(1000,1)\n",
        "\n",
        "xB_query,_,_ = VAE_MRF.forward(xA_evidence.float(),xB.float(), attribute='B')\n",
        "print(xB_query.size())\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 2])\n",
            "xB_query mean of each column:\n",
            "tensor([0.6938, 0.3062], grad_fn=<MeanBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZJqFYQKqEWZ"
      },
      "source": [
        "- No matter xA evidence, if B encoder always given -1's B decoder same xB\n",
        "- No matter xA evidence, if B encoder always given 0's B decoder returns same xB\n",
        "- If feed in valid xB as evidence, then get correct xB as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niyylLQg52C9"
      },
      "source": [
        "# Querying the VAE-MRF\r\n",
        "Once  the VAE-MRF is trained, to query P(B|A=0=(1,0,0,0,0,0,0,0))\r\n",
        "- Feed $x_A$ into the A encoder to obtain $\\mu_A, \\Sigma_A$\r\n",
        "- Sample $z_A$ using $\\mu_A, \\Sigma_A$ (standard VAE reparameterization trick)\r\n",
        "- Since no input $x_B$ to the B encoder, assume $\\mu_B, \\Sigma_B$ come from the prior P(z) = Normal (0, Identity)\r\n",
        "- Using $z_A, \\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$\r\n",
        "- Feed $z_B$ into the B decoder to obtain $\\hat{x}_B$ \\\\\r\n",
        "\r\n",
        "Repeat, feeding in evidence $x_A$ multiple times to the VAE-MRF to obtain a probability distribution $P(\\hat{x}_B|x_A)$\r\n",
        "\r\n",
        "# Extension to Two Datasets AB and BC (not yet implemented)\r\n",
        "$P(z_A,z_B,z_C) = Normal\r\n",
        "\\left(\\left( \\begin{array}{r} \\mu_A \\\\ \\mu_B \\\\ \\mu_C \\end{array} \\right), \r\n",
        "\\left[ \\begin{array}{r} \\Sigma_{A} & \\Sigma_{AB} & 0 \\\\ \\Sigma_{BA} & \\Sigma_{B} & \\Sigma_{BC}  \\\\ 0 & \\Sigma_{CB} & \\Sigma_{C} \\end{array} \\right] \\right) $ \r\n",
        "\r\n",
        "In addition to the AB VAE-MRF: \\\\\r\n",
        "  - $\\mu_{C}$,  $\\Sigma_{C}$ are the outputs of the C encoder \\\\\r\n",
        "  -\t$\\Sigma_{BC}$ = $\\Sigma_{CB}^T$ \r\n",
        "\r\n",
        "## Training the ABC VAE-MRF \r\n",
        "First sample $x_B$ from either the AB or BC dataset. Then using $x_B$, sample $x_A$ from the AB dataset and sample $x_C$ from the BC dataset.\r\n",
        "\r\n",
        "- As given previously, feed $x_A, x_B$ to their respective encoders to obtain  $\\mu_A, \\Sigma_A,  \\mu_B, \\Sigma_B$ and obtain reconstructions $\\hat{x_A}, \\hat{x_B}$. Then sum the losses (reconstruction error and KL-divergence) from both A and B  and backpropagate once per batch\r\n",
        "\r\n",
        "- Feed in $x_C$ and $x_B$ to their respective encoders to:\r\n",
        "  - obtain $\\mu_C, \\Sigma_C$ from encoder C\r\n",
        "  - obtain $\\mu_B, \\Sigma_B$ from encoder B\r\n",
        "\r\n",
        "- To reconstruct $x_C$:\r\n",
        "  - Sample $z_B$ using $\\mu_B, \\Sigma_B$ (standard VAE reparameterization trick)\r\n",
        "  - Using $z_B,\\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_C$ from $P(z_C|z_B)$ (modified VAE reparameterization trick)\r\n",
        "  - Feed $z_C$ into the C decoder to obtain the reconstruction $\\hat{x}_C$ for $x_C$\r\n",
        "\r\n",
        "- To reconstruct $x_B$:\r\n",
        "  - Sample $z_C$ using $\\mu_C, \\Sigma_C$ (standard VAE reparameterization trick)\r\n",
        "  - Using $z_C,\\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_C)$ (modified VAE reparameterization trick)\r\n",
        "  - Feed $z_B$ into the B decoder to obtain the reconstruction $\\hat{x}_B$ for $x_B$\r\n",
        "\r\n",
        "- Sum the losses (reconstruction error and KL-divergence) from both B and C  and backpropagate once per batch\r\n",
        "\r\n",
        "## Querying the ABC VAE-MRF\r\n",
        "Once  the VAE-MRF is trained, to query P(C|A=0=(1,0,0,0,0,0,0,0))\r\n",
        "- Feed $x_A$ into the A encoder to obtain $\\mu_A, \\Sigma_A$\r\n",
        "- Sample $z_A$ using $\\mu_A, \\Sigma_A$ (standard VAE reparameterization trick)\r\n",
        "- Since no input $x_B, x_C$ to the B or C encoders, assume $\\mu_B, \\Sigma_B$ and $\\mu_C, \\Sigma_C$ come from the prior P(z) = Normal (0, Identity)\r\n",
        "- Using $z_A, \\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$\r\n",
        "- Using $z_B, \\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_C$ from $P(z_C|z_B)$\r\n",
        "- Feed $z_C$ into the C decoder to obtain $\\hat{x}_C$ \\\\\r\n",
        "\r\n",
        "Repeat, feeding in evidence $x_A$ multiple times to the VAE-MRF to obtain a probability distribution $P(\\hat{x}_B|x_A)$\r\n",
        "\r\n",
        "# Notes\r\n",
        "\r\n",
        "A symmetric matrix is positive definite if:\r\n",
        "\r\n",
        "- all the diagonal entries are positive, and\r\n",
        "- each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPKlzMCE8abG"
      },
      "source": [
        "# Questions and Notes\n",
        "\n",
        "Requires alternating between AB and BC samples where B is the same.\n",
        "\n",
        "Have a separate plate for each dataset.\n",
        "In Bayesian network, need to learn P(B),P(A|B), P(C|B). \\\\\n",
        "In MRF need to learn factors $\\phi(A,B)$ and $\\phi(B,C)$.\n",
        "\n",
        "How to handle datasets with 3 dimensions.\n",
        "Latent edges between A,B,C (clique)?\n",
        "\n",
        "Do we need to incorporate the parition function Z? If want probabilities that sum to 1 then yes. But if just looking to have input into the decoders then normalizing isn't necessary?\n",
        "\n",
        "Koller Definition 4.3: \\\\\n",
        "$Z = \\sum_{AB,BC} \\phi(A,B) \\times \\phi(B,C)$ \\\\\n",
        "$P(A,B,C) = \\frac{1}{Z} \\phi(A,B) \\times \\phi(B,C)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgk-LlXB64eb"
      },
      "source": [
        "# To Do\n",
        "\n",
        "- Query P(A|B=0)\n",
        "- Add BC Plate\n",
        "- Visualize latent space\n",
        "- Try more than 1 sample when sampling zA and zB\n",
        "- During training, try reconstructing A given only x_B and reconstructing B given only x_A. I believe feeding in A (and B) to reconstruct A during train time does not match what is required of the model during test time where we feed in only B to reconstruct A.\n",
        "- Modifying variational_beta to lowest value that reconstructions were valid did not change ressults (0.0001), any higher variational_beta gave poor reconstructions.\n",
        "- Check if training on only A improves performance\n",
        "- Formalize in Overleaf\n",
        "- Answer general research questions\n",
        "- Try different likelihood functions (bernoulli, gaussian)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCII451nHRR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ_f2Kmg7H9O"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}