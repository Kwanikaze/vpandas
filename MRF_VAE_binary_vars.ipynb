{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRF_VAE_binary_vars",
      "provenance": [],
      "collapsed_sections": [
        "tvSWt2iUw9xE",
        "_bTvWAZ9UARW"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kwanikaze/vpandas/blob/master/MRF_VAE_binary_vars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZaO7CHX93gN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b366f64-15af-4433-c29e-1cae0d19fd60"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import multivariate_normal\n",
        "import numpy as np\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "!pip install pgmpy==0.1.9\n",
        "import pgmpy\n",
        "import networkx as nx\n",
        "from pgmpy.models import BayesianModel\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "!pip install -i https://test.pypi.org/simple/ PPandas==0.0.1.7.1\n",
        "!pip install python-intervals\n",
        "!pip install geopandas\n",
        "!pip install geovoronoi\n",
        "import ppandas\n",
        "from ppandas import PDataFrame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pgmpy==0.1.9 in /usr/local/lib/python3.6/dist-packages (0.1.9)\n",
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: PPandas==0.0.1.7.1 in /usr/local/lib/python3.6/dist-packages (0.0.1.7.1)\n",
            "Requirement already satisfied: python-intervals in /usr/local/lib/python3.6/dist-packages (1.10.0.post1)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.6/dist-packages (0.8.2)\n",
            "Requirement already satisfied: fiona in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.8.18)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (3.0.0.post1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.15.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.1.2)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.1.1)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (20.3.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (2.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (2020.12.5)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2018.9)\n",
            "Requirement already satisfied: geovoronoi in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from geovoronoi) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from geovoronoi) (1.19.5)\n",
            "Requirement already satisfied: shapely>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from geovoronoi) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iNkadXIh0gD"
      },
      "source": [
        "# Load Data and Create Sample Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9UE259FbtK1"
      },
      "source": [
        "# Function to create OHE dataset for specified attributes given a global df\n",
        "def OHE_sample(sample_df, features_to_OHE: list):\n",
        "  for feature in features_to_OHE:\n",
        "    feature_OHE = pd.get_dummies(prefix = feature,data= sample_df[feature])\n",
        "    sample_df = pd.concat([sample_df,feature_OHE],axis=1)\n",
        "  sample_df.drop(features_to_OHE,axis=1,inplace=True)\n",
        "  print(sample_df)\n",
        "  return sample_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykDGUc_-Q2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c50f55e-ea65-40ef-c0c2-9a7e95a3f486"
      },
      "source": [
        "#Hardcode 2x2 P(A,B)\n",
        "# Load global relation from github\n",
        "from numpy import genfromtxt\n",
        "data_2 = genfromtxt('https://raw.githubusercontent.com/Kwanikaze/vpandas/master/data_2.csv', delimiter=',',skip_header=1)\n",
        "data_2_1000 = np.tile(data_2, (100, 1))\n",
        "\n",
        "#print(data_2.shape)\n",
        "#print(data_2_1000.shape)\n",
        "df = pd.DataFrame(pd.np.tile(data_2, (100, 1)))\n",
        "df.columns=['A','B']\n",
        "df=df.astype(int)\n",
        "#print(df)\n",
        "#df.to_csv('data_2_1000rows.csv',index=False)\n",
        "\n",
        "\n",
        "#df = pd.read_csv(\"data_2_1000rows.csv\") # 3columns A,B,C that each contain values 0 to 1, block diagonal\n",
        "print(df.shape)\n",
        "\n",
        "#Create two datasets containing AB and BC\n",
        "num_samples = 500\n",
        "sample1_df = df[['A','B']].sample(n=num_samples, random_state=2)\n",
        "print(sample1_df.shape)\n",
        "print(sample1_df.head())\n",
        "#sample2_df = df[['B','C']].sample(n=num_samples, random_state=3)\n",
        "#print(sample2_df.head())\n",
        "\n",
        "# Make A,B,C inputs all 8 bits\n",
        "#Could add noise so not exactly OHE: 0.01...0.9...0.01\n",
        "sample1_OHE = OHE_sample(sample1_df,['A','B'])\n",
        "#sample2_OHE = OHE_sample(sample2_df,['B','C'])\n",
        "\n",
        "# Could onvert pandas dataframes to list of lists of lists\n",
        "# [ [[OHE A1],[OHE B1]], [[OHE A2],[OHE B2]], ...  ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 2)\n",
            "(500, 2)\n",
            "     A  B\n",
            "37   1  1\n",
            "726  1  1\n",
            "846  1  1\n",
            "295  1  0\n",
            "924  1  0\n",
            "     A_0  A_1  B_0  B_1\n",
            "37     0    1    0    1\n",
            "726    0    1    0    1\n",
            "846    0    1    0    1\n",
            "295    0    1    1    0\n",
            "924    0    1    1    0\n",
            "..   ...  ...  ...  ...\n",
            "194    0    1    1    0\n",
            "136    0    1    0    1\n",
            "581    1    0    1    0\n",
            "662    1    0    1    0\n",
            "671    1    0    1    0\n",
            "\n",
            "[500 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvSWt2iUw9xE"
      },
      "source": [
        "# Global Relation Bayesian Network Ground Truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up-Ps6PEoQB4"
      },
      "source": [
        "P(A,B) = \r\n",
        "*   P(A=0,B=0) = 0.3\r\n",
        "*P(A=0,B=1) = 0.1\r\n",
        "*P(A=1,B=0) = 0.2\r\n",
        "*P(A=1,B=1) = 0.4\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubgZqS2rxNrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c582a419-d02f-4ed9-e69f-9bca557f8bc7"
      },
      "source": [
        "def groundTruth(df,query_attribute,evidence):\n",
        "    \"\"\"\n",
        "    Extracts ground truth from global relation\n",
        "    \"\"\"\n",
        "    model = BayesianModel([('B', 'A')])\n",
        "    model.fit(df)\n",
        "    nx.draw(model, with_labels=True)\n",
        "    plt.show()\n",
        "    print('\\n Global Relation Ground Truth')\n",
        "    #for var in model.nodes():\n",
        "    #    print(model.get_cpds(var))\n",
        "    inference = VariableElimination(model)\n",
        "    \n",
        "    #q = inference.query(variables=['A','B','C'])\n",
        "    #joint_prob = q.values.flatten()\n",
        "    #print(joint_prob)\n",
        "    #print('\\n P(A,B,C) \\n Ground Truth')\n",
        "    #print(q)\n",
        "    q = inference.query(variables=[query_attribute], evidence=evidence)\n",
        "    print(q)\n",
        "\n",
        "print('\\n P(B|A=0) Ground Truth')\n",
        "groundTruth(df,query_attribute = 'B', evidence = {'A':0})\n",
        "\n",
        "print('\\n P(A|B=0) Ground Truth')\n",
        "groundTruth(df,query_attribute = 'B', evidence = {'A':1})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " P(B|A=0) Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| B    |   phi(B) |\n",
            "+======+==========+\n",
            "| B(0) |   0.7500 |\n",
            "+------+----------+\n",
            "| B(1) |   0.2500 |\n",
            "+------+----------+\n",
            "\n",
            " P(A|B=0) Ground Truth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Finding Elimination Order: : : 0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Global Relation Ground Truth\n",
            "+------+----------+\n",
            "| B    |   phi(B) |\n",
            "+======+==========+\n",
            "| B(0) |   0.3333 |\n",
            "+------+----------+\n",
            "| B(1) |   0.6667 |\n",
            "+------+----------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bTvWAZ9UARW"
      },
      "source": [
        "# ppandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bto996MFUCnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c0ddbe-872b-484a-eca1-66f663dd8f77"
      },
      "source": [
        "def ppandas_query(sample1_df,num_samples,query_attribute,evidence):\n",
        "    pd1 = PDataFrame(['B'],sample1_df)\n",
        "    q = pd1.query(['A','B'])\n",
        "    cols = q.columns.tolist()\n",
        "    q = q.rename(columns={q.columns[2]:'Probability(A,B)'})\n",
        "    #Reorder columns\n",
        "    q = q[['A','B','Probability(A,B)']]\n",
        "    q= q.sort_values(by=['A','B'])\n",
        "    #print(q)\n",
        "    #Sort rows in dataframe by descending order\n",
        "    print(\"\\n ppandas P({}|{}) , n={} \\n \".format(query_attribute,evidence,num_samples))\n",
        "    q1 = pd1.query([query_attribute],evidence_vars=evidence)\n",
        "    print(q1)\n",
        "    q1 = pd1.map_query([query_attribute],evidence_vars=evidence)\n",
        "    #pd_join.visualise()\n",
        "    return q1\n",
        "\n",
        "q1 = ppandas_query(sample1_df,num_samples,query_attribute='B',evidence={'A':0})\n",
        "q1 = ppandas_query(sample1_df,num_samples,query_attribute='B',evidence={'A':1})\n",
        "q1 = ppandas_query(sample1_df,num_samples,query_attribute='A',evidence={'B':0})\n",
        "q1 = ppandas_query(sample1_df,num_samples,query_attribute='A',evidence={'B':1})\n",
        "#print(ppandas_C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ppandas P(B|{'A': 0}) , n=500 \n",
            " \n",
            "     B  Probability(B)\n",
            "0  0.0        0.770732\n",
            "1  1.0        0.229268\n",
            "\n",
            " ppandas P(B|{'A': 1}) , n=500 \n",
            " \n",
            "     B  Probability(B)\n",
            "0  0.0        0.318644\n",
            "1  1.0        0.681356\n",
            "\n",
            " ppandas P(A|{'B': 0}) , n=500 \n",
            " \n",
            "     A  Probability(A)\n",
            "0  0.0        0.626984\n",
            "1  1.0        0.373016\n",
            "\n",
            " ppandas P(A|{'B': 1}) , n=500 \n",
            " \n",
            "     A  Probability(A)\n",
            "0  0.0        0.189516\n",
            "1  1.0        0.810484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3YIf_-iAm8"
      },
      "source": [
        "# VAE-MRF Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Re5YHgVF-q"
      },
      "source": [
        "A Gaussian Markov Random Field is a Gaussian Process with a linear kernel (covariance function). $k(x,x') = x \\cdot x'$\n",
        "\n",
        "\n",
        "Pg 16: \n",
        "\"Fortunately,\n",
        "in probabilistic terms this operation is extremely simple, corresponding to conditioning the joint Gaussian prior distribution on the observations\"\n",
        "\n",
        "## Multivariate Normal\n",
        "Koller Equation 7.3: \\\\\n",
        "$P(z_A,z_B) = Normal\n",
        "\\left(\\left( \\begin{array}{r} \\mu_A \\\\ \\mu_B \\end{array} \\right), \n",
        "\\left[ \\begin{array}{r} \\Sigma_{A} & \\Sigma_{AB} \\\\ \\Sigma_{BA} & \\Sigma_{B} \\end{array} \\right] \\right) $ \n",
        "\n",
        "which is equivalent to the Matrix Cookbook (353 and 354) https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf: \\\\\n",
        "$P(z_A|z_B) = Normal_{z_A}(\\hat{\\mu}_A, \\hat{\\Sigma}_A)$ \\\\\n",
        "where: \\\\\n",
        "$\\hat{\\mu}_A = \\mu_A + \\Sigma_{AB} \\Sigma_{B}^{-1}(z_B - \\mu_B)$ \\\\\n",
        "$\\hat{\\Sigma}_A = \\Sigma_A - \\Sigma_{AB} \\Sigma_B^{-1} \\Sigma_{AB}^T$ \\\\\n",
        "\n",
        "$P(z_B|z_A) = Normal_{z_B}(\\hat{\\mu}_B, \\hat{\\Sigma}_B)$ \\\\\n",
        "where: \\\\\n",
        "$\\hat{\\mu}_B = \\mu_B + \\Sigma_{AB}^T \\Sigma_{A}^{-1}(z_A - \\mu_A)$ \\\\\n",
        "$\\hat{\\Sigma}_B = \\Sigma_B - \\Sigma_{AB}^T \\Sigma_A^{-1} \\Sigma_{AB}$ \\\\\n",
        "\n",
        "\n",
        "The output of the VAE encoders are assumed to be the mean and variance of the unary normal potentials in the MRF over the latent z's where:\n",
        "\n",
        "•\tMean: $\\mu_{A}$ and diagonal variance matrix: $\\Sigma_{A}$ are the outputs of the A encoder \\\\\n",
        "•\t$\\mu_{B}$,  $\\Sigma_{B}$ are the outputs of the B encoder \\\\\n",
        "\n",
        "\n",
        "The additional pairwise k-ary Normal potentials, which represent undirected graphical model structure between the latent A and latent B : \\\\\n",
        "•\t$\\Sigma_{AB}$ = $\\Sigma_{BA}^T$ \n",
        "\n",
        "If the latent space is dimension 3, each $\\mu \\in \\mathcal{R}^{1 \\times 3}$ and each $\\Sigma \\in \\mathcal{R}^{3 \\times 3}$.\n",
        "\n",
        "\n",
        "#Three Options to learn Gaussian MRF (in Stage 2):\n",
        "## 1. Fully Emperical (works best)\n",
        "Emperically estimate $\\mu_A, \\mu_B, \\Sigma_A$ and $\\Sigma_B$ by taking sample mean of the mu and calculating sample variance of logvar output of the entire population from marginal encoders (less accurate, need to debug $\\Sigma_A$ and $\\Sigma_B$)...\n",
        "\n",
        "Emperically estimate  $\\mu_A, \\mu_B, \\Sigma_A, \\Sigma_B, \\Sigma_{AB}$ by using sampled z_A and z_B of the entire population (N=num_samples=500) and the sample mean and covariance matrix. This way avoid having to do gradient descent in Stage 2.\n",
        "\n",
        "Note we could use more reliable $\\mu_A, \\mu_B$ instead of sample mean of $z_A, z_B$\n",
        "https://www.itl.nist.gov/div898/handbook/pmc/section5/pmc541.htm\n",
        "\n",
        "## 2. Fully Learned\n",
        "Learn $\\Sigma_A$ and $\\Sigma_B$ by gradient descent which involves doing gradient descent through an inverse.\n",
        "\n",
        "Learn $\\Sigma_{AB} by gradient descent\n",
        "\n",
        "## 3. Half Emperical Half Learned\n",
        "Emperically estimate $\\Sigma_A$ and $\\Sigma_B$ using Option 1 to avoid gradient descent through an inverse.\n",
        "\n",
        "Learn $\\Sigma_{AB}$ by gradient descent.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Training the VAE-MRF using Option 1:\n",
        "Assuming no missing data. Training is done in two stages. The first stage learns marginal VAEs for each attribute. The second stage learns the covariance matrix $\\Sigma_{AB}$ to capture the intervariable dependencies between A and B.\n",
        "\n",
        "##Stage 1 - Marginal VAEs (Identical to Stage 1 VAEM)\n",
        "Train A and B VAEs separately.\n",
        "In each epoch, break the training data into batches. Each batch contains samples of OHE input $x_A$ or $x_B$:\n",
        "- Feed in $x_A$ or $x_B$ to their respective encoders to obtain either:\n",
        "  - $\\mu_A, \\Sigma_A$ from encoder A\n",
        "  - $\\mu_B, \\Sigma_B$ from encoder B\n",
        "\n",
        "- To reconstruct $x_A$ ($x_B$):\n",
        "  - Sample $z_A$ ($z_B$) using $\\mu_A, \\Sigma_A$ ($\\mu_B, \\Sigma_B$) through standard VAE reparameterization trick\n",
        "  - Feed $z_A$  ($z_B$) into the **A** (**B**) decoder to obtain the reconstruction $\\hat{x}_A$ ($\\hat{x}_B$)\n",
        "\n",
        "- Sum the losses (reconstruction error and KL-divergence) from either A or B  and backpropagate once per batch.\n",
        "\n",
        "For marginal VAEs, fix the parameters: encoder $\\phi$ and decoder $\\theta$.\n",
        "\n",
        "## Stage 2 - Intervariable Dependency Gaussian Process\n",
        "In each epoch, break the training data into batches. Each batch contains samples of OHE input $x_A$ and $x_B$. By reconstructing $x_A, x_B$ given $x_A$ and $x_B$, learn $\\Sigma_{AB}$:\n",
        "  - Feed entire batch of $x_A$ to marginal A encoder to obtain Monte Carlo emperical $\\mu_A, \\Sigma_A$\n",
        "  - Feed entire batch of $x_B$ to marginal B encoder to obtain Monte Carlo emperical $\\mu_B, \\Sigma_B$\n",
        "  - If memory allows - feed in entire train population of $x_A$ and $x_B$ for more reliable emperical estimates (as is done with 2 binary variables A, B and 500 samples)\n",
        "  - To reconstruct a specific $x_A$:\n",
        "      - Feed specific corresponding $x_B$ to marginal encoder to obtain sample $z_B$ (standard VAE reparameterization trick)\n",
        "      - Using $z_B,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_A$ from $P(z_A|z_B)$ (modified VAE reparameterization trick)\n",
        "      - Feed $z_A$ into the A decoder to obtain the reconstruction $\\hat{x}_A$\n",
        "\n",
        "  - To reconstruct $x_B$:\n",
        "    - Feed specific corresponding $x_A$ to marginal encoder to obtain sample $z_A$ (standard VAE reparameterization trick) \n",
        "    - Using $z_A,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$ (modified VAE reparameterization trick)\n",
        "    - Feed $z_B$ into the B decoder to obtain the reconstruction $\\hat{x}_B$\n",
        "\n",
        "\n",
        "Sum the losses (reconstruction error and KL-divergence) from both A and B and backpropagate once per batch.\n",
        "Repeat for each batch. \\\\\n",
        "\n",
        "Note that $\\mu_A$, $\\Sigma_A$, $\\mu_B$, $\\Sigma_B$ are fixed for each batch.  There is only one $\\Sigma_{AB}$ to be shared. \n",
        "\n",
        "# Stage 2 - Intervariable Dependency CRF (missing data at train time scenario, not implemented)\n",
        "Training the VAE-MRF on x_A only: \n",
        "- Feed entire batch of $x_A$ to marginal A encoder to obtain emperical $\\mu_A, \\Sigma_A$\n",
        "-  Feed entire batch of $x_B$ to marginal B encoder to obtain emperical $\\mu_B, \\Sigma_B$ (If no $x_B$ assume prior P(zB) = Normal (0, Identity))\n",
        "- Sample $z_B$ using $\\mu_B, \\Sigma_B$ (standard VAE reparameterization trick)\n",
        "- Using $z_B,\\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_A$ from $P(z_A|z_B)$ (modified VAE reparameterization trick)\n",
        "-  Feed $z_A$ into the A decoder to obtain the reconstruction $\\hat{x}_A$ for $x_A$\n",
        "- Sum the losses (reconstruction error and KL-divergence) from A and backpropagate once per batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UMLBM0iE4y"
      },
      "source": [
        "# VAE Parameters\n",
        "num = 2 # digits from 0 to 1\n",
        "latent_dims = 1 # Latent z_A, z_B same dimension size\n",
        "num_epochs = 500\n",
        "batch_size = 50\n",
        "learning_rate = 1e-2 #1e-4\n",
        "use_gpu = True\n",
        "variational_beta = 5e-9 #tuned 5e-5 or 5e-6\n",
        "\n",
        "#batch_size_list = [25,50]\n",
        "#learning_rate_list = [1e-2, 1e-3, 1e-4]\n",
        "#variational_beta_list = [0.1,0.001,0.0001]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifSVkjMe-lJj"
      },
      "source": [
        "def vae_loss(batch_recon, batch_targets, mu, logvar):\r\n",
        "  criterion = nn.CrossEntropyLoss()\r\n",
        "  CE = criterion(batch_recon, batch_targets)\r\n",
        "  #print(CE)\r\n",
        "  KLd = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes\r\n",
        "  #print(KLd)\r\n",
        "  return CE,variational_beta*KLd, CE + variational_beta*KLd\r\n",
        "\r\n",
        "#Train marginal VAE\r\n",
        "def trainVAE(VAE, sample1_OHE, attribute: str):\r\n",
        "  print(\"\\nTraining marginal VAE for \" + attribute+ \" started!\")\r\n",
        "  VAE.train() #set model mode to train\r\n",
        "  optimizer = torch.optim.Adam(params = VAE.parameters(), lr = learning_rate)\r\n",
        "  x = sample1_OHE.filter(like=attribute, axis=1).values\r\n",
        "  #sample2_OHE when do BC plate\r\n",
        "  \r\n",
        "  inds = list(range(x.shape[0]))\r\n",
        "  N = num_samples\r\n",
        "  freq = num_epochs // 10 # floor division\r\n",
        "\r\n",
        "  loss_hist = []\r\n",
        "  x = Variable(torch.from_numpy(x))\r\n",
        "  \r\n",
        "  for epoch in range(num_epochs):\r\n",
        "      VAE.train()\r\n",
        "      #print('epoch' + str(epoch))\r\n",
        "      inds = np.random.permutation(inds)\r\n",
        "      x = x[inds]\r\n",
        "      x = x.to(device)\r\n",
        "      \r\n",
        "      loss = 0\r\n",
        "      CE = 0\r\n",
        "      KLd = 0\r\n",
        "      #num_batches = N / batch_size\r\n",
        "      for b in range(0, N, batch_size):\r\n",
        "          #get the mini-batch\r\n",
        "          x_batch = x[b: b+batch_size]\r\n",
        "          #feed forward\r\n",
        "          batch_recon,latent_mu,latent_logvar = VAE.forward(x_batch.float())\r\n",
        "          # Error\r\n",
        "          #Convert x_batch from OHE vectors to single scalar\r\n",
        "          # max returns index location of max value in each sample of batch \r\n",
        "          _, x_batch_targets = x_batch.max(dim=1)\r\n",
        "          train_CE, train_KLd, train_loss = vae_loss(batch_recon, x_batch_targets, latent_mu, latent_logvar)\r\n",
        "          loss += train_loss.item() / N # update epoch loss\r\n",
        "          CE += train_CE.item() / N\r\n",
        "          KLd += train_KLd.item() / N\r\n",
        "\r\n",
        "          #Backprop the error, compute the gradient\r\n",
        "          optimizer.zero_grad()\r\n",
        "          train_loss.backward()\r\n",
        "\r\n",
        "          #update parameters based on gradient\r\n",
        "          optimizer.step()\r\n",
        "          \r\n",
        "      #Record loss per epoch        \r\n",
        "      loss_hist.append(loss)\r\n",
        "      \r\n",
        "      if epoch % freq == 0:\r\n",
        "          print('')\r\n",
        "          print(\"Epoch %d/%d\\t CE: %.5f, KLd: %.5f, Train loss=%.5f\" % (epoch + 1, num_epochs,CE,KLd, loss), end='\\t', flush=True)\r\n",
        "\r\n",
        "          #Test with all training data\r\n",
        "          VAE.eval()\r\n",
        "          train_recon, train_mu, train_logvar = VAE.forward(x.float())\r\n",
        "          _, x_targets = x.max(dim=1)\r\n",
        "          CE_,KLd,test_loss = vae_loss(train_recon, x_targets, train_mu, train_logvar)\r\n",
        "          print(\"\\t CE: {:.5f}, KLd: {:.5f}, Test loss: {:.5f}\".format(CE,KLd,test_loss.item()), end='')\r\n",
        "\r\n",
        "          #print('Visualize ' + attribute + 'predictions')\r\n",
        "          #print(train_recon[0:5])\r\n",
        "          #print(x_targets[0:5])\r\n",
        "\r\n",
        "  print(\"\\nTraining marginal VAE for \" + attribute+ \" finished!\")\r\n",
        "  #print(loss_hist)\r\n",
        "\r\n",
        "#Each attribute has a marginal VAE\r\n",
        "class marginal_VAE(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.latent_dims = latent_dims\r\n",
        "        self.fc1 = nn.Linear(num, latent_dims)\r\n",
        "        self.fc_mu = nn.Linear(latent_dims, latent_dims)\r\n",
        "        self.fc_logvar = nn.Linear(latent_dims, latent_dims)\r\n",
        "        self.fc_out = nn.Linear(latent_dims,num)\r\n",
        "    \r\n",
        "    #accepts OHE input of an attribute, returns mu and log variance\r\n",
        "    def encode(self, x):\r\n",
        "        h1 = torch.sigmoid(self.fc1(x))\r\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\r\n",
        "\r\n",
        "    #Given mu and logvar generates latent z\r\n",
        "    def reparameterize(self, mu, logvar):\r\n",
        "        std = torch.exp(0.5*logvar) \r\n",
        "        eps = torch.randn_like(std)\r\n",
        "        return mu + eps*std\r\n",
        "\r\n",
        "    #Decodes latent z into reconstruction with dimension equal to num\r\n",
        "    def decode(self, z):\r\n",
        "        if z.size()[0] == self.latent_dims: #resize from [1] to [1,1]\r\n",
        "          z = z.view(1, self.latent_dims)\r\n",
        "        softmax = nn.Softmax(dim=1)  #normalizes reconstruction to range [0,1] and sum to 1\r\n",
        "        recon = softmax(self.fc_out(z))\r\n",
        "        return recon\r\n",
        "    \r\n",
        "    #Given x, returns: reconstruction x_hat, mu, log_var\r\n",
        "    def forward(self, x):\r\n",
        "        mu, logvar = self.encode(x)\r\n",
        "        z = self.reparameterize(mu, logvar)\r\n",
        "        return self.decode(z), mu, logvar\r\n",
        "    \r\n",
        "    #Given x, returns latent z\r\n",
        "    def latent(self, x, add_variance=True):\r\n",
        "        if add_variance == False:\r\n",
        "          mu, _ = self.encode(x)\r\n",
        "          return mu\r\n",
        "        else:\r\n",
        "          mu, logvar = self.encode(x)\r\n",
        "          z = self.reparameterize(mu, logvar)\r\n",
        "          return z\r\n",
        "      \r\n",
        "    \r\n",
        "    # ignore latent_mu, latent_logvar, instead generate z values from standard normal\r\n",
        "    def sample(self, num_samples):\r\n",
        "      z = torch.randn(num_samples, self.latent_dims)\r\n",
        "      z = z.to(device)\r\n",
        "      samples = self.decode(z)\r\n",
        "      return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4mAAWbsurcY"
      },
      "source": [
        "#x = torch.tensor([0.5])\r\n",
        "#x = x.repeat_interleave(10)\r\n",
        "#print(x)\r\n",
        "#x = x.unsqueeze(1)\r\n",
        "#print(x)\r\n",
        "#x = x.unsqueeze(2)\r\n",
        "#print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0FiF8-RkNLB"
      },
      "source": [
        "class VariationalAutoencoder_MRF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.latent_dims = latent_dims\n",
        "        #Marginal VAEs\n",
        "        self.VAE_A = marginal_VAE()\n",
        "        self.VAE_B = marginal_VAE()\n",
        "        #Emperical mu and logvar\n",
        "        self.muA_emp = 0\n",
        "        self.muB_emp = 0\n",
        "        self.logvarA_emp = 0\n",
        "        self.logvarB_emp = 0\n",
        "        #Sigma_{AB} = Sigma_{BA}^T\n",
        "        self.covarianceAB = torch.randn(size=(self.latent_dims,self.latent_dims))\n",
        "        self.covarianceAB = torch.nn.Parameter(self.covarianceAB,requires_grad=True)\n",
        "        #print(self.covarianceAB)\n",
        "\n",
        "\n",
        "    #Stage 1 - Train Marginal VAEs and then freeze parameters\n",
        "    def train_marginals(self):\n",
        "        trainVAE(self.VAE_A,sample1_OHE, 'A')\n",
        "        trainVAE(self.VAE_B,sample1_OHE, 'B')\n",
        "\n",
        "    def emp_covariance(self,xA,xB):\n",
        "      zA = self.latent(xA.float(), attribute='A', add_variance=True)\n",
        "      np_zA = zA.cpu().detach().numpy().reshape(num_samples,latent_dims)\n",
        "      zB = self.latent(xB.float(), attribute='B', add_variance=True)\n",
        "      np_zB = zB.cpu().detach().numpy().reshape(num_samples,latent_dims)\n",
        "      \n",
        "      z_obs = np.concatenate((np_zA, np_zB),axis=1) #(num_samples,2)\n",
        "      \n",
        "      z_obs_mean = np.mean(z_obs,axis=0) #Sample mean\n",
        "      \n",
        "      self.muA_emp= torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(z_obs_mean[0]).float(),0),0))\n",
        "      self.muB_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(z_obs_mean[1]).float(),0),0))\n",
        "\n",
        "      z_obs_cov = np.cov(z_obs,rowvar=False) #Sample covariance\n",
        "      self.logvarA_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(z_obs_cov[0][0]).float(),0),0))\n",
        "      self.logvarB_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(z_obs_cov[1][1]).float(),0),0))\n",
        "      self.covarianceAB = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(z_obs_cov[1][0]).float(),0),0))\n",
        "\n",
        "      print(\"Means of zA,zB\")\n",
        "      print(z_obs_mean)\n",
        "      #print(self.muA_emp)\n",
        "      #print(self.muB_emp)\n",
        "      print(\"Covariance Matrix zAzB\")\n",
        "      print(z_obs_cov)\n",
        "      #print(self.logvarA_emp)\n",
        "      #print(self.logvarB_emp)\n",
        "      #print(self.covarianceAB)\n",
        "\n",
        "\n",
        "    # Conditional of Multivariate Gaussian: matrix cookbook 353 and 354\n",
        "    # Attribute is the attribute of the returned z_cond \n",
        "    def conditional(self, muA, varA, muB, varB, z, attribute):\n",
        "        covarianceA = torch.diag_embed(varA) #Convert var vector to diagonal matrix\n",
        "        covarianceB = torch.diag_embed(varB) #batch_size,3,3\n",
        "        #self.covarianceAB = torch.nn.Parameter(torch.log(self.covarianceAB))\n",
        "        muA = muA.unsqueeze(2) #batch_size,latent dims\n",
        "        muB = muB.unsqueeze(2)\n",
        "        z = z.unsqueeze(2)\n",
        "        if attribute == 'A':\n",
        "          mu_cond = muA + torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                    torch.inverse(covarianceB)),\n",
        "                                      (z - muB)) # z is zB\n",
        "          var_cond = covarianceA - torch.matmul(torch.matmul(self.covarianceAB, \n",
        "                                                                torch.inverse(covarianceB)),\n",
        "                                                  torch.transpose(self.covarianceAB,0,1))\n",
        "          #var_cond = var_cond + 20*torch.eye(latent_dims) # regularization\n",
        "        elif attribute == 'B':\n",
        "          mu_cond = muB + torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1),\n",
        "                                                    torch.inverse(covarianceA)), \n",
        "                                       (z - muA)) # z is zA\n",
        "          var_cond = covarianceB - torch.matmul(torch.matmul(torch.transpose(self.covarianceAB,0,1), \n",
        "                                                              torch.inverse(covarianceA)),\n",
        "                                                 self.covarianceAB)\n",
        "              # var_cond is not a diagonal covariance matrix\n",
        "          #var_cond = var_cond + 20*torch.eye(latent_dims)\n",
        "\n",
        "        # METHOD1: re-parameterization trick to sample z_cond\n",
        "        eps = torch.randn_like(mu_cond) #64x3x1, 64x3x3 if use var_cond\n",
        "        z_cond = mu_cond + torch.matmul(torch.sqrt(var_cond),eps) #64x3x1 \n",
        "        #z_cond = mu_cond + torch.matmul(var_cond,eps)\n",
        "        z_cond = z_cond.squeeze(2) #64x3\n",
        "        return z_cond\n",
        "\n",
        "    #return mu, logvar\n",
        "    def encode(self, x, attribute):\n",
        "      if attribute == 'A':\n",
        "        return self.VAE_A.encode(x)\n",
        "      elif attribute =='B':\n",
        "        return self.VAE_B.encode(x)\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "    \n",
        "    #return reconstruction\n",
        "    def decode(self, z, attribute):\n",
        "      if attribute == 'A':\n",
        "        return self.VAE_A.decode(z)\n",
        "      elif attribute =='B':\n",
        "        return self.VAE_B.decode(z)\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "    \n",
        "    #Given xA, xB and attribute to reconstruct, return reconstruction\n",
        "    def forward(self, xA, xB, attribute):\n",
        "      muA, logvarA = self.encode(xA, attribute='A') #logvar is size [64,3]\n",
        "      muB, logvarB = self.encode(xB, attribute='B')\n",
        "      #When given both xA and xB, need to recalculate mu's and logvar's??\n",
        "      #self.emp_covariance(xA,xB)\n",
        "\n",
        "\n",
        "      # Take batch emperical average of mus and logvars\n",
        "      #size_placeholder = muA.size() #[batch_size,latent_dims]\n",
        "      #muA_emp = torch.mean(muA,0,keepdim=True).repeat(size_placeholder,1) #(batchsize,latent_dims) all repeated values of avg\n",
        "      #logvarA_emp = torch.mean(logvarA,0,keepdim=True).repeat(size_placeholder,1)\n",
        "      #muB_emp = torch.mean(muB,0,keepdim=True).repeat(size_placeholder,1)\n",
        "      #logvarB_emp = torch.mean(logvarB,0,keepdim=True).repeat(size_placeholder,1)\n",
        "      #print(logvarA)\n",
        "      if attribute == 'A':\n",
        "        zB = self.VAE_B.reparameterize(muB, logvarB)\n",
        "        zA = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zB, attribute)\n",
        "        return self.decode(zA,attribute), self.muA_emp, self.logvarA_emp #should error use emperical avg or not?\n",
        "      elif attribute == 'B':\n",
        "        zA = self.VAE_A.reparameterize(muA, logvarA)\n",
        "        zB = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zA, attribute)\n",
        "        return self.decode(zB,attribute), self.muB_emp, self.logvarB_emp\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "\n",
        "    def latent(self,x,attribute, add_variance=True, query_repetitions=1):\n",
        "      if attribute == 'A':\n",
        "          z = self.VAE_A.latent(x, add_variance)\n",
        "      elif attribute == 'B':\n",
        "          z = self.VAE_B.latent(x, add_variance)\n",
        "      z = z.repeat_interleave(query_repetitions)\n",
        "      z = z.unsqueeze(1)\n",
        "      return z\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "\n",
        "    #Given x, returns: reconstruction x_hat, mu, log_var\n",
        "    def forward_single_attribute(self, x, attribute):\n",
        "      if attribute == 'A':\n",
        "        return self.VAE_A.forward(x)\n",
        "      elif attribute == 'B':\n",
        "        return self.VAE_B.forward(x)\n",
        "      raise Exception('Invalid attribute {} provided.'.format(x))\n",
        "\n",
        "    def query_single_attribute(self, x_evidence, evidence_attribute, query_repetitions=10000):\n",
        "      add_variance=False\n",
        "      if evidence_attribute =='A':\n",
        "        zA = self.latent(x_evidence,evidence_attribute, add_variance,query_repetitions)\n",
        "        #print(\"zA\")\n",
        "        #print(zA)\n",
        "        #Use emperical mus and logvars\n",
        "        zB = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zA, attribute='B')\n",
        "        #print(\"zB\")\n",
        "        #print(zB)\n",
        "        return self.decode(zB,attribute='B')\n",
        "\n",
        "      elif evidence_attribute =='B':\n",
        "        zB = self.latent(x_evidence,evidence_attribute,add_variance,query_repetitions)\n",
        "        zA = self.conditional(self.muA_emp, self.logvarA_emp, self.muB_emp, self.logvarB_emp, zB, attribute='A')\n",
        "        return self.decode(zA,attribute='A')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_7LH-GQRW01"
      },
      "source": [
        "def trainVAE_MRF(VAE_MRF):\n",
        "  VAE_MRF.train() #set model mode to train\n",
        "  xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "  xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "  #print(xA.shape)\n",
        "\n",
        "  #sample2_OHE when do BC plate\n",
        "  \n",
        "  indsA = list(range(xA.shape[0]))\n",
        "  indsB = list(range(xB.shape[0]))\n",
        "  N = num_samples # 1000\n",
        "  freq = num_epochs // 10 # floor division\n",
        "\n",
        "  loss_hist = []\n",
        "  xA = Variable(torch.from_numpy(xA))\n",
        "  xB = Variable(torch.from_numpy(xB))\n",
        "  \n",
        "  VAE_MRF.emp_covariance(xA.float(),xB.float())\n",
        "\n",
        "  print(\"\\nTraining MRF finished!\")\n",
        "  #print(loss_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjRUnGgjnIvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5de0b7f-d0cd-4a24-95a6-ccee897072eb"
      },
      "source": [
        "#  use gpu if available\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "VAE_MRF = VariationalAutoencoder_MRF()\n",
        "VAE_MRF = VAE_MRF.to(device)\n",
        "\n",
        "VAE_MRF.train_marginals() #Stage 1, then freeze marginal VAEs\n",
        "print('Parameters for Marginal VAEs fixed')\n",
        "for param in VAE_MRF.VAE_A.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in VAE_MRF.VAE_B.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "#num_params = sum(p.numel() for p in VAE_MRF.parameters() if p.requires_grad)\n",
        "#print(\"Number of parameters: %d\" % num_params) #8*3 + 3 = 27, 3*8 + 8 = 32 3*3+3 = 12 *2 = 24, 27+32+24=83\n",
        "\n",
        "#for param in VAE_MRF.parameters():\n",
        "#    print(type(param.data), param.size())\n",
        "#print(list(VAE_MRF.parameters()))\n",
        "#print(VAE_MRF.parameters)\n",
        "\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, VAE_MRF.parameters()), lr = learning_rate)\n",
        "#print(\"CovarianceAB before training %f\" % VAE_MRF.covarianceAB.cpu().detach().numpy())\n",
        "trainVAE_MRF(VAE_MRF)\n",
        "#print(\"CovarianceAB after emperically estimating or learning by gradient descent %f\" % VAE_MRF.covarianceAB.cpu().detach().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training marginal VAE for A started!\n",
            "\n",
            "Epoch 1/500\t CE: 0.01430, KLd: 0.00000, Train loss=0.01430\t\t CE: 0.01430, KLd: 0.00000, Test loss: 0.69933\n",
            "Epoch 51/500\t CE: 0.00629, KLd: 0.00000, Train loss=0.00629\t\t CE: 0.00629, KLd: 0.00001, Test loss: 0.31449\n",
            "Epoch 101/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31361\n",
            "Epoch 151/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31341\n",
            "Epoch 201/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31334\n",
            "Epoch 251/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31332\n",
            "Epoch 301/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31330\n",
            "Epoch 351/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31329\n",
            "Epoch 401/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31328\n",
            "Epoch 451/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31328\n",
            "Training marginal VAE for A finished!\n",
            "\n",
            "Training marginal VAE for B started!\n",
            "\n",
            "Epoch 1/500\t CE: 0.01401, KLd: 0.00000, Train loss=0.01401\t\t CE: 0.01401, KLd: 0.00000, Test loss: 0.69893\n",
            "Epoch 51/500\t CE: 0.00629, KLd: 0.00000, Train loss=0.00629\t\t CE: 0.00629, KLd: 0.00001, Test loss: 0.31473\n",
            "Epoch 101/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31359\n",
            "Epoch 151/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31340\n",
            "Epoch 201/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31334\n",
            "Epoch 251/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31332\n",
            "Epoch 301/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31330\n",
            "Epoch 351/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31329\n",
            "Epoch 401/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31328\n",
            "Epoch 451/500\t CE: 0.00627, KLd: 0.00000, Train loss=0.00627\t\t CE: 0.00627, KLd: 0.00001, Test loss: 0.31328\n",
            "Training marginal VAE for B finished!\n",
            "Parameters for Marginal VAEs fixed\n",
            "Means of zA,zB\n",
            "[0.416552  0.2150363]\n",
            "Covariance Matrix zAzB\n",
            "[[ 2.36924266 -1.35830278]\n",
            " [-1.35830278  3.86764668]]\n",
            "\n",
            "Training MRF finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjBs_8dGferW"
      },
      "source": [
        "## Visualizing Multivariate Normal\r\n",
        "https://peterroelants.github.io/posts/multivariate-normal-primer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKa-T94O5mXH"
      },
      "source": [
        "# P(A,B) Multivariate Gaussian Ground Truth (1000 data points)\r\n",
        "# Hardcode Ground truth logvar,mu, Covariance\r\n",
        "Cannot hardcode since marginal encoders weights and biases generate sample specific mu and logvar that are widely different than ground truth emperical estimates. Therefore zA  and zB (not 0 or 1) differs from actual xA and xB (0 or 1).\r\n",
        "\r\n",
        "While mu and logvar of each sample (0 or 1) stays the same, the sampled z differs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IODOkX2f0f4",
        "outputId": "0d487d3f-3cfa-4221-c2a3-85bb9e16dc9f"
      },
      "source": [
        "mean = np.mean(data_2_1000, axis=0)\r\n",
        "cov = np.cov(data_2_1000, rowvar=False)\r\n",
        "print(\"Ground Truth Mean Vector\")\r\n",
        "print(mean)\r\n",
        "print(\"Ground Truth Covariance Matrix\")\r\n",
        "print(cov)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ground Truth Mean Vector\n",
            "[0.6 0.5]\n",
            "Ground Truth Covariance Matrix\n",
            "[[0.24024024 0.1001001 ]\n",
            " [0.1001001  0.25025025]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvdkGk8t6qFr",
        "outputId": "c053e3c6-70ab-4fe0-cc0c-e5da90292ea6"
      },
      "source": [
        "print(\"GP-VAE z_AB  Mean Vector\")\r\n",
        "print([-0.214131, -0.09071361])\r\n",
        "print(\"GP-VAE z_AB Covariance Matrix\")\r\n",
        "print([[5.76495748, -2.4135967],[-2.4135967, 5.09340934]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GP-VAE z_AB  Mean Vector\n",
            "[-0.214131, -0.09071361]\n",
            "GP-VAE z_AB Covariance Matrix\n",
            "[[5.76495748, -2.4135967], [-2.4135967, 5.09340934]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "0PClk2WjecgH",
        "outputId": "bb3a0fab-ed07-4f3a-8e5e-c22e897e5fef"
      },
      "source": [
        "x, y = np.mgrid[-3:3:.01, -3:3:.01]\r\n",
        "pos = np.dstack((x, y))\r\n",
        "rv = multivariate_normal([0.6, 0.5], [[0.24024024, 0.1001001], [0.1001001, 0.25025025]])\r\n",
        "fig2 = plt.figure()\r\n",
        "plt.title(\"Ground Truth Multivariate Normal\")\r\n",
        "ax2 = fig2.add_subplot(111)\r\n",
        "ax2.contourf(x, y, rv.pdf(pos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.contour.QuadContourSet at 0x7f270877d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxjVeK606_Ub"
      },
      "source": [
        "#VAE_MRF.eval()\r\n",
        "#hardcode = False\r\n",
        "#if hardcode == True:\r\n",
        "#  with torch.no_grad():\r\n",
        "#      VAE_MRF.covarianceAB = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.1001001),0),0))\r\n",
        "#      VAE_MRF.muA_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.6),0),0))\r\n",
        "#      VAE_MRF.muB_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.5),0),0))\r\n",
        "#      VAE_MRF.logvarA_emp = torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.24024024),0),0))\r\n",
        "#      VAE_MRF.logvarB_emp =torch.nn.Parameter(torch.unsqueeze(torch.unsqueeze(torch.tensor(0.25025025),0),0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpNuirlCG4b1"
      },
      "source": [
        "Reconstructions match original inputs. Each marginal encoder learns a tight ELBO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrqYmOIxeZvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e23aa9c-386b-4f08-b6c3-eb263209d47c"
      },
      "source": [
        "x_test = np.eye(num)[np.arange(num)]  # Test data (one-hot encoded)\n",
        "duplicates = 5\n",
        "x_test = np.repeat(x_test, [duplicates,duplicates],axis=0)\n",
        "x_test = Variable(torch.from_numpy(x_test))\n",
        "x_test = x_test.to(device)\n",
        "\n",
        "print(\"Print prediction results for A only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE_MRF.forward_single_attribute(x=x.float(), attribute='A')[0].cpu().detach().numpy(),decimals=2)))\n",
        "\n",
        "print(\"Print prediction results for B only:\")\n",
        "for x in x_test:\n",
        "    print(\"\\tInput: {} \\t Output: {}\".format(x.cpu().detach().numpy(), np.round(VAE_MRF.forward_single_attribute(x=x.float(), attribute='B')[0].cpu().detach().numpy(),decimals=2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Print prediction results for A only:\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "Print prediction results for B only:\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [1. 0.] \t Output: [[1. 0.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n",
            "\tInput: [0. 1.] \t Output: [[0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRbLTCH8lrJe"
      },
      "source": [
        "# Visualize Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "jMyJHpAIdzHw",
        "outputId": "a401112e-6aa7-4044-bd40-167cafaba590"
      },
      "source": [
        "x, y = np.mgrid[-3:3:.01, -3:3:.01]\r\n",
        "pos = np.dstack((x, y))\r\n",
        "rv = multivariate_normal([-0.214131, -0.09071361], [[5.76495748, -2.4135967], [-2.4135967, 5.09340934]])\r\n",
        "fig2 = plt.figure()\r\n",
        "plt.title(\"GP-VAE Latent Space Multivariate Normal\")\r\n",
        "ax2 = fig2.add_subplot(111)\r\n",
        "ax2.contourf(x, y, rv.pdf(pos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.contour.QuadContourSet at 0x7f27083da668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H_qSXLpOWCG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "2213e674-f362-4b30-863b-539934eb4564"
      },
      "source": [
        "x, y = np.mgrid[-3:3:.01, -3:3:.01]\r\n",
        "pos = np.dstack((x, y))\r\n",
        "rv = multivariate_normal([0.416552,  0.2150363], [[ 2.36924266, -1.35830278],[-1.35830278, 3.86764668]])\r\n",
        "fig2 = plt.figure()\r\n",
        "plt.title(\"GP-VAE Latent Space Multivariate Normal\")\r\n",
        "ax2 = fig2.add_subplot(111)\r\n",
        "ax2.contourf(x, y, rv.pdf(pos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.contour.QuadContourSet at 0x7f27082ffb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dfdQkZXmnr3sG3gEZHNYwcQLDhyIRiRI1xO9NSIQAxo+Y4Im4MRk1h+QYNyTrxlUhJhtDwkqOmE30KCcQogFERYWzIRF0zUGjuM4qu4HM6AILzKh8CDswyMcEufePqnJqerrf7rf7qXo+6ned0+e8XdVv1VPVXVf/+n6eqjJ3RwghRL6sit0AIYQQiyGRCyFE5kjkQgiRORK5EEJkjkQuhBCZI5ELIUTmSORC9IiZnWBm25eZf7iZPWhmqztYd2fLzg0zO9LM3Mz2id2WEEjkM2JmrzWzr5jZ98zs7vrvN5uZ1fMvNrNd9YFyn5lda2bHjCxjPzPbYWY/O2b555vZJ1rP/7D+oD1/5HWbzOz79Xraj0MmtNvN7GlzbO9tZnbiSv9vwrKmHjRmdpCZXWRmd5rZTjP7ppm9PcT6Q9Dahq+PTD+4ft9vm3O5e+xnd7/D3de6+/cXbPJerGTZ075wZvx/N7MPjEz/opltmne5YjwS+QyY2VuBPwfOAzYATwZ+E3gxsNR66XvcfS2wEbgbuLi9HHd/BLgc+NWR5a8GTgf+pn5u9WvuG31tzZfrA7L9+Pai2xmZ84G1wDOAdcArgZujtmg8TzCzZ7aevw74v7EaMyuRkuf3gNeb2ZGLLqiU5NwZ7q7HMg8qqXwP+KUpr7sY+OPW858HHhzzuhcBO4EntKa9jEr8+9TPfwp4GPh3wL3AUuu1m4AvrqD9DjxtzPSjgP9eL/+7wCXAQfW8jwCP1214EHhbPf0FwJeAHcD/Ak5oLe8fgXcD/1Rv3zXAwfW8O+p2PFg/XjimPTcCvzBlO34buLVu73nAqmnbUs8/DPgkcE/9mr9szXsjsAX4f8BngCMmrP/Iug1nA+e1pm8GzgJum7TP258N4ARg+6T93FrPPsAvA5tH2vG7wFWtz9jXgQeAbcAfjmnvm+r9f1172fVr3lBv+856v/5GPf2Auk2Pt96zQ6iC39uBW+r9+DHgSRP21wnAduAvgL9uTf8isKn+e1W9P2+n+vx/GFi3TPs3UX2+zqf6DN5KdTxtqrf/buDXRo7Baftnn9iOCfGI3oDUH8ApwGPT3vCRg3UtcCnwhQmv/SbwK63nlwHvaz2/sD5I9q0PmF9qzdtEGJE/DTgJWAOsrw+UdhtuA05sPT+0bsvL6gPwpPr5+nr+P9YH+I8C+9fPz63nTT1ogL8CbqKSy9ETtuPzwJOAw+t9+OvTtgVYTfWlcz6VoPYDXlLPexVV6n8GlTjPBr40oX3NNhxZS2E1cCywFTiROUQ+YT//YF8BT6CS7NGt+V8FXtta1rPq9+M44C7qL8PWcj5cb/f+o+8DleiOAgz4aeAh4Lnj2llPOxO4nuoX5xrgQ8BlE/bXCVQi30Al0qfX09sif2O9/59Kdcx8EvjIMu3fRHUsvqHe/39MJfn31+35uXp/rV3B/pHIh/AAfgW4c2Rak0ofBn6qnnYx8Eg9/U7gKuCoCcs8G7im/vuJ9QH0nPr5E+oPfvOB+xBwZet/mw/zjtbjlmXaP1bkY173C8DXW89vY0/B/KfmIGtN+wx1AqIS99mteW8G/qH+e+pBUx+o7wT+J/Cv9QF+6sh2nDKy/M9N2xbghVRJfK91A38PvKn1fFX9Xhwx5rU/2Abgs8DJwLlUabwTkdfP/xZ4V/330Yz8mhtp4/uA80eW89RJyx7z/58GzhzXznraFuClrec/Ur9X4/btD/4feA9wef13W+SfA97c+p+nN8ub0P5NwP9pPX9W/Zont6bdCzx7BfunCJGrRj6de4GD2zU6d3+Rux9Uz2vvwz9z94PcfYO7v9LdbzGzf9vqkLypft1HgJ+pOyhPoxJx04n2aipRX10/vwQ41czWt9Zzfb2e5nHUSjfKzJ5sZh81s2+Z2QNUwjh4mX85AnhN3Vm7w8x2AC+hOpgb7mz9/RBVypoJd3/Y3f/E3X8C+CGqXyQfN7MntV62rfX37VQ/96dty2HA7e7+2IRt+vPW9txHlU4PndLcD1NJ5XSq97JLLq3XA1U9/tPu/hCAmT3fzD5vZveY2f1U/Taj7+E2JmBmp5rZ9XXn/A6qX1vTPgOfau2vLcD3qfqMluO/ACeb2Y+PTD+E6n1suJ1K4u3ljbb/rtbfDwO4++i0tTDz/ikCiXw6XwYepfoZvmLc/Qu+u0Pyx+pptwNfoEr7r6fu5Kz5NaoP4h1mdifwcaoSy+vm34Sx/AlVInmWuz+xbou1mz7y+m1Uibz9BXKAu587w7pGl7X8i90fqNt3APCU1qzDWn8fDjQdvMttyzbg8AmdZduo6sLtbdrf3b80pYlXUJUlbnX3O8bMf4jql1XDhmWWNW3fXAusN7NnUwn90ta8S6l++R3m7uuAD7Lnezhx+Wa2hmo7/owq0R5EFR5smf/bRvUrqb2/9nP3by23Ae5+L1UafvfIrG9TfTk0HE4VYtpiXtFnZ4RZ9k8RSORTcPcdwH8GPmBmp5nZgWa2qj6wDlhg0X8DvIVq5MslAGZ2KPBS4OXAs+vHj1MlmnGjV2ZlqR762DxWAwdSdWLdX6/390b+5y6q2mXD3wKvMLOTzWx1vZwTzGzjDOu/h6rj7KmTXmBmv29mP2lmS2a2H1U9dgfwjdbLfs/M/o2ZHVbPv7yevty2/A/gO8C5ZnZA3e4X1/M+CLzDzH6sbsM6M3vNtI1x9+8BPwv8+oSX3AC8rt5Pp1DVnycxup9H1/WvVF/m51H1D1zbmn0gcJ+7P2Jmz2NlX/ZLVHXle4DHzOxUqhpzu10/ZGbrWtM+CJxjZkcAmNl6M5s14LyXqmPyGa1plwG/a2ZPMbO1VF/Il0/49TQPi+yfrJDIZ8Dd3wP8B6pRBXfVjw9R1Y2npbdJXEF1YH7O3b9TT3s9cIO7X+PudzYP4L8Cx7WGvb1wzDjyn1xmXTdR/eRsHm+g+nJ6LnA/8HdUHU1t/hQ4u/4Z/R/dfRvVr5J3Uh3826iEOfUzVJcCzgH+qV7eC8a9DPhrqlEn36bqvPx5d3+w9ZorqWroN9RtvrCePnFbvBoz/QqqDtE7qDrgfrme9ymqL8mP1iWZG4FTp21P/b+b3f2WCbPPrNe5g2rk0aeXWdQe+3nCay6lqsN/fERybwb+yMx2Au+iKkfNhLvvpBoF9DGqETuvo0qvzfytVKK9tW7bIVRDcK8CrqnXeT3w/NFlT1jfA1S18nap7CKq0tR1VEM4HwH+/azbMANz75/csLrwL0TSmJlTjd5IcWy5EFFRIhdCiMyRyIUQInNUWhFCiMxRIhdCiMyJciGapVX7+/6rD+xlXb7f0vQXJcz39yty2KuYwOpH8v6FbI/sit2EonngsXu+6+7rR6dHEfn+qw/kRQdPHa67MLuOmWWIc7rcf9Sa2E0QkVh3y6OxmzA3S1vnvvqtmMI/3PmB28dN16UhE0QCF81nIEehNwFKQu+PYmvkuaZxSVy0uf+oNT945Eaux2COFCvyHMnxYBX9kaPQdx2zUULvAZVWEiG3A3QRdh7RXQfugbfn3Vk4CzmWXXYds1Gllg4pUuS5JYASJd6lrBdZb0miz03oqp13R5Eiz4lSJB5L3CtlXDtzl3uOQpfMw1KcyHNK4zlLPBdxz0Ipcs9J6ErnYSlO5LmQo8RLkvc0Rrc1J7HnJnTJfHEk8gjkJPEhyXs5chR7LkJXOl+cokSeU1kldSTw5clJ7DkJXTKfj6JEngOpp3EJfD7a+y1VqecgdKXz+ZDIeyRliUvg4Uhd6rkIXTKfnWLO7Ey9rJKqxHceYZJ4hzT7N8V9nOpnsiH1YzollMgHSopiKZ1mn6eU0lNP5yq1zEYxiTxlUks+knhcUkzpqV/HRel8eSTygZGSPER6pS3JPE+KKK2k/AancmCkJAuxNyl1kKZcblGpZTxK5B0iiYt5SCWlp1xuSTm8xUAiL5wUhCDmIyWhp4hkvhuJvCNS+PCnIAGxOCkIPdV0LplXFFEjF3sT+8BflEcP7+5u7GvuWOps2V2SQh39/qPWJFc7V91cIi+SnCTepbDnWWcuko85Jj3VztAhnw2avchT/GkV8ydo6hKPIe6VMK59Kcs9ttAl8zTIXuRiN6lKPHV5T2O0/SmKPZbQU0znQ5S5RB6YWGk8RYnnLvBJpCz2nUeY0jnDq5tL5CIopcp7OVITu9L5boaSzjX8sABSSOOPHr5rkBIfR7MvYu+PWMMWUxummGI/WmiUyAMS4wMcW+KxZZU67f0TK6nHSOippfPSk7kSecZI4nkRO6XHSOgppfOSk7lELlZMbCHlTuzSi2ReHhJ5psRK4xJ4WGIJve90ntIp/ruO2Vic0BcWuZkdZmafN7N/MbObzOzMEA3LjVQ+pF0iiXdHrJSudF4GIRL5Y8Bb3f1Y4AXAb5nZsQGWKybQ98GnUkq/9L2/Y6TzVChF5guL3N2/4+5fq//eCWwBDl10uSINJPB4lCz01EotuRO0Rm5mRwLPAb4yZt4ZZrbZzDbvevzhkKsdFH0mJ0k8DWIIvS8k8zAEE7mZrQWuAH7H3R8Yne/uF7j78e5+/NKq/UOtNglS+TCGRBJPjz6F3nc6T4GcZR7khCAz25dK4pe4+ydDLFOIWThy4z1Bl3fb9vVBl9cFjcz7OMGor2u3pHKtllxPHFpY5GZmwIXAFnd/7+JNEpPoKyGlmsZDS3ul60hN8n0Jva8zQ1M5GzRHmYdI5C8GXg/8s5ndUE97p7tfHWDZomdSk3gf8p6V0bakIvY+hT6UdJ6bzBcWubt/EYh/1SaxMKlIPCV5L0dqYu9D6JJ5mujMzkyIfV2VPjhy4z3ZSHwcTftjb0PXX8h9dYSm0AmaSweoRC6AuGk8BfmFJrbU+xjhIpmng0Quokm8RIGPI6bUuxa6ZJ4Guh55BpRWVhmCvCfR3vY+a+qPHr6rs9p5H6NaUhjRknLNXIl84PSdxocs8VH6TulK54uTajKXyEVvSOLj6bv0IpkvRooyl8hFL0jis9GX0LtM532MapHM90QiHzB9lVUk8ZXTp9C7QjLvD4lcdIokvhh9CL3rdN4lknlF9iJPtRdZSOIh6UvoXSCZd0/2Ii+drg6Crssqkng3dC30rtK5ZN4tErkIjiTePX0IPTRdd4IOWeYSuRAZk5vModt0PlSZ68zOAdJlWSX1NH7Shq1z/++1dx4TsCXhaPZ5F2eKdnVFxS6vohj7yokxzgCVyEWxLCLtlSwvFcF3LXTJfHb6lnkRIl/auj16Z4NII42Hlvc864wt9iM33iOZMyyZFyFyIWIIfBLttsSSelfpXDJPE4lcBCFWGk9J4OOILfUu0nkXdfNSZd5XKpfIRZakLvBxxJJ6LulcMp8fDT8UC9N3Gs9R4qOctGFr79vRxfsUegRUqUMTu+7DK0bkMU/VH0INLhVKkHibRuh9bZdkXqbMixG5mI2Y9+ZclNIkPkpfQu/irFDJfDa6krlELrKgdIm36VPoIZHM4yGRJ06X90EMQR/18SFJvE0fQpfM+6eLVF6UyHVJ2/IYqsTb9CHzkELPSeaxCC3zokQuRKnkls5DXw63K5mXUi+XyAORy8iV0GfldYnS+N50LfSUSy2S+WSCiNzMLjKzu83sxhDLE0Isj2Qeltw7P0Ml8ouBUwItayFUJy8DpfHpdJnOJfP+CJHKg4jc3a8D7guxLCHEyuhS5ql2gkrme6IaeUC6qpOnPgRRxCendB4KyXw3vYnczM4ws81mtnnX4w93ui6VV/JGZZX5SV3mOZ9Z3Afzyrw3kbv7Be5+vLsfv7Rq/75WK8TgGJLMS0vl86LSihAF0lWpRTLvnnlSeajhh5cBXwaebmbbzexNIZa7CLHKK7mMJxfDQDJfjFxkHmrUyunu/iPuvq+7b3T3C0MsV+wmZIdnTicFicWRzMtHdwgSYkZOe+LXZnrdJx54bsctWTknbdga/K5EoW4jF/JOQ13cZSjW3YVWcmehokW+tHV753fmGMe6Wx7NrrNEjGdWeS/3P6mIfSgy74LUZa7OzozQePL+OO2JX5tL4sstK9TyFqGLTtDUyixDrJdL5AMlVPoJfUPf2HQt3FSkXvpY/aHVy4sXuUaviFnpW66xhR5S5qmlcuhG5qmm8uJFXhoqr5RHTKGXLvMuSFHmgxC5TtkX04hd6mjaEKMdJcu8tHr5JAYh8likXl5JeZRAn6Qg8TaSeUXqMk+JwYi8pFSeWnmltA7PFIhdP1+U1K6YWFK9fByDEXksUk/lIm36lHmKI1lUL58NiXzgqLySPrnKXCWW/hiUyFVe6Y6Q5ZXQZx8uRy7li6HLPBSlllgGJfJYqLwiQtBn3Tw1mavEsjyDE7lS+d6ovJIXOco8BCqxTGZwIo/FEFK5Rq/0Ry4loQaVWLplkCIvKZWHIrVU3ledPJUrE6aKSix5MEiRx6KLVJ5ap6foD5VYFqOkVD5YkSuV702IVJ7r6JVcUYklPWLIfLAij4VSuQhNHzIvtcRSSsfnoEWuVN4N6vTsn9xkHoKUZd53Kh+0yGORciofYqenOjz7Zwgllj4ZvMiVyrtBqbx/lMrnJ/dUPniRx0KpfHaUymcnp85PpfJwSOQolY8jtREsIh2Uymenr1QukUck5VSeGkrlZaJUHgaJvKakVJ5SiUWpvH9UK5+fXFO5RB4ZXYNldpTKy2QIqbxrmUvkLWKl8pRLLOr4zBOl8vnJ8SShICI3s1PM7BtmdrOZvT3EMmNRUoklJVRiEcuhVL4YC4vczFYD7wdOBY4FTjezYxdd7tBQKp8dpfJ0UCpPgxCJ/HnAze5+q7vvAj4KvCrAcqOhVL43qXV8SublMYRU3hUhRH4osK31fHs9bQ/M7Awz22xmm3c9/nCA1ZZHyqk8FLmVWCTz/Eg5lXdVXumts9PdL3D34939+KVV+/e12rlRx+feDLHEApL5NFIrrwyRECL/FnBY6/nGelr2qMSyN0MssYBk3heplVdySeUhRP5V4Ggze4qZLQGvBa4KsNzBknIqD4VkLrpkaLeEW1jk7v4Y8BbgM8AW4GPuftOiy02FklJ5aiUWyVwMldCpPEiN3N2vdvcfdfej3P2cEMscOqmf8ZlavbxPPvHAcyX0EVK7g1BIchiKqDM7Z0Adn92RYypvkMzTZkjlFYl8RlRi2Zshl1gaJPNhkHqnp0SeOEMpseQu89SEnlp7Vkpq5ZXUkchXgEos45HMK1IRegptSIXUyyuhUrlEvkIk83yIIXNIR+giLCl3ekrkA6fkejnEkznEEbq+QIaJRD4HJaXykEjm4+lD6PoVMJkhlFck8jkpSeal18shvsxht2xDCrdkgavDc3b2id0AkQYH3u7BaoBr7lgKkoJu274+6MF87Z3HJHOBp1H5ruSOPqWKOwd2HmFJ9i1J5AuwtHU7u47Z2Pt6193yaCcX3pHM4yE5D5v7j1qz0K9tlVYWpKQSC6Q5kqWLMksKpRYhQiGRB6A0mYci5PVYurghhWQ+HEJ2eKY4DFEiz5yhdH6CZC7KZpFyqUQeiJKuxQKSuRA5IZEHpLQSy9BkLqHPj/ZdXCTywEjmk0ld5iAhiTyRyAtCMg+D0rmYRlcdnvPWySXyDohZL5fMwyGZz4b2U3wk8o4orfMT0pa50rkYMhJ5h5RWL4d0ZQ5K5zHQfkkDibxjSpR5SHKTucS1G+2LbpinTi6RF0wO9XLIS+YgofdF1+9jSUjkPaDOz+nkJnMYttCHut2pIpH3hGQ+nS5kLqGHZ0jbmgsSeY9I5tNZc8dSlukchiH00rcvVyTynpHMZyNXmUO5Qu9zm3Koj6d0FUSJPAKS+WzkWmppKEnopWxHLqx05MpCIjez15jZTWb2uJkdv8iyRH8MWebQf9prhJ6jDHNt99BY9FZvNwK/CHwoQFsGRazbxDV0ebs4CPuzM9Rt49o0Mu/7Br9tKaZ4y7mGmPLOoaySGguJ3N23AJilUyvKidgy75KQ9/+EbmQO4e8JuhJSlLrSd570dvNlMzsDOANgv1Vr+1pt8sSUeVepvKELmUPY23ZBvHTeZlSgfYo9JXkrjc/HVJGb2WeBDWNmneXuV866Ine/ALgAYN2+P5zeHX4jIpmvjC7TOcQVesMkuYYQfEri7osu+lpSYqrI3f3EPhoydCTzldGVzCFuuWUaJUtYaXx+NPwwIUocltjQ1YiWrpJW30MVRX508Zmel0WHH77azLYDLwT+zsw+E6ZZw0UyXzld/myW0PtB+3gxFhK5u3/K3Te6+xp3f7K7nxyqYUMmtsxzuZ55m65roBJ6d3S9X0uvj4NKK8kS+w5DXcs8t1JLg4QeFu3LMEjkYiIqtUxGQl8c7b9wSOQJEzuVQ94yl9DTpa99NoSyCkjkySOZL0ZfB7KEPjvaT+GRyDNgKDLPPZ2DhD6NPvfNUNI4SOTZMASZQxnpHCT0cZS0P1IaQw4SeVZI5ovTZzqH3UIvSWIrJcb2DymNg0SeHZJ5GPoWOgxT6kPa1phI5BkyJJn3IfQYlC71mNs2tDQOPV7GVoQlhWuZNzLv8oJb0M1Ft9p0dXncWWkLL9WLdc1CqV9Ko6RWHweJPGtSkDl0f/VE6ObOQ6PEFjrsLcMcxJ6SwIeYxkEiz54hyRy6T+eQhtAbUhR7SuJuM1SJg0ReBE3NPLbQS5I5dHvN83mZJNEuBZ+quNv0JfEUyyogkRdFCum8T5lDt6UWSCudL0cOshWzs9LBBBq1UhipjGjpY1QL9JeQYgxXFLOh90UiL5IUZA79DFGEfoYpNkjoadHne5FqWQUk8mIZmsyh3wOtEbqkHg/t+91I5AWTksxLTOcNEnq/xNjfKadxkMiLJxWZQ//pXEIvjyHs33mOE4l8AAxV5hAnSans0g2x9mfqaRwk8sGQmsxLT+cNEvrixNyHOUgcJPJBsbR1e3JC75MUhC6prwztr9mQyAfIkGUO8VOWpD6dFPZPjM/JvMeDzuwcKCmcBdrQ11UU2/R1Zug02rJK/ezRPogt74bYX/YrRYl8wKSUzCFeOk/loB1yUh/qdodCiXzgpJTMIU46h3QSesOo1EpM66mKO9YX+yJBRiIXyckc+rv41iipCb2hFLGnKu+GVH6drZSFRG5m5wGvAHYBtwBvcPcdIRom+iWVS+G2iZXOIV2hN4wTYopyT13cbWJKfNGy4qI18muBZ7r7ccA3gXcsuDwRmdTq5hCndt6QUg19Gu0ae4x6e8x1L0ou7/EkFkrk7n5N6+n1wGmLNUekQKqlFoiTziH9hL4cOQk1BrElHiKohBy18kbg7yfNNLMzzGyzmW3e9fjDAVcruiDFZA5x0znkldDFdEp5L6cmcjP7LLBhzKyz3P3K+jVnAY8Bl0xajrtfAFwAsG7fHy5j7xVOinVziJ/OIe+ELipSkHioYDJV5O5+4nLzzWwT8HLgpe4ef8+I4KRYaoF4I1vatGUgqedDChIPyUKlFTM7BXgb8Ep3fyhMk0SKpFxqiV1uaVDZJX1Seo9Cfm4XrZH/JXAgcK2Z3WBmHwzQJpEoqV10q42ELqZR8nuy6KiVp4VqiMiHVEstkEa5pUFll3RITeKhQ4fO7BRzkbrMIW5n6CiSehxSE3hXSORiblId1dKQotBBUu+DlAXeRQlQVz8UC5Nq3bwhpfr5KKqnhyX1/dnV51CJXAQh5VJLQ6oJHfZOkErqKyNleTd0GSYkchGM1EstDSl1iE5C5ZfZyEHgfSCRi+AonYdFaX1PcpR316U9iVx0Qk7pHPIQesMQxZ6jvBv66J+RyEWn5JDOIU+hN5Qq9pzl3dBXJ7tELjonl3QOeQu9YZIAUxZ8CdIepc+RUhK56I1c0jmUIfRRlpNlX5IvUdjj6Hu4q0QueiWndA57HpAlSX2UoQi2D2Kcs6ATgkQUUj+JaBwpn1gk0iDW50MiF9FI+WqKyyGhi3HE/ExI5CI6OcocJHSxm9ifA9XIRRLkVjtvM5Q6utib2AJvUCIXSZFrOm9QSh8OKb3PSuQiOXJO5w1K6eWSksAblMhFsuTaGTqKUno5pPo+KpGL5MnpRKLlUErPl1QF3iCRiywoodzSRlLPg9QF3iCRi6woTeggqadKLhIHiVxkSolCB0k9NjnJu41ELrKmlPr5OCT1/shV4A0SucieUtN5m1HRSOxhyF3gDRK5KIYhCL1BYp+fUuTdRiIXxTEkoTdI7MtTorzbSOSiWIYo9Iahi710cY8ikYviGbLQGyaJrRTBD03coywkcjN7N/Aq4HHgbmCTu387RMOECI2Evjc5Cn7o0h7Hoon8PHf/fQAz+23gXcBvLtwqITpEQp/OSmQZSvoS9PwsJHJ3f6D19ABAN/4T2SChh0ECjo+5L+ZeMzsH+FXgfuBn3P2eCa87AzijfvpM4MaFVpw2BwPfjd2IDtH25U3J21fytgEc4e7rRydOFbmZfRbYMGbWWe5+Zet17wD2c/c/mNYSM9vs7sdPb3OeaPvyRtuXLyVv23JMLa24+4kzLusS4GpgqsiFEEKEY6EbS5jZ0a2nrwK2LtYcIYQQK2XRUSvnmtnTqYYf3s7sI1YuWHC9qaPtyxttX76UvG0TWbizUwghRFx0z04hhMgciVwIITInmsjN7N1m9r/N7AYzu8bMDonVli4ws/PMbGu9jZ8ys4NitykUZvYaM7vJzB43s2KGepnZKWb2DTO72czeHrs9ITGzi8zsbjMr8vwNMzvMzD5vZv9SfzbPjN2mPomZyM9z9+Pc/dnAf6M6vb8krgWe6e7HAd8E3hG5PSG5EfhF4LrYDQmFma0G3g+cChwLnG5mx8ZtVVAuBk6J3YgOeQx4q7sfC7wA+K3C3r9liSby0k/vd/dr3P2x+un1QDHngbv7Fnf/Rux2BOZ5wM3ufqu77wI+SjWktgjc/Trgvtjt6Ap3/467f63+eyewBTg0bqv6I+plbEdP74/Zlo55I3B57EaIZTkU2NZ6vh14fkSed20AAAFASURBVKS2iAUwsyOB5wBfiduS/uhU5NNO73f3s4Cz6tP730JmZ4XOcvkCMzuL6mffJX22bVFmvTSDEClhZmuBK4DfGfnVXzSdirz00/unbZ+ZbQJeDrzUMxuwv4L3rhS+BRzWer6xniYywcz2pZL4Je7+ydjt6ZOYo1aKPr3fzE4B3ga80t0fit0eMZWvAkeb2VPMbAl4LXBV5DaJGTEzAy4Etrj7e2O3p2+indlpZlcAe5ze7+7FJCAzuxlYA9xbT7re3Yu46YaZvRr4C2A9sAO4wd1PjtuqxTGzlwHvA1YDF7n7OZGbFAwzuww4geoyr3cBf+DuF0ZtVEDM7CXAF4B/pnIKwDvd/ep4reoPnaIvhBCZozM7hRAicyRyIYTIHIlcCCEyRyIXQojMkciFECJzJHIhhMgciVwIITLn/wMtxpJEbQIPqwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkKiDijtuUHt"
      },
      "source": [
        "## Check encoder, decoders work on their own\r\n",
        "It appears the marginal VAEs converges to a local minimum with accurate reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0jQpqykvUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "60ba674a-b7ae-497e-ebcc-92eb4f8b0d12"
      },
      "source": [
        "xA = sample1_OHE.filter(like='A', axis=1).values\n",
        "xB = sample1_OHE.filter(like='B', axis=1).values\n",
        "xA = Variable(torch.from_numpy(xA))\n",
        "xB = Variable(torch.from_numpy(xB))\n",
        "\n",
        "zA = VAE_MRF.latent(xA.float(), attribute='A',add_variance=True)\n",
        "np_zA = zA.cpu().detach().numpy().reshape(num_samples,latent_dims)\n",
        "\n",
        "zB = VAE_MRF.latent(xB.float(), attribute='B',add_variance=True)\n",
        "np_zB = zB.cpu().detach().numpy().reshape(num_samples,latent_dims)\n",
        "\n",
        "\n",
        "if latent_dims==1:\n",
        "  plt.plot(np_zA, 'o', color='black',label=\"zA\");\n",
        "  plt.plot(np_zB, 's', color='red',label=\"zB\");\n",
        "\n",
        "  plt.title(\"Latent Encodings z from A and B Marginal Encoders\")\n",
        "elif latent_dims ==2:\n",
        "  #plt.plot(np_zA[:,0], np_zA[:,1],'o', color='black');\n",
        "  plt.plot(np_zA[:,0],np_zA[:,1], 'o', color='black',label=\"zA\");\n",
        "  plt.plot(np_zB[:,0],np_zB[:,1], 's', color='red',label=\"zB\");\n",
        "elif latent_dims ==3:\n",
        "  from mpl_toolkits.mplot3d import Axes3D\n",
        "  fig = plt.figure()\n",
        "  ax = Axes3D(fig)\n",
        "  #t = np.arange(1000)\n",
        "  ax.scatter(np_zA[:,0], np_zA[:,1], np_zA[:,2])\n",
        "plt.title(\"Latent Encodings z from A and B Marginal Encoders\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f27084c4208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R19NH78B_O9s"
      },
      "source": [
        "Plot shows that 0 and 1 one hot encoded input are separated in latent space for both A and B marginal encoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vBTljFA9YMa"
      },
      "source": [
        "# Query P(B|A=0).\n",
        "##Ground truth: \n",
        "\n",
        "P(B=0|A=0) = 0.75\n",
        "\n",
        "P(B=1|A=0) = 0.25\n",
        "\n",
        "\n",
        "Feed nothing into B encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuZdoZXu9biT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a31a81-26c0-42ec-9691-57dd4ecc4c2b"
      },
      "source": [
        "xA_evidence = x_test[0] #Evidence is A=0\n",
        "#xA_evidence = xA_evidence.repeat(2,1)\n",
        "print('A evidence input, first 5 rows:')\n",
        "print(xA_evidence[0:5].detach().numpy()) #need to resize/ view for single sample, or make evidence a batch repeated\n",
        "\n",
        "xB_query = VAE_MRF.query_single_attribute(x_evidence=xA_evidence.float(), evidence_attribute = 'A', query_repetitions=10000)\n",
        "print('B query output, first 5 rows:')\n",
        "print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\n",
        "\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0).detach().numpy())\n",
        "\n",
        "#Taking max of each row in xB_query and counting times each element is max\n",
        "print('xB_query count of when each column is max:')\n",
        "_,indices_max =xB_query.max(dim=1) \n",
        "#print(indices_max.numpy())\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A evidence input, first 5 rows:\n",
            "[1. 0.]\n",
            "B query output, first 5 rows:\n",
            "[[1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [0.65 0.35]]\n",
            "xB_query mean of each column:\n",
            "[0.71865934 0.28134075]\n",
            "xB_query count of when each column is max:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 7218, 1: 2782}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQ15yUZnHsc"
      },
      "source": [
        "# Query P(B|A=1), \n",
        "## Groundtruth: \n",
        "\n",
        "P(B=0|A=1) = 1/3\n",
        "\n",
        "P(B=1|A=1) = 2/3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFsGFCqQmIZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d3eb42-187b-471f-b419-14eaede2d2a6"
      },
      "source": [
        "xA_evidence = x_test[1+duplicates] #Evidence is A=1\n",
        "#xA_evidence = xA_evidence.repeat(10000,1)\n",
        "print('A evidence input, first 5 rows:')\n",
        "print(xA_evidence[0:5].detach().numpy()) #need to resize/ view for single sample, or make evidence a batch repeated\n",
        "\n",
        "print('B query output, first 5 rows:')\n",
        "xB_query = VAE_MRF.query_single_attribute(x_evidence=xA_evidence.float(), evidence_attribute = 'A',query_repetitions=10000)\n",
        "print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\n",
        "\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0).cpu().detach().numpy())\n",
        "\n",
        "#Taking max of each row in xB_query and counting times each element is max\n",
        "print('xB_query count of when each column is max:')\n",
        "_,indices_max =xB_query.max(dim=1) \n",
        "#print(indices_max.numpy())\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A evidence input, first 5 rows:\n",
            "[0. 1.]\n",
            "B query output, first 5 rows:\n",
            "[[1.   0.  ]\n",
            " [0.99 0.01]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [1.   0.  ]]\n",
            "xB_query mean of each column:\n",
            "[0.33374    0.66625994]\n",
            "xB_query count of when each column is max:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 3328, 1: 6672}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkaagB3p778s"
      },
      "source": [
        "# Query P(A|B=0), \r\n",
        "## Groundtruth: \r\n",
        "\r\n",
        "P(A=0|B=0) = 3/5\r\n",
        "\r\n",
        "P(A=1|B=0) = 2/5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fG2m11O76ua",
        "outputId": "5f47f009-fc5c-4860-9889-1a50b44dea86"
      },
      "source": [
        "xB_evidence = x_test[0] #Evidence is A=0\r\n",
        "#xB_evidence = xB_evidence.repeat(10000,1)\r\n",
        "print('B evidence input, first 5 rows:')\r\n",
        "print(xB_evidence[0:5].detach().numpy()) #need to resize/ view for single sample, or make evidence a batch repeated\r\n",
        "\r\n",
        "xA_query = VAE_MRF.query_single_attribute(x_evidence=xB_evidence.float(), evidence_attribute = 'B', query_repetitions=10000)\r\n",
        "print('A query output, first 5 rows:')\r\n",
        "print(np.round(xA_query[0:5].cpu().detach().numpy(),decimals=2))\r\n",
        "\r\n",
        "#Averaging all xB_query\r\n",
        "print('xA_query mean of each column:')\r\n",
        "print(torch.mean(xA_query,0).cpu().detach().numpy())\r\n",
        "\r\n",
        "#Taking max of each row in xB_query and counting times each element is max\r\n",
        "print('xA_query count of when each column is max:')\r\n",
        "_,indices_max =xA_query.max(dim=1) \r\n",
        "#print(indices_max.numpy())\r\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\r\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B evidence input, first 5 rows:\n",
            "[1. 0.]\n",
            "A query output, first 5 rows:\n",
            "[[0.98 0.02]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [1.   0.  ]\n",
            " [0.   1.  ]]\n",
            "xA_query mean of each column:\n",
            "[0.6138562  0.38614383]\n",
            "xA_query count of when each column is max:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 6160, 1: 3840}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCHA7Db88RQr"
      },
      "source": [
        "# Query P(A|B=1), \r\n",
        "## Groundtruth: \r\n",
        "\r\n",
        "P(A=0|B=1) = 1/5\r\n",
        "\r\n",
        "P(A=1|B=1) = 4/5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJVNrF4J8z0i",
        "outputId": "89987b8f-350d-4b97-bc5f-0a6d81a68880"
      },
      "source": [
        "xB_evidence = x_test[1+duplicates] #Evidence is A=0\r\n",
        "#xB_evidence = xB_evidence.repeat(10000,1)\r\n",
        "print('B evidence input, first 5 rows')\r\n",
        "print(xB_evidence[0:5].detach().numpy()) #need to resize/ view for single sample, or make evidence a batch repeated\r\n",
        "\r\n",
        "xA_query = VAE_MRF.query_single_attribute(x_evidence=xB_evidence.float(), evidence_attribute = 'B', query_repetitions=10000)\r\n",
        "print('A query output, first 5 rows:')\r\n",
        "print(np.round(xA_query[0:5].cpu().detach().numpy(),decimals=2))\r\n",
        "print(xA_query.size())\r\n",
        "\r\n",
        "#Averaging all xB_query\r\n",
        "print('xA_query mean of each column:')\r\n",
        "print(torch.mean(xA_query,0).detach().numpy())\r\n",
        "\r\n",
        "#Taking max of each row in xB_query and counting times each element is max\r\n",
        "print('xA_query count of when each column is max:')\r\n",
        "_,indices_max =xA_query.max(dim=1) \r\n",
        "#print(indices_max.numpy())\r\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\r\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B evidence input, first 5 rows\n",
            "[0. 1.]\n",
            "A query output, first 5 rows:\n",
            "[[0.37 0.63]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.   1.  ]\n",
            " [0.03 0.97]]\n",
            "torch.Size([10000, 2])\n",
            "xA_query mean of each column:\n",
            "[0.23016104 0.769839  ]\n",
            "xA_query count of when each column is max:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 2256, 1: 7744}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8gVDBBLmw7u"
      },
      "source": [
        "The VAE_MRF is almost as accurate as ppandas, consistently under estimates the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "PkGbSSK1iQ1e",
        "outputId": "78af993d-3ce5-4469-c448-81ff39c1ab82"
      },
      "source": [
        "break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-6aaf1f276005>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5whHhIl14l5"
      },
      "source": [
        "# Query P(B|A=1,B=1)\r\n",
        "Feed both A and B, correctly identifies correct B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPPraAN111xK"
      },
      "source": [
        "xA_evidence = x_test[1+duplicates] #Evidence is A=1\r\n",
        "xA_evidence = xA_evidence.repeat(500,1)\r\n",
        "\r\n",
        "xB_evidence = x_test[1+duplicates] #Evidence is A=1\r\n",
        "xB_evidence = xB_evidence.repeat(500,1)\r\n",
        "\r\n",
        "xB_query,_,_ = VAE_MRF.forward(xA_evidence.float(),xB_evidence.float(), 'A')\r\n",
        "print(np.round(xB_query[0:5].cpu().detach().numpy(),decimals=2))\r\n",
        "print(xB_query.size())\r\n",
        "\r\n",
        "#Averaging all xB_query\r\n",
        "print('xB_query mean of each column:')\r\n",
        "print(torch.mean(xB_query,0))\r\n",
        "\r\n",
        "#Taking max of each row in xB_query and counting times each element is max\r\n",
        "print('xB_query count of when each column is max:')\r\n",
        "_,indices_max =xB_query.max(dim=1) \r\n",
        "#print(indices_max.numpy())\r\n",
        "unique, counts = np.unique(indices_max.numpy(), return_counts=True)\r\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oMp0BWBo3po"
      },
      "source": [
        "#Query P(A|B= -1)\n",
        "Try feeding into B encoder negative ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN5B_9zMpBib"
      },
      "source": [
        "xA_evidence = x_test[0] #Evidence is A=0\n",
        "xA_evidence = xA_evidence.repeat(500,1)\n",
        "xB = torch.tensor([0,0])\n",
        "#xB = torch.tensor([0,0,0,0,0,0,0,0])\n",
        "#xB = torch.tensor([0,0,0,0,0,0,0,1]) # if feed in valid input, get correct result\n",
        "xB = xB.repeat(500,1)\n",
        "\n",
        "xB_query,_,_ = VAE_MRF.forward(xA_evidence.float(),xB.float(), attribute='B')\n",
        "print(xB_query.size())\n",
        "#Averaging all xB_query\n",
        "print('xB_query mean of each column:')\n",
        "print(torch.mean(xB_query,0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZJqFYQKqEWZ"
      },
      "source": [
        "- No matter xA evidence, if B encoder always given -1's B decoder same xB\n",
        "- No matter xA evidence, if B encoder always given 0's B decoder returns same xB\n",
        "- If feed in valid xB as evidence, then get correct xB as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niyylLQg52C9"
      },
      "source": [
        "# Querying the VAE-MRF\r\n",
        "Once  the VAE-MRF is trained, to query P(B|A=0=(1,0,0,0,0,0,0,0))\r\n",
        "- Feed $x_A$ into the A encoder to obtain $\\mu_A, \\Sigma_A$\r\n",
        "- Sample $z_A$ using $\\mu_A, \\Sigma_A$ (standard VAE reparameterization trick)\r\n",
        "- Since no input $x_B$ to the B encoder, assume $\\mu_B, \\Sigma_B$ come from the prior P(z) = Normal (0, Identity)\r\n",
        "- Using $z_A, \\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$\r\n",
        "- Feed $z_B$ into the B decoder to obtain $\\hat{x}_B$ \\\\\r\n",
        "\r\n",
        "Repeat, feeding in evidence $x_A$ multiple times to the VAE-MRF to obtain a probability distribution $P(\\hat{x}_B|x_A)$\r\n",
        "\r\n",
        "# Extension to Two Datasets AB and BC (not yet implemented)\r\n",
        "$P(z_A,z_B,z_C) = Normal\r\n",
        "\\left(\\left( \\begin{array}{r} \\mu_A \\\\ \\mu_B \\\\ \\mu_C \\end{array} \\right), \r\n",
        "\\left[ \\begin{array}{r} \\Sigma_{A} & \\Sigma_{AB} & 0 \\\\ \\Sigma_{BA} & \\Sigma_{B} & \\Sigma_{BC}  \\\\ 0 & \\Sigma_{CB} & \\Sigma_{C} \\end{array} \\right] \\right) $ \r\n",
        "\r\n",
        "In addition to the AB VAE-MRF: \\\\\r\n",
        "  - $\\mu_{C}$,  $\\Sigma_{C}$ are the outputs of the C encoder \\\\\r\n",
        "  -\t$\\Sigma_{BC}$ = $\\Sigma_{CB}^T$ \r\n",
        "\r\n",
        "## Training the ABC VAE-MRF \r\n",
        "First sample $x_B$ from either the AB or BC dataset. Then using $x_B$, sample $x_A$ from the AB dataset and sample $x_C$ from the BC dataset.\r\n",
        "\r\n",
        "- As given previously, feed $x_A, x_B$ to their respective encoders to obtain  $\\mu_A, \\Sigma_A,  \\mu_B, \\Sigma_B$ and obtain reconstructions $\\hat{x_A}, \\hat{x_B}$. Then sum the losses (reconstruction error and KL-divergence) from both A and B  and backpropagate once per batch\r\n",
        "\r\n",
        "- Feed in $x_C$ and $x_B$ to their respective encoders to:\r\n",
        "  - obtain $\\mu_C, \\Sigma_C$ from encoder C\r\n",
        "  - obtain $\\mu_B, \\Sigma_B$ from encoder B\r\n",
        "\r\n",
        "- To reconstruct $x_C$:\r\n",
        "  - Sample $z_B$ using $\\mu_B, \\Sigma_B$ (standard VAE reparameterization trick)\r\n",
        "  - Using $z_B,\\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_C$ from $P(z_C|z_B)$ (modified VAE reparameterization trick)\r\n",
        "  - Feed $z_C$ into the C decoder to obtain the reconstruction $\\hat{x}_C$ for $x_C$\r\n",
        "\r\n",
        "- To reconstruct $x_B$:\r\n",
        "  - Sample $z_C$ using $\\mu_C, \\Sigma_C$ (standard VAE reparameterization trick)\r\n",
        "  - Using $z_C,\\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_C)$ (modified VAE reparameterization trick)\r\n",
        "  - Feed $z_B$ into the B decoder to obtain the reconstruction $\\hat{x}_B$ for $x_B$\r\n",
        "\r\n",
        "- Sum the losses (reconstruction error and KL-divergence) from both B and C  and backpropagate once per batch\r\n",
        "\r\n",
        "## Querying the ABC VAE-MRF\r\n",
        "Once  the VAE-MRF is trained, to query P(C|A=0=(1,0,0,0,0,0,0,0))\r\n",
        "- Feed $x_A$ into the A encoder to obtain $\\mu_A, \\Sigma_A$\r\n",
        "- Sample $z_A$ using $\\mu_A, \\Sigma_A$ (standard VAE reparameterization trick)\r\n",
        "- Since no input $x_B, x_C$ to the B or C encoders, assume $\\mu_B, \\Sigma_B$ and $\\mu_C, \\Sigma_C$ come from the prior P(z) = Normal (0, Identity)\r\n",
        "- Using $z_A, \\mu_A, \\Sigma_A, \\mu_B, \\Sigma_B$, sample $z_B$ from $P(z_B|z_A)$\r\n",
        "- Using $z_B, \\mu_C, \\Sigma_C, \\mu_B, \\Sigma_B$, sample $z_C$ from $P(z_C|z_B)$\r\n",
        "- Feed $z_C$ into the C decoder to obtain $\\hat{x}_C$ \\\\\r\n",
        "\r\n",
        "Repeat, feeding in evidence $x_A$ multiple times to the VAE-MRF to obtain a probability distribution $P(\\hat{x}_B|x_A)$\r\n",
        "\r\n",
        "# Notes\r\n",
        "\r\n",
        "A symmetric matrix is positive definite if:\r\n",
        "\r\n",
        "- all the diagonal entries are positive, and\r\n",
        "- each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row/column.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPKlzMCE8abG"
      },
      "source": [
        "# Questions and Notes\n",
        "\n",
        "Requires alternating between AB and BC samples where B is the same.\n",
        "\n",
        "Have a separate plate for each dataset.\n",
        "In Bayesian network, need to learn P(B),P(A|B), P(C|B). \\\\\n",
        "In MRF need to learn factors $\\phi(A,B)$ and $\\phi(B,C)$.\n",
        "\n",
        "How to handle datasets with 3 dimensions.\n",
        "Latent edges between A,B,C (clique)?\n",
        "\n",
        "Do we need to incorporate the parition function Z? If want probabilities that sum to 1 then yes. But if just looking to have input into the decoders then normalizing isn't necessary?\n",
        "\n",
        "Koller Definition 4.3: \\\\\n",
        "$Z = \\sum_{AB,BC} \\phi(A,B) \\times \\phi(B,C)$ \\\\\n",
        "$P(A,B,C) = \\frac{1}{Z} \\phi(A,B) \\times \\phi(B,C)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgk-LlXB64eb"
      },
      "source": [
        "# To Do\n",
        "\n",
        "- Query P(A|B=0)\n",
        "- Add BC Plate\n",
        "- Visualize latent space\n",
        "- Try more than 1 sample when sampling zA and zB\n",
        "- During training, try reconstructing A given only x_B and reconstructing B given only x_A. I believe feeding in A (and B) to reconstruct A during train time does not match what is required of the model during test time where we feed in only B to reconstruct A.\n",
        "- Modifying variational_beta to lowest value that reconstructions were valid did not change ressults (0.0001), any higher variational_beta gave poor reconstructions.\n",
        "- Check if training on only A improves performance\n",
        "- Formalize in Overleaf\n",
        "- Answer general research questions\n",
        "- Try different likelihood functions (bernoulli, gaussian)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCII451nHRR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ_f2Kmg7H9O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}